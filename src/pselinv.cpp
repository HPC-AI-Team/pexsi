/// @file pselinv.cpp
/// @brief Implementation of the parallel SelInv.
/// @author Lin Lin
/// @date 2012-11-20
#include "pselinv.hpp"


#ifdef USE_TAU
#include "TAU.h"
#elif defined (PROFILE) || defined(PMPI)
#define TAU
#include "timer.h"
#endif



#define MOD(a,b) \
  ( ((a)%(b)+(b))%(b))


namespace PEXSI{

  Grid::Grid	( MPI_Comm Bcomm, int nprow, int npcol )
  {
#ifndef _RELEASE_
    PushCallStack("Grid::Grid");
#endif
    Int info;
    MPI_Initialized( &info );
    if( !info ){
      throw std::logic_error( "MPI has not been initialized." );
    }
    comm = Bcomm;
    MPI_Comm_rank( comm, &mpirank );
    MPI_Comm_size( comm, &mpisize );
    if( mpisize != nprow * npcol ){
      throw std::logic_error( "mpisize != nprow * npcol." ); 
    }

    numProcRow = nprow;
    numProcCol = npcol;

    Int myrow = mpirank / npcol;
    Int mycol = mpirank % npcol;

    MPI_Comm_split( comm, myrow, mycol, &rowComm );
    MPI_Comm_split( comm, mycol, myrow, &colComm );


#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method Grid::Grid  ----- 


  Grid::~Grid	(  )
  {
#ifndef _RELEASE_
    PushCallStack("Grid::~Grid");
#endif
    // Dot not free grid.comm which is not generated by Grid().

    MPI_Comm_free( &rowComm );
    MPI_Comm_free( &colComm ); 

#ifndef _RELEASE_
    PopCallStack();
#endif
    return ;
  } 		// -----  end of method Grid::~Grid  ----- 

} // namespace PEXSI


namespace PEXSI{


  PMatrix::PMatrix ( const PEXSI::Grid* g, const PEXSI::SuperNode* s, const PEXSI::SuperLUOptions * o ):grid_(g), super_(s), options_(o)
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::PMatrix");
#endif
    if( grid_->numProcRow != grid_->numProcCol ){
      throw std::runtime_error( "The current version of SelInv only works for square processor grids." ); }


    L_.resize( this->NumLocalBlockCol() );
    U_.resize( this->NumLocalBlockRow() );
    //workingSet_.resize(this->NumSuper());
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "PMatrix is constructed. The grid information: " << std::endl;
    statusOFS << "mpirank = " << MYPROC(grid_) << std::endl;
    statusOFS << "myrow   = " << MYROW(grid_) << std::endl; 
    statusOFS << "mycol   = " << MYCOL(grid_) << std::endl; 
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif
    return ;
  } 		// -----  end of method PMatrix::PMatrix  ----- 

  PMatrix::~PMatrix() {}	



  void PMatrix::GetEtree(std::vector<Int> & etree_supno )
  {

#ifndef _RELEASE_
    PushCallStack("PMatrix::GetEtree");
    double begin =  MPI_Wtime( );
#endif
    Int nsupers = this->NumSuper();

    if( options_->ColPerm != "PARMETIS" ) {
      /* Use the etree computed from serial symb. fact., and turn it
         into supernodal tree.  */
      const PEXSI::SuperNode * superNode = this->SuperNode();


      //translate from columns to supernodes etree using supIdx
      etree_supno.resize(this->NumSuper());
      for(Int i = 0; i < superNode->etree.m(); ++i){
        Int curSnode = superNode->superIdx[i];
        Int parentSnode = (superNode->etree[i]>= superNode->etree.m()) ?this->NumSuper():superNode->superIdx[superNode->etree[i]];
        if( curSnode != parentSnode){
          etree_supno[curSnode] = parentSnode;
        }
      }

    } else { /* ParSymbFACT==YES and SymPattern==YES  and RowPerm == NOROWPERM */
      /* Compute an "etree" based on struct(L), 
         assuming struct(U) = struct(L').   */

      /* find the first block in each supernodal-column of local L-factor */
      std::vector<Int> etree_supno_l( nsupers, nsupers  );
      for( Int ksup = 0; ksup < nsupers; ksup++ ){
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          // L part
          std::vector<LBlock>& Lcol = this->L( LBj(ksup, grid_) );
          if(Lcol.size()>0){
            Int firstBlk = 0;
            if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
              firstBlk=1;

            for( Int ib = firstBlk; ib < Lcol.size(); ib++ ){
              etree_supno_l[ksup] = std::min(etree_supno_l[ksup] , Lcol[ib].blockIdx);
            }
          }
        }
      }


#if ( _DEBUGlevel_ >= 1 )
      statusOFS << std::endl << " Local supernodal elimination tree is " << etree_supno_l <<std::endl<<std::endl;

#endif
      /* form global e-tree */
      etree_supno.resize( nsupers );
      mpi::Allreduce( (Int*) &etree_supno_l[0],(Int *) &etree_supno[0], nsupers, MPI_MIN, grid_->comm );
      etree_supno[nsupers-1]=nsupers;
    }

#ifndef _RELEASE_
    double end =  MPI_Wtime( );
    statusOFS<<"Building the list took "<<end-begin<<"s"<<std::endl;
#endif
#ifndef _RELEASE_
    PopCallStack();
#endif
  }


  void PMatrix::ConstructCommunicationPattern	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::ConstructCommunicationPattern");
#endif
    Int numSuper = this->NumSuper();
#ifndef _RELEASE_
    PushCallStack( "Initialize the communication pattern" );
#endif
    isSendToBelow_.Resize(grid_->numProcRow, numSuper);
    isSendToRight_.Resize(grid_->numProcCol, numSuper);
    isSendToCrossDiagonal_.Resize( numSuper );
    isSendToDiagonal_.Resize( numSuper );
    SetValue( isSendToBelow_, false );
    SetValue( isSendToRight_, false );
    SetValue( isSendToCrossDiagonal_, false );
    SetValue( isSendToDiagonal_, false );

    isRecvFromAbove_.Resize( numSuper );
    isRecvFromLeft_.Resize( numSuper );
    isRecvFromCrossDiagonal_.Resize( numSuper );
    isRecvFromBelow_.Resize( grid_->numProcCol, numSuper );
    SetValue( isRecvFromAbove_, false );
    SetValue( isRecvFromBelow_, false );
    SetValue( isRecvFromLeft_, false );
    SetValue( isRecvFromCrossDiagonal_, false );
#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local column communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local column communication" << std::endl;
#endif
    // localColBlockRowIdx stores the nonzero block indices for each local block column.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockCol x numNonzeroBlock
    std::vector<std::set<Int> >   localColBlockRowIdx;

    localColBlockRowIdx.resize( this->NumLocalBlockCol() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<Int>  tBlockRowIdx;
        tBlockRowIdx.clear();

        // L part
        std::vector<LBlock>& Lcol = this->L( LBj(ksup, grid_) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          tBlockRowIdx.push_back( Lcol[ib].blockIdx );
        }

        // U part
        for( Int ib = 0; ib < this->NumLocalBlockRow(); ib++ ){
          std::vector<UBlock>& Urow = this->U(ib);
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            if( Urow[jb].blockIdx == ksup ){
              tBlockRowIdx.push_back( GBi( ib, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockRowIdx;
        mpi::Allgatherv( tBlockRowIdx, tAllBlockRowIdx, grid_->colComm );

        localColBlockRowIdx[LBj( ksup, grid_ )].insert(
            tAllBlockRowIdx.begin(), tAllBlockRowIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Column block " << ksup 
          << " has the following nonzero block rows" << std::endl;
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for(ksup)


#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local row communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local row communication" << std::endl;
#endif
    // localRowBlockColIdx stores the nonzero block indices for each local block row.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockRow x numNonzeroBlock
    std::vector<std::set<Int> >   localRowBlockColIdx;

    localRowBlockColIdx.resize( this->NumLocalBlockRow() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<Int>  tBlockColIdx;
        tBlockColIdx.clear();

        // U part
        std::vector<UBlock>& Urow = this->U( LBi(ksup, grid_) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          tBlockColIdx.push_back( Urow[jb].blockIdx );
        }

        // L part
        for( Int jb = 0; jb < this->NumLocalBlockCol(); jb++ ){
          std::vector<LBlock>& Lcol = this->L(jb);
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            if( Lcol[ib].blockIdx == ksup ){
              tBlockColIdx.push_back( GBj( jb, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockColIdx;
        mpi::Allgatherv( tBlockColIdx, tAllBlockColIdx, grid_->rowComm );

        localRowBlockColIdx[LBi( ksup, grid_ )].insert(
            tAllBlockColIdx.begin(), tAllBlockColIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Row block " << ksup 
          << " has the following nonzero block columns" << std::endl;
        for( std::set<Int>::iterator si = localRowBlockColIdx[LBi( ksup, grid_ )].begin();
            si != localRowBlockColIdx[LBi( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for(ksup)

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack("SendToBelow / RecvFromAbove");
#endif

    MPI_Group baseColGrp;
    MPI_Comm_group(grid_->colComm, &baseColGrp);


    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes to the right of ksup


      for( Int jsup = ksup + 1; jsup < numSuper; jsup++ ){
        Int jsupLocalBlockCol = LBj( jsup, grid_ );
        Int jsupProcCol = PCOL( jsup, grid_ );
        if( MYCOL( grid_ ) == jsupProcCol ){

          // SendToBelow / RecvFromAbove only if (ksup, jsup) is nonzero.
          if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 ) {
            for( std::set<Int>::iterator si = localColBlockRowIdx[jsupLocalBlockCol].begin();
                si != localColBlockRowIdx[jsupLocalBlockCol].end(); si++	 ){
              Int isup = *si;
              Int isupProcRow = PROW( isup, grid_ );
              if( isup > ksup ){
                if( MYROW( grid_ ) == isupProcRow ){
                  isRecvFromAbove_(ksup) = true;
                }
                if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
                  isSendToBelow_( isupProcRow, ksup ) = true;
                }
              } // if( isup > ksup )
            } // for (si)
          } // if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 )

        } // if( MYCOL( grid_ ) == PCOL( jsup, grid_ ) )

      } // for(jsup)
    } // for(ksup)

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToBelow:" << isSendToBelow_ << std::endl;


    statusOFS << std::endl << "isSendToBelow:" << std::endl;
    for(int j = 0;j< isSendToBelow_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToBelow_.m();i++){
        statusOFS<< isSendToBelow_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }



    statusOFS << std::endl << "isRecvFromAbove:" << isRecvFromAbove_ << std::endl;
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif












#ifndef _RELEASE_
    PushCallStack("SendToRight / RecvFromLeft");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes below ksup

      for( Int isup = ksup + 1; isup < numSuper; isup++ ){
        Int isupLocalBlockRow = LBi( isup, grid_ );
        Int isupProcRow       = PROW( isup, grid_ );
        if( MYROW( grid_ ) == isupProcRow ){
          // SendToRight / RecvFromLeft only if (isup, ksup) is nonzero.
          if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 ){
            for( std::set<Int>::iterator si = localRowBlockColIdx[isupLocalBlockRow].begin();
                si != localRowBlockColIdx[isupLocalBlockRow].end(); si++ ){
              Int jsup = *si;
              Int jsupProcCol = PCOL( jsup, grid_ );
              if( jsup > ksup ){

                if( MYCOL( grid_ ) == jsupProcCol ){
                  isRecvFromLeft_(ksup) = true;
                }
                if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
                  isSendToRight_( jsupProcCol, ksup ) = true;
                }
              }
            } // for (si)
          } // if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 )



        } // if( MYROW( grid_ ) == isupProcRow )


        if( MYCOL( grid_ ) == PCOL(ksup, grid_) ){

          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){ 
            isRecvFromBelow_(isupProcRow,ksup) = true;
          }    
          else if (MYROW(grid_) == isupProcRow){
            isSendToDiagonal_(ksup)=true;
          }    
        } // if( MYCOL( grid_ ) == PCOL(ksup, grid_) )




      } // for (isup)





    }	 // for (ksup)


#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToRight:" << isSendToRight_ << std::endl;

    statusOFS << std::endl << "isSendToRight:" << std::endl;
    for(int j = 0;j< isSendToRight_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToRight_.m();i++){
        statusOFS<< isSendToRight_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }

    statusOFS << std::endl << "isRecvFromLeft:" << isRecvFromLeft_ << std::endl;
    statusOFS << std::endl << "isRecvFromBelow:" << isRecvFromBelow_ << std::endl;
#endif
#ifndef _RELEASE_
    PopCallStack();
#endif








#ifndef _RELEASE_
    PushCallStack("SendToCrossDiagonal / RecvFromCrossDiagonal");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end(); si++ ){
          Int isup = *si;
          Int isupProcRow = PROW( isup, grid_ );
          if( isup > ksup && MYROW( grid_ ) == isupProcRow ){
            isSendToCrossDiagonal_( ksup ) = true;
          }
        } // for (si)
      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for (ksup)

    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localRowBlockColIdx[ LBi(ksup, grid_) ].begin();
            si != localRowBlockColIdx[ LBi(ksup, grid_) ].end(); si++ ){
          Int jsup = *si;
          Int jsupProcCol = PCOL( jsup, grid_ );
          if( jsup > ksup && MYCOL(grid_) == jsupProcCol ){
            isRecvFromCrossDiagonal_[ksup] = true;
          }
        } // for (si)
      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for (ksup)
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToCrossDiagonal:" << isSendToCrossDiagonal_ << std::endl;
    statusOFS << std::endl << "isRecvFromCrossDiagonal:" << isRecvFromCrossDiagonal_ << std::endl;
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif

    //Build the list of supernodes based on the elimination tree from SuperLU
    std::vector<std::vector<Int> > & WSet = this->WorkingSet();

    //do the real stuff with elimination trees
    //translate from columns to supernodes etree using supIdx
    std::vector<Int> snodeEtree(this->NumSuper());
    GetEtree(snodeEtree);
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << " Supernodal elimination tree is " << snodeEtree <<std::endl<<std::endl;
#endif

#ifndef _RELEASE_
    double begin = MPI_Wtime();
#endif


    //find roots in the supernode etree (it must be postordered)
    //initialize the parent we are looking at 
    Int rootParent = snodeEtree[this->NumSuper()-2];

    //look for roots in the forest
    std::vector< Int>  initialRootList(1,rootParent);
    std::vector< Int>  mergeRootBuf;
    bool needMerge = false;
    Int prevRootIdx = -1;
    std::vector< Int> & prevRoot = initialRootList;

    /* initialize the num of child for each node */
    Int nsupers = this->NumSuper();
    std::vector<Int> num_child;
    num_child.resize(nsupers,0);
    for(Int i=0; i<nsupers; i++ ) if( snodeEtree[i] != nsupers ) num_child[snodeEtree[i]] ++;

    while(prevRoot.size()>0){
      WSet.push_back(std::vector<Int>());
      Int totalChild =0;
      for(Int i = 0; i<prevRoot.size();++i){ totalChild += num_child[prevRoot[i]]; }
      WSet.back().reserve(totalChild);

      for(Int i = 0; i<prevRoot.size();++i){
        rootParent = prevRoot[i];
        std::vector<Int>::iterator parentIt = snodeEtree.begin()+rootParent;
        std::vector<Int>::iterator curRootIt = std::find (snodeEtree.begin() ,parentIt, rootParent);
        while(curRootIt != parentIt){
          Int curNode = curRootIt - snodeEtree.begin();
          WSet.back().push_back(curNode);
          //switch the sign to remove this root
          *curRootIt =-*curRootIt;
          //look for next root
          curRootIt = std::find (snodeEtree.begin() ,parentIt, rootParent);
        }
      }
      //No we have now several roots >> must maintain a vector of roots
      if(needMerge){
        mergeRootBuf.clear();
        prevRootIdx++;
        for(Int j=prevRootIdx;j<WSet.size();++j){
          mergeRootBuf.insert(mergeRootBuf.end(),WSet[j].begin(),WSet[j].end());
        }
        prevRoot = mergeRootBuf;
        needMerge = false;
      }
      else{
        prevRootIdx++;
        prevRoot = WSet[prevRootIdx];
      }
    }
    if(WSet.back().size()==0){
      WSet.pop_back();
    }


    for (Int lidx=0; lidx<WSet.size() ; lidx++){
      if(options_->MaxPipelineDepth){
        if(WSet[lidx].size()>options_->MaxPipelineDepth)
        {
          std::vector<std::vector<Int> >::iterator pos = WSet.begin()+lidx+1;               
          WSet.insert(pos,std::vector<Int>());
          WSet[lidx+1].insert(WSet[lidx+1].begin(),WSet[lidx].begin() +options_->MaxPipelineDepth ,WSet[lidx].end());
          WSet[lidx].erase(WSet[lidx].begin()+options_->MaxPipelineDepth,WSet[lidx].end());
        }
      }
    }
#ifndef _RELEASE_
    double end =  MPI_Wtime( );
    statusOFS<<std::endl<<"Time for building working set: "<<end-begin<<std::endl<<std::endl;
#endif

#if ( _DEBUGlevel_ >= 0 )
    for (Int lidx=0; lidx<WSet.size() ; lidx++){
      statusOFS << std::endl << "L"<< lidx << " is: {";
      for (Int supidx=0; supidx<WSet[lidx].size() ; supidx++){
        statusOFS << WSet[lidx][supidx] << " ["<<snodeEtree[WSet[lidx][supidx]]<<"] ";
      }
      statusOFS << " }"<< std::endl;
    }
#endif


    return ;
  } 		// -----  end of method PMatrix::ConstructCommunicationPattern  ----- 

  void PMatrix::SelInv	(  )
  {
#ifdef SELINV_MEMORY
#ifdef USE_TAU
    TAU_TRACK_MEMORY();
    TAU_ENABLE_TRACKING_MEMORY();
#else
    Real globalBufSize = 0;
#endif
#endif

#ifdef SELINV_TIMING
    Real begin_SendULWaitContentFirst, end_SendULWaitContentFirst, time_SendULWaitContentFirst = 0;
#if defined (PROFILE) || defined(PMPI) || defined(USE_TAU)
    TAU_PROFILE_SET_CONTEXT(MPI_COMM_WORLD);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_START("SelInv");
#elif defined (PROFILE)
    TAU_FSTART(SelInv);
#endif
#endif

#ifndef _RELEASE_
    PushCallStack("PMatrix::SelInv");
#endif



    Int numSuper = this->NumSuper(); 

    // Main loop
    std::vector<std::vector<Int> > & superList = this->WorkingSet();
    Int numSteps = superList.size();

    for (Int lidx=0; lidx<numSteps ; lidx++){
      Int stepSuper = superList[lidx].size(); 


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("AllocateBuffer");
#elif defined(PROFILE)
      TAU_FSTART(AllocateBuffer);
#endif
#endif

      std::vector<std::vector<MPI_Request> >  arrMpireqsSendToBelow;
      std::vector<std::vector<MPI_Request> >  arrMpireqsSendToRight;
      std::vector<MPI_Request>   arrMpireqsRecvSizeFromAny;
      std::vector<MPI_Request>   arrMpireqsRecvContentFromAny;
      std::vector<NumMat<Scalar> >  arrLUpdateBuf;
      std::vector<NumMat<Scalar> >  arrDiagBuf;
      std::vector<std::vector<Int> >  arrRowLocalPtr;
      std::vector<std::vector<Int> >  arrBlockIdxLocal;
      std::vector<std::vector<char> > arrSstrLcolSend;
      std::vector<std::vector<char> > arrSstrUrowSend;
      std::vector<std::vector<char> > arrSstrLcolRecv;
      std::vector<std::vector<char> > arrSstrUrowRecv;
      std::vector< std::vector<char> > arrSstrCrossDiag;
      std::vector<Int > arrSstrLcolSizeSend;
      std::vector<Int > arrSstrUrowSizeSend;
      std::vector<Int> arrSizeStmFromAbove;
      std::vector<Int> arrSizeStmFromLeft;
      std::vector<Int >  arrSstrSizeCrossDiag;

      //allocate the buffers for this supernode
      arrMpireqsSendToBelow.resize( stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcRow, MPI_REQUEST_NULL ));
      arrMpireqsSendToRight.resize(stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcCol, MPI_REQUEST_NULL ));
      arrMpireqsRecvSizeFromAny.resize(stepSuper*2 , MPI_REQUEST_NULL);
      arrMpireqsRecvContentFromAny.resize(stepSuper*2 , MPI_REQUEST_NULL);
      arrSstrUrowSend.resize(stepSuper, std::vector<char>( ));
      arrSstrLcolSend.resize(stepSuper, std::vector<char>( ));
      arrSstrUrowSizeSend.resize(stepSuper, 0);
      arrSstrLcolSizeSend.resize(stepSuper, 0);
      arrSstrUrowRecv.resize(stepSuper, std::vector<char>( ));
      arrSstrLcolRecv.resize(stepSuper, std::vector<char>( ));
      arrSizeStmFromLeft.resize(stepSuper,0);
      arrSizeStmFromAbove.resize(stepSuper,0);
      arrLUpdateBuf.resize(stepSuper,NumMat<Scalar>());
      arrRowLocalPtr.resize(stepSuper,std::vector<Int>());
      arrBlockIdxLocal.resize(stepSuper,std::vector<Int>());
      arrDiagBuf.resize(stepSuper,NumMat<Scalar>());
      arrSstrCrossDiag.resize(stepSuper);
      arrSstrSizeCrossDiag.resize(stepSuper,0);

      arrMpireqsSendToBelow.assign( stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcRow, MPI_REQUEST_NULL ));
      arrMpireqsSendToRight.assign(stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcCol, MPI_REQUEST_NULL ));
      arrMpireqsRecvSizeFromAny.assign(stepSuper*2 , MPI_REQUEST_NULL );
      arrMpireqsRecvContentFromAny.assign(stepSuper*2 , MPI_REQUEST_NULL );
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("AllocateBuffer");
#elif defined(PROFILE)
      TAU_FSTOP(AllocateBuffer);
#endif
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateL");
#endif
#if ( _DEBUGlevel_ >= 1 )
      statusOFS << std::endl << "Communication to the Schur complement." << std::endl << std::endl; 
#endif


      // Senders
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

        std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
        std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
        std::vector<char> & sstrUrowSend = arrSstrUrowSend[supidx];
        std::vector<char> & sstrLcolSend = arrSstrLcolSend[supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl <<  "["<<ksup<<"] "<< "Communication for the U part." << std::endl << std::endl; 
#endif


        // Communication for the U part.
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
          // Pack the data in U
          std::stringstream sstm;
          std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
          std::vector<UBlock>&  Urow = this->U( LBi(ksup, grid_) );
          // All blocks are to be sent down.
          serialize( (Int)Urow.size(), sstm, NO_MASK );
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            serialize( Urow[jb], sstm, mask );
          }
          sstrUrowSend.resize( Size( sstm ) );
          sstm.read( &sstrUrowSend[0], sstrUrowSend.size() );
          arrSstrUrowSizeSend[supidx] = sstrUrowSend.size();

          for( Int iProcRow = 0; iProcRow < grid_->numProcRow; iProcRow++ ){
            if( MYROW( grid_ ) != iProcRow &&
                isSendToBelow_( iProcRow,ksup ) == true ){
              // Use Isend to send to multiple targets
              MPI_Isend( &arrSstrUrowSizeSend[supidx], 1, MPI_INT,  
                  iProcRow, SELINV_TAG_COUNT*supidx+SELINV_TAG_U_SIZE, grid_->colComm, &mpireqsSendToBelow[2*iProcRow] );
              MPI_Isend( (void*)&sstrUrowSend[0], arrSstrUrowSizeSend[supidx], MPI_BYTE, 
                  iProcRow, SELINV_TAG_COUNT*supidx+SELINV_TAG_U_CONTENT, 
                  grid_->colComm, &mpireqsSendToBelow[2*iProcRow+1] );
#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<<  "Sending U " << arrSstrUrowSizeSend[supidx] << " BYTES"<< std::endl <<  std::endl; 
#endif
            } // Send 
          } // for (iProcRow)
        } // if I am the sender

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<< "Communication for the L part." << std::endl << std::endl; 
#endif



        // Communication for the L part.
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          // Pack the data in L 
          std::stringstream sstm;
          std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
          mask[LBlockMask::NZVAL] = 0; // nzval is excluded 

          std::vector<LBlock>&  Lcol = this->L( LBj(ksup, grid_) );
          // All blocks except for the diagonal block are to be sent right
          if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
            serialize( (Int)Lcol.size() - 1, sstm, NO_MASK );
          else
            serialize( (Int)Lcol.size(), sstm, NO_MASK );

          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            if( Lcol[ib].blockIdx > ksup ){
#if ( _DEBUGlevel_ >= 2 )
              statusOFS << std::endl << "["<<ksup<<"] "<<  "Serializing Block index " << Lcol[ib].blockIdx << std::endl;
#endif
              serialize( Lcol[ib], sstm, mask );
            }
          }
          sstrLcolSend.resize( Size( sstm ) );
          sstm.read( &sstrLcolSend[0], sstrLcolSend.size() );
          arrSstrLcolSizeSend[supidx] = sstrLcolSend.size();
          for( Int iProcCol = 0; iProcCol < grid_->numProcCol ; iProcCol++ ){
            if( MYCOL( grid_ ) != iProcCol &&
                isSendToRight_( iProcCol, ksup ) == true ){
              // Use Isend to send to multiple targets
              MPI_Isend( &arrSstrLcolSizeSend[supidx], 1, MPI_INT,  
                  iProcCol, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, 
                  grid_->rowComm, &mpireqsSendToRight[2*iProcCol] );
              MPI_Isend( (void*)&sstrLcolSend[0], arrSstrLcolSizeSend[supidx], MPI_BYTE, 
                  iProcCol, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, 
                  grid_->rowComm, &mpireqsSendToRight[2*iProcCol+1] );
#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<<  "Sending L " << arrSstrLcolSizeSend[supidx]<< " BYTES"  << std::endl <<  std::endl; 
#endif
            } // Send 
          } // for (iProcCol)
        } // if I am the sender
      } //Senders

      //TODO Ideally, we should not receive data in sequence but in any order with ksup packed with the data
      // Receivers (Size)
      for (Int supidx=0; supidx<stepSuper ; supidx++){
        Int ksup = superList[lidx][supidx];
        MPI_Request * mpireqsRecvFromAbove = &arrMpireqsRecvSizeFromAny[supidx*2];
        MPI_Request * mpireqsRecvFromLeft = &arrMpireqsRecvSizeFromAny[supidx*2+1];
        Int & sizeStmFromLeft = arrSizeStmFromLeft[supidx];
        Int & sizeStmFromAbove = arrSizeStmFromAbove[supidx];

        // Receive the size first
        if( isRecvFromAbove_( ksup ) && 
            MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          MPI_Irecv( &sizeStmFromAbove, 1, MPI_INT, PROW( ksup, grid_ ), 
              SELINV_TAG_COUNT*supidx+SELINV_TAG_U_SIZE,
              grid_->colComm, mpireqsRecvFromAbove );
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Receiving U size on tag " << (int)SELINV_TAG_COUNT*supidx+SELINV_TAG_U_SIZE<< std::endl <<  std::endl; 
#endif
        } // if I need to receive from up


        if( isRecvFromLeft_( ksup ) &&
            MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
          MPI_Irecv( &sizeStmFromLeft, 1, MPI_INT, PCOL( ksup, grid_ ), 
              SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE,
              grid_->rowComm, mpireqsRecvFromLeft );
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Receiving L size on tag " << (int)SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE<< std::endl <<  std::endl; 
#endif
        } // if I need to receive from left
      }

      //Wait to receive all the sizes
      {
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("WaitSize_UL");
#elif defined(PROFILE)
        TAU_FSTART(WaitSize_UL);
#endif
#endif
        mpi::Waitall(arrMpireqsRecvSizeFromAny);
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_STOP("WaitSize_UL");
#elif defined(PROFILE)
        TAU_FSTOP(WaitSize_UL);
#endif
#endif
      }






      // Receivers (Content)
      for (Int supidx=0; supidx<stepSuper ; supidx++){
        Int ksup = superList[lidx][supidx];

        MPI_Request * mpireqsRecvFromAbove = &arrMpireqsRecvContentFromAny[supidx*2];
        MPI_Request * mpireqsRecvFromLeft = &arrMpireqsRecvContentFromAny[supidx*2+1];

        std::vector<char> & sstrUrowRecv = arrSstrUrowRecv[supidx];
        std::vector<char> & sstrLcolRecv = arrSstrLcolRecv[supidx];

        Int & sizeStmFromLeft = arrSizeStmFromLeft[supidx];
        Int & sizeStmFromAbove = arrSizeStmFromAbove[supidx];

        if( isRecvFromAbove_( ksup ) && 
            MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          sstrUrowRecv.resize( sizeStmFromAbove );
          MPI_Irecv( &sstrUrowRecv[0], sizeStmFromAbove, MPI_BYTE, 
              PROW( ksup, grid_ ), SELINV_TAG_COUNT*supidx+SELINV_TAG_U_CONTENT, 
              grid_->colComm, mpireqsRecvFromAbove );
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Receiving U " << sizeStmFromAbove << " BYTES"<< std::endl <<  std::endl; 
#endif
        } // if I need to receive from up

        if( isRecvFromLeft_( ksup ) &&
            MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
          sstrLcolRecv.resize( sizeStmFromLeft );
          MPI_Irecv( &sstrLcolRecv[0], sizeStmFromLeft, MPI_BYTE, 
              PCOL( ksup, grid_ ), SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, 
              grid_->rowComm,
              mpireqsRecvFromLeft );
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Receiving L " << sizeStmFromLeft << " BYTES"<< std::endl <<  std::endl; 
#endif
        } // if I need to receive from left
      }


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Compute_Sinv_LT);
#endif
#endif

      Int gemmProcessed = 0;
      Int gemmToDo = 0;
      //copy the list of supernodes we need to process
      std::vector<Int> localSupidx;
      std::vector<Int> readySupidx;
      std::vector<Int> isReady(stepSuper,0);
      //find local things to do
      for(Int supidx = 0;supidx<stepSuper;supidx++){
        Int ksup = superList[lidx][supidx];
        if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup )){
          gemmToDo++;
          if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
            isReady[supidx]++;

          if(  MYROW( grid_ ) == PROW( ksup, grid_ ) )
            isReady[supidx]++;

          if(isReady[supidx]==2){
            readySupidx.push_back(supidx);
#if ( _DEBUGlevel_ >= 1 )
            statusOFS<<std::endl<<"Locally processing ["<<ksup<<"]"<<std::endl;
#endif
          }
        }

        else if( isRecvFromLeft_( ksup ) && MYCOL( grid_ ) != PCOL( ksup, grid_ ) )
        {
          //Dummy 0-b send If I was a receiver, I need to send my data to proc in column of ksup
          std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
          NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];
          MPI_Isend( LUpdateBuf.Data(), LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar), MPI_BYTE, PCOL(ksup,grid_) ,SELINV_TAG_COUNT*supidx+SELINV_TAG_L_REDUCE, grid_->rowComm, &mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYCOL(grid_)<<" has sent "<< LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar) << " bytes to " << PCOL(ksup,grid_) << std::endl;
#endif
        }// if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ))
      }

#if ( _DEBUGlevel_ >= 1 )
      statusOFS<<std::endl<<"gemmToDo ="<<gemmToDo<<std::endl;
      statusOFS<<std::endl<<"isReady ="<<isReady<<std::endl;
      statusOFS<<std::endl<<"readySupidx ="<<readySupidx<<std::endl;
#endif

#ifdef SELINV_TIMING
      end_SendULWaitContentFirst=0;
      begin_SendULWaitContentFirst=0;
#endif

      while(gemmProcessed<gemmToDo){
        Int reqidx = -1;
        Int supidx = -1;
        Int ksup = -1;
        std::vector<LBlock> LcolRecv;
        std::vector<UBlock> UrowRecv;

#if ( _DEBUGlevel_ >= 1 )
        statusOFS<<std::endl<<"gemmProcessed ="<<gemmProcessed<<std::endl;
#endif

        //while I don't have anything to do, wait for data to arrive 
        while(readySupidx.size()==0){
          //then process with the remote ones

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("WaitContent_UL");
#elif defined(PROFILE)
          TAU_FSTART(WaitContent_UL);
          if(begin_SendULWaitContentFirst==0){
            TAU_FSTART(WaitContent_UL_First);
          }
#endif
#endif

          MPI_Waitany(2*stepSuper, &arrMpireqsRecvContentFromAny[0], &reqidx, MPI_STATUS_IGNORE);

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("WaitContent_UL");
#elif defined(PROFILE)
          TAU_FSTOP(WaitContent_UL);
#endif
#endif

          //I've received something
          if(reqidx!=MPI_UNDEFINED)
          { 
            supidx = reqidx/2;
            isReady[supidx]++;

            ksup = superList[lidx][supidx];
#if ( _DEBUGlevel_ >= 1 )
            statusOFS<<std::endl<<"Received data for ["<<ksup<<"] reqidx%2="<<reqidx%2<<std::endl;
#endif
            //if we received both L and U, the supernode is ready
            if(isReady[supidx]==2){
              readySupidx.push_back(supidx);

#ifdef SELINV_TIMING
#if defined(PROFILE)
              if(end_SendULWaitContentFirst==0){
                TAU_FSTOP(WaitContent_UL_First);
              }
#endif
#endif
            }
          }

        }

        //If I have some work to do 
        if(readySupidx.size()>0){
          supidx = readySupidx.back();
          readySupidx.pop_back();
          ksup = superList[lidx][supidx];

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
          TAU_FSTART(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif

#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Unpack the received data for processors participate in Gemm. " << std::endl << std::endl; 
#endif

          std::vector<char> & sstrUrowRecv = arrSstrUrowRecv[supidx];
          std::vector<char> & sstrLcolRecv = arrSstrLcolRecv[supidx];
          Int & sizeStmFromLeft = arrSizeStmFromLeft[supidx];
          Int & sizeStmFromAbove = arrSizeStmFromAbove[supidx];
          NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];


          std::vector<LBlock> LcolRecv;
          std::vector<UBlock> UrowRecv;
          if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){
            // U part
            if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
              std::stringstream sstm;
              sstm.write( &sstrUrowRecv[0], sizeStmFromAbove );
              std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
              Int numUBlock;
              deserialize( numUBlock, sstm, NO_MASK );
              UrowRecv.resize( numUBlock );
              for( Int jb = 0; jb < numUBlock; jb++ ){
                deserialize( UrowRecv[jb], sstm, mask );
              } 
            } // sender is not the same as receiver
            else{
              // U is obtained locally, just make a copy. Include everything
              // (there is no diagonal block)
              UrowRecv = this->U( LBi( ksup, grid_ ) );
            } // sender is the same as receiver


            //L part
            if( MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
              std::stringstream     sstm;
              sstm.write( &sstrLcolRecv[0], sizeStmFromLeft );
              std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
              mask[LBlockMask::NZVAL] = 0; // nzval is excluded
              Int numLBlock;
              deserialize( numLBlock, sstm, NO_MASK );
              LcolRecv.resize( numLBlock );
              for( Int ib = 0; ib < numLBlock; ib++ ){
                deserialize( LcolRecv[ib], sstm, mask );
              }
            } // sender is not the same as receiver
            else{
              // L is obtained locally, just make a copy. 
              // Do not include the diagonal block
              std::vector<LBlock>& Lcol =  this->L( LBj( ksup, grid_ ) );
              if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
                LcolRecv.resize( Lcol.size() );
                for( Int ib = 0; ib < Lcol.size(); ib++ ){
                  LcolRecv[ib] = Lcol[ib];
                }
              }
              else{
                LcolRecv.resize( Lcol.size() - 1 );
                for( Int ib = 0; ib < Lcol.size() - 1; ib++ ){
                  LcolRecv[ib] = Lcol[ib+1];
                }
              }
            } // sender is the same as receiver
          } // if I am a receiver


#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "Main work: Gemm" << std::endl << std::endl; 
#endif

          // Save all the data to be updated for { L( isup, ksup ) | isup > ksup }.
          // The size will be updated in the Gemm phase and the reduce phase

          // Only the processors received information participate in the Gemm 
          if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){
            // rowPtr[ib] gives the row index in LUpdateBuf for the first
            // nonzero row in LcolRecv[ib]. The total number of rows in
            // LUpdateBuf is given by rowPtr[end]-1
            std::vector<Int> rowPtr(LcolRecv.size() + 1);
            // colPtr[jb] gives the column index in UBuf for the first
            // nonzero column in UrowRecv[jb]. The total number of rows in
            // UBuf is given by colPtr[end]-1
            std::vector<Int> colPtr(UrowRecv.size() + 1);

            rowPtr[0] = 0;
            for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
              rowPtr[ib+1] = rowPtr[ib] + LcolRecv[ib].numRow;
            }
            colPtr[0] = 0;
            for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
              colPtr[jb+1] = colPtr[jb] + UrowRecv[jb].numCol;
            }

            Int numRowAinvBuf = *rowPtr.rbegin();
            Int numColAinvBuf = *colPtr.rbegin();

#if ( _DEBUGlevel_ >= 2 )
            statusOFS << "["<<ksup<<"] "<<  "AinvBuf ~ " << numRowAinvBuf << " x " << numColAinvBuf << std::endl;
            statusOFS << "["<<ksup<<"] "<<  "rowPtr:" << std::endl << rowPtr << std::endl;
            statusOFS << "["<<ksup<<"] "<<  "colPtr:" << std::endl << colPtr << std::endl;
#endif
            // Allocate for the computational storage
            NumMat<Scalar> AinvBuf( numRowAinvBuf, numColAinvBuf );


            LUpdateBuf.Resize( numRowAinvBuf, SuperSize( ksup, super_ ) );
            NumMat<Scalar> UBuf( SuperSize( ksup, super_ ), numColAinvBuf );
            SetValue( AinvBuf, SCALAR_ZERO );
            SetValue( LUpdateBuf, SCALAR_ZERO );
            SetValue( UBuf, SCALAR_ZERO );

            // Fill UBuf first.  Make the transpose later in the Gemm phase.
            for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
              UBlock& UB = UrowRecv[jb];
              if( UB.numRow != SuperSize(ksup, super_) ){
                throw std::logic_error( "The size of UB is not right.  Something is seriously wrong." );
              }
              lapack::Lacpy( 'A', UB.numRow, UB.numCol, UB.nzval.Data(),
                  UB.numRow, UBuf.VecData( colPtr[jb] ), UBuf.m() );	
            }

            // Calculate the relative indices for (isup, jsup)
            // Fill AinvBuf with the information in L or U block.
            for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
              for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
                LBlock& LB = LcolRecv[ib];
                UBlock& UB = UrowRecv[jb];
                Int isup = LB.blockIdx;
                Int jsup = UB.blockIdx;
                Scalar* nzvalAinv = &AinvBuf( rowPtr[ib], colPtr[jb] );
                Int     ldAinv    = AinvBuf.m();

#if ( _DEBUGlevel_ >= 2 )
                statusOFS << std::endl << "["<<ksup<<"] "<< "isup = "<<isup<< " LB["<<ib<<"]: " << LB << std::endl;
                statusOFS << std::endl << "["<<ksup<<"] "<< "jsup = "<<jsup<< " UB["<<jb<<"]: " << UB << std::endl;
#endif
                // Pin down the corresponding block in the part of Sinv.
                if( isup >= jsup ){
                  std::vector<LBlock>&  LcolSinv = this->L( LBj(jsup, grid_ ) );
                  bool isBlockFound = false;
                  for( Int ibSinv = 0; ibSinv < LcolSinv.size(); ibSinv++ ){
                    // Found the (isup, jsup) block in Sinv
                    if( LcolSinv[ibSinv].blockIdx == isup ){
                      LBlock& SinvB = LcolSinv[ibSinv];
#if ( _DEBUGlevel_ >= 2 )
                      //                      statusOFS << std::endl << "["<<ksup<<"] "<< "LBj "<< LBj(jsup,grid_) << " LcolSinv["<<ibSinv<<"]: " << LcolSinv[ibSinv] << std::endl;
#endif

                      // Row relative indices
                      std::vector<Int> relRows( LB.numRow );
                      Int* rowsLBPtr    = LB.rows.Data();
                      Int* rowsSinvBPtr = SinvB.rows.Data();
                      for( Int i = 0; i < LB.numRow; i++ ){
                        bool isRowFound = false;
                        for( Int i1 = 0; i1 < SinvB.numRow; i1++ ){
                          if( rowsLBPtr[i] == rowsSinvBPtr[i1] ){
                            isRowFound = true;
                            relRows[i] = i1;
                            break;
                          }
                        }
                        if( isRowFound == false ){
                          std::ostringstream msg;
                          msg << "Row " << rowsLBPtr[i] << 
                            " in LB cannot find the corresponding row in SinvB" << std::endl
                            << "LB.rows    = " << LB.rows << std::endl
                            << "SinvB.rows = " << SinvB.rows << std::endl;
                          throw std::runtime_error( msg.str().c_str() );
                        }
                      }

                      // Column relative indicies
                      std::vector<Int> relCols( UB.numCol );
                      Int SinvColsSta = FirstBlockCol( jsup, super_ );
                      for( Int j = 0; j < UB.numCol; j++ ){
                        relCols[j] = UB.cols[j] - SinvColsSta;
                      }

                      // Transfer the values from Sinv to AinvBlock
                      Scalar* nzvalSinv = SinvB.nzval.Data();
                      Int     ldSinv    = SinvB.numRow;
                      for( Int j = 0; j < UB.numCol; j++ ){
                        for( Int i = 0; i < LB.numRow; i++ ){
                          nzvalAinv[i+j*ldAinv] =
                            nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                        }
                      }

                      isBlockFound = true;
                      break;
                    }	
                  } // for (ibSinv )
                  if( isBlockFound == false ){
                    std::ostringstream msg;
                    msg << "Block(" << isup << ", " << jsup 
                      << ") did not find a matching block in Sinv." << std::endl;
                    throw std::runtime_error( msg.str().c_str() );
                  }
                } // if (isup, jsup) is in L
                else{
                  std::vector<UBlock>&   UrowSinv = this->U( LBi( isup, grid_ ) );
                  bool isBlockFound = false;
                  for( Int jbSinv = 0; jbSinv < UrowSinv.size(); jbSinv++ ){
                    // Found the (isup, jsup) block in Sinv
                    if( UrowSinv[jbSinv].blockIdx == jsup ){
                      UBlock& SinvB = UrowSinv[jbSinv];
#if ( _DEBUGlevel_ >= 2 )
                      //                      statusOFS << std::endl << "["<<ksup<<"] "<<  "UrowSinv["<<jbSinv<<"]: " << UrowSinv[jbSinv] << std::endl;
#endif

                      // Row relative indices
                      std::vector<Int> relRows( LB.numRow );
                      Int SinvRowsSta = FirstBlockCol( isup, super_ );
                      for( Int i = 0; i < LB.numRow; i++ ){
                        relRows[i] = LB.rows[i] - SinvRowsSta;
                      }

                      // Column relative indices
                      std::vector<Int> relCols( UB.numCol );
                      Int* colsUBPtr    = UB.cols.Data();
                      Int* colsSinvBPtr = SinvB.cols.Data();
                      for( Int j = 0; j < UB.numCol; j++ ){
                        bool isColFound = false;
                        for( Int j1 = 0; j1 < SinvB.numCol; j1++ ){
                          if( colsUBPtr[j] == colsSinvBPtr[j1] ){
                            isColFound = true;
                            relCols[j] = j1;
                            break;
                          }
                        }
                        if( isColFound == false ){
                          std::ostringstream msg;
                          msg << "Col " << colsUBPtr[j] << 
                            " in UB cannot find the corresponding row in SinvB" << std::endl
                            << "UB.cols    = " << UB.cols << std::endl
                            << "UinvB.cols = " << SinvB.cols << std::endl;
                          throw std::runtime_error( msg.str().c_str() );
                        }
                      }

                      // Transfer the values from Sinv to AinvBlock
                      Scalar* nzvalSinv = SinvB.nzval.Data();
                      Int     ldSinv    = SinvB.numRow;
                      for( Int j = 0; j < UB.numCol; j++ ){
                        for( Int i = 0; i < LB.numRow; i++ ){
                          nzvalAinv[i+j*ldAinv] =
                            nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                        }
                      }

                      isBlockFound = true;
                      break;
                    }
                  } // for (jbSinv)
                  if( isBlockFound == false ){
                    std::ostringstream msg;
                    msg << "Block(" << isup << ", " << jsup 
                      << ") did not find a matching block in Sinv." << std::endl;
                    throw std::runtime_error( msg.str().c_str() );
                  }
                } // if (isup, jsup) is in U

              } // for( ib )
            } // for ( jb )

#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<  "AinvBuf: " << AinvBuf << std::endl;
            statusOFS << std::endl << "["<<ksup<<"] "<<  "UBuf   : " << UBuf << std::endl;
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
            TAU_STOP("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
            TAU_FSTOP(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
            TAU_START("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
            TAU_FSTART(Compute_Sinv_LT_GEMM);
#endif
#endif

            // Gemm for LUpdateBuf = -AinvBuf * UBuf^T
            blas::Gemm( 'N', 'T', AinvBuf.m(), UBuf.m(), AinvBuf.n(), SCALAR_MINUS_ONE, 
                AinvBuf.Data(), AinvBuf.m(), 
                UBuf.Data(), UBuf.m(), SCALAR_ZERO,
                LUpdateBuf.Data(), LUpdateBuf.m() ); 
#ifdef SELINV_TIMING
#ifdef USE_TAU
            TAU_STOP("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
            TAU_FSTOP(Compute_Sinv_LT_GEMM);
#endif
#endif

#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<  "LUpdateBuf: " << LUpdateBuf << std::endl;
#endif
          } // if Gemm is to be done locally


          //If I was a receiver, I need to send my data to proc in column of ksup
          if( isRecvFromLeft_( ksup ) && MYCOL( grid_ ) != PCOL( ksup, grid_ ) )
          {
            std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
            MPI_Isend( LUpdateBuf.Data(), LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar), MPI_BYTE, PCOL(ksup,grid_) ,SELINV_TAG_COUNT*supidx+SELINV_TAG_L_REDUCE, grid_->rowComm, &mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
            statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYCOL(grid_)<<" has sent "<< LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar) << " bytes to " << PCOL(ksup,grid_) << std::endl;
#endif
          }//Sender
          gemmProcessed++;
        }
      }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Compute_Sinv_LT);
#endif
#endif




      //Reduce Sinv L^T to the processors in PCOL(ksup,grid_)
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Sinv_LT);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

        NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

          //determine the number of rows in LUpdateBufReduced
          std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
          std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
          Int numRowLUpdateBuf;
          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            rowLocalPtr.resize( Lcol.size() + 1 );
            blockIdxLocal.resize( Lcol.size() );
            rowLocalPtr[0] = 0;
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib+1] = rowLocalPtr[ib] + Lcol[ib].numRow;
              blockIdxLocal[ib] = Lcol[ib].blockIdx;
            }
          } // I do not own the diagonal block
          else{
            rowLocalPtr.resize( Lcol.size() );
            blockIdxLocal.resize( Lcol.size() - 1 );
            rowLocalPtr[0] = 0;
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib] = rowLocalPtr[ib-1] + Lcol[ib].numRow;
              blockIdxLocal[ib-1] = Lcol[ib].blockIdx;
            }
          } // I own the diagonal block, skip the diagonal block
          numRowLUpdateBuf = *rowLocalPtr.rbegin();


          NumMat<Scalar>  LUpdateBufRecv(numRowLUpdateBuf,SuperSize( ksup, super_ ) );
          NumMat<Scalar> & LUpdateBufReduced = LUpdateBuf;
          if( numRowLUpdateBuf > 0 ){
            if( LUpdateBuf.m() == 0 && LUpdateBuf.n() == 0 ){
              LUpdateBufReduced.Resize( numRowLUpdateBuf,SuperSize( ksup, super_ ) );
              // Fill zero is important
              SetValue( LUpdateBufReduced, SCALAR_ZERO );
            }
          }

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBuf Before Reduction: " <<  LUpdateBufReduced << std::endl << std::endl; 
#endif

          Int totCountRecv = 0;
          Int numRecv = CountSendToRight(ksup);
          for( Int countRecv = 0; countRecv < numRecv ; ++countRecv ){
            //Do the blocking recv
            MPI_Status stat;
            Int size = 0;
            MPI_Recv(LUpdateBufRecv.Data(), numRowLUpdateBuf*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, MPI_ANY_SOURCE,SELINV_TAG_COUNT*supidx+SELINV_TAG_L_REDUCE, grid_->rowComm,&stat);
            MPI_Get_count(&stat, MPI_BYTE, &size);
            //if the processor contributes
            if(size>0){

#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYCOL(grid_)<<" has received "<< size << " bytes from " << stat.MPI_SOURCE << std::endl;
#endif
#if ( _DEBUGlevel_ >= 2 )
              statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufRecv: " <<  LUpdateBufRecv << std::endl << std::endl; 
#endif



              //do the sum
              blas::Axpy(numRowLUpdateBuf*SuperSize( ksup, super_ ), SCALAR_ONE, LUpdateBufRecv.Data(),
                  1, LUpdateBufReduced.Data(), 1 );
            }
          } // for (iProcCol)

#if ( _DEBUGlevel_ >= 2 ) 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufReduced: " <<  LUpdateBufReduced << std::endl << std::endl; 
#endif

#ifdef SEND_CROSSDIAG_ASYNC
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Send_L_CrossDiag);
#endif
#endif


          // Send LUpdateBufReduced to the cross diagonal blocks. 
          // NOTE: This assumes square processor grid
          if( isSendToCrossDiagonal_( ksup ) ){
            Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
            if( MYPROC( grid_ ) != dest	){
              std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
              std::stringstream sstm;
              std::vector<char> & sstr = arrSstrCrossDiag[supidx];
              Int & sizeStm = arrSstrSizeCrossDiag[supidx];
              serialize( rowLocalPtr, sstm, NO_MASK );
              serialize( blockIdxLocal, sstm, NO_MASK );
              serialize( LUpdateBufReduced, sstm, NO_MASK );
              mpi::Isend( sstm, sstr, sizeStm, dest, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm, mpireqsSendToBelow[0], mpireqsSendToBelow[1]);
            }
          } // sender to cross diagonal
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif
#endif
        } // Receiver
      }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Sinv_LT);
#endif
#endif



      //--------------------- End of reduce of LUpdateBuf-------------------------

#if ( _DEBUGlevel_ >= 1 )
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && LUpdateBufReduced.m() > 0 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufReduced: " << LUpdateBufReduced << std::endl << std::endl; 
      }
#endif

#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateD");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Update_Diagonal);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<   "Update the diagonal block" << std::endl << std::endl; 
#endif
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

          //---------Computing  Diagonal block, all processors in the column are participating to all pipelined supernodes

          std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
          std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
          NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
          Int numRowLUpdateBuf = LUpdateBufReduced.m();


          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );

          //Allocate DiagBuf even if Lcol.size() == 0
          NumMat<Scalar> & DiagBuf = arrDiagBuf[supidx];
          DiagBuf.Resize(SuperSize( ksup, super_ ), SuperSize( ksup, super_ ));
          SetValue(DiagBuf, SCALAR_ZERO);

          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                  SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib], 0 ), LUpdateBufReduced.m(),
                  Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                  SCALAR_ONE, DiagBuf.Data(), SuperSize( ksup, super_ ) );
            }

            if(isSendToDiagonal_(ksup)){
              //send to above
              std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
              MPI_Isend( DiagBuf.Data(),  SuperSize( ksup, super_ )*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, PROW(ksup,grid_) ,SELINV_TAG_COUNT*supidx+SELINV_TAG_D_REDUCE, grid_->colComm, &mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" has sent "<< DiagBuf.m()*DiagBuf.n()*sizeof(Scalar) << " bytes of DiagBuf to " << PROW(ksup,grid_) << " isSendToDiagonal = "<< isSendToDiagonal_(ksup) <<  std::endl;
#endif
            }

          } // I do not own the diagonal block
          else{
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                  SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ), LUpdateBufReduced.m(),
                  Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                  SCALAR_ONE, DiagBuf.Data(), SuperSize( ksup, super_ ) );
            }
          } // I own the diagonal block, skip the diagonal block
        }
      }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Update_Diagonal);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Diagonal);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){


          NumMat<Scalar> & DiagBuf = arrDiagBuf[supidx];
#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBuf: " << DiagBuf << std::endl << std::endl; 
#endif

          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
            //receive from below
            Int totCountRecv = 0;
            Int numRecv = CountRecvFromBelow(ksup);

#if ( _DEBUGlevel_ >= 1 )
            statusOFS << std::endl << "["<<ksup<<"] "<< "P"<<MYROW(grid_)<<" should receive DiagBuf from "<< numRecv << " processors" << std::endl;
#endif
            if(DiagBuf.m()==0 && DiagBuf.n()==0){
              DiagBuf.Resize( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ));
              SetValue(DiagBuf, SCALAR_ZERO);
            }

            NumMat<Scalar>  DiagBufRecv(DiagBuf.m(),DiagBuf.n());

            for( Int countRecv = 0; countRecv < numRecv ; ++countRecv ){
              //Do the blocking recv
              MPI_Status stat;
              Int size = 0;
              MPI_Recv(DiagBufRecv.Data(), SuperSize( ksup, super_ )*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, MPI_ANY_SOURCE,SELINV_TAG_COUNT*supidx+SELINV_TAG_D_REDUCE, grid_->colComm,&stat);
              MPI_Get_count(&stat, MPI_BYTE, &size);
#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< "P"<<MYROW(grid_)<<" has received "<< size << " bytes of DiagBuf from " << stat.MPI_SOURCE << std::endl;
#endif
              //if the processor contributes
              if(size>0){
#if ( _DEBUGlevel_ >= 2 )
                statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBufRecv: " <<  DiagBufRecv << std::endl << std::endl; 
#endif
                //do the sum
                blas::Axpy(SuperSize( ksup, super_ )*SuperSize( ksup, super_ ), SCALAR_ONE, DiagBufRecv.Data(),
                    1, DiagBuf.Data(), 1 );
              }
            }
            // Add DiagBufReduced to diagonal block.
#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBufReduced: " << DiagBuf << std::endl << std::endl; 
#endif
            LBlock&  LB = this->L( LBj( ksup, grid_ ) )[0];
            // Symmetrize LB
            blas::Axpy( LB.numRow * LB.numCol, SCALAR_ONE, DiagBuf.Data(),
                1, LB.nzval.Data(), 1 );
            Symmetrize( LB.nzval );
#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<   "Diag of Ainv: " << LB.nzval << std::endl << std::endl; 
#endif
          }
        } 
      }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Diagonal);
#endif
#endif




#ifdef SELINV_TIMING
#if not ( defined (USE_TAU) || defined (PROFILE) || defined(PMPI) )
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        //              if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        //                MPI_Barrier(grid_->colComm);
        //#if ( _DEBUGlevel_ >= 1 )
        //                statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" is done sending DiagBuf to P" << PROW(ksup,grid_) <<  std::endl;
        //#endif
        //              }
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && isSendToDiagonal_(ksup) ){
          // Now all the Isend / Irecv should have finished.
          std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
          mpi::Wait( mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" is done sending DiagBuf to P" << PROW(ksup,grid_) <<  std::endl;
#endif
        }
      }

#ifdef SELINV_TIMING
#if not ( defined (USE_TAU) || defined (PROFILE) || defined(PMPI) )
#endif
#endif




#ifndef _RELEASE_
      PopCallStack();
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateU");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_U");
#elif defined(PROFILE)
      TAU_FSTART(Update_U);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl <<  "["<<ksup<<"] "<<  "Update the upper triangular block" << std::endl << std::endl; 
#endif
        // Send LUpdateBufReduced to the cross diagonal blocks. 
        // NOTE: This assumes square processor grid
        std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
        std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        Int numRowLUpdateBuf = LUpdateBufReduced.m();

#if not (defined(SEND_CROSSDIAG_ASYNC)) 
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTART(Send_L_CrossDiag);
#endif
#endif
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && isSendToCrossDiagonal_( ksup ) ){
          Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
          if( MYPROC( grid_ ) != dest	){
            std::stringstream sstm;
            serialize( rowLocalPtr, sstm, NO_MASK );
            serialize( blockIdxLocal, sstm, NO_MASK );
            serialize( LUpdateBufReduced, sstm, NO_MASK );
            mpi::Send( sstm, dest, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm );
          }
        } // sender
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif
#endif

        if( MYROW( grid_ ) == PROW( ksup, grid_ ) && isRecvFromCrossDiagonal_( ksup ) ){
          std::vector<Int>  rowLocalPtrRecv;
          std::vector<Int>  blockIdxLocalRecv;
          NumMat<Scalar> UUpdateBuf;
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Recv_L_CrossDiag);
#endif
#endif

          Int src = PNUM( MYCOL( grid_ ), PCOL( ksup, grid_ ), grid_ );
          if( MYPROC( grid_ ) != src ){
            std::stringstream sstm;
            mpi::Recv( sstm, src, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm );

            deserialize( rowLocalPtrRecv, sstm, NO_MASK );
            deserialize( blockIdxLocalRecv, sstm, NO_MASK );
            deserialize( UUpdateBuf, sstm, NO_MASK );	
          } // sender is not the same as receiver
          else{
            rowLocalPtrRecv   = rowLocalPtr;
            blockIdxLocalRecv = blockIdxLocal;
            UUpdateBuf = LUpdateBufReduced;
          } // sender is the same as receiver

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Recv_L_CrossDiag);
#endif
#endif

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "rowLocalPtrRecv:" << rowLocalPtrRecv << std::endl << std::endl; 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "blockIdxLocalRecv:" << blockIdxLocalRecv << std::endl << std::endl; 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "UUpdateBuf:" << UUpdateBuf << std::endl << std::endl; 
#endif

          // Update U
          std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
          Int cntRow = 0;
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            UBlock& UB = Urow[jb];
            bool isBlockFound = false;
            NumMat<Scalar> Ltmp( UB.numCol, UB.numRow );
            for( Int ib = 0; ib < blockIdxLocalRecv.size(); ib++ ){
              if( UB.blockIdx == blockIdxLocalRecv[ib] ){
                lapack::Lacpy( 'A', Ltmp.m(), Ltmp.n(), 
                    &UUpdateBuf( rowLocalPtrRecv[ib], 0 ),
                    UUpdateBuf.m(), Ltmp.Data(), Ltmp.m() );
                cntRow += UB.numCol;
                isBlockFound = true;
              }
            }
            if( isBlockFound == false ){
              throw std::logic_error( "UBlock cannot find its update. Something is seriously wrong." );
            }
            Transpose( Ltmp, UB.nzval );
          } // for (jb)
          if( cntRow != UUpdateBuf.m() ){
            std::ostringstream msg;
            msg << "The number of rows received from L is " << UUpdateBuf.m()
              << ", which does not match the total number of columns in U which is "
              << cntRow <<  std::endl;
            throw std::runtime_error( msg.str().c_str() );
          }
        } // receiver
      }


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_U");
#elif defined(PROFILE)
      TAU_FSTOP(Update_U);
#endif
#endif

#ifndef _RELEASE_
      PopCallStack();
#endif

#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateLFinal");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_L");
#elif defined(PROFILE)
      TAU_FSTART(Update_L);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<  "Finish updating the L part by filling LUpdateBufReduced back to L" << std::endl << std::endl; 
#endif
        std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
        std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        Int numRowLUpdateBuf = LUpdateBufReduced.m();

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && numRowLUpdateBuf > 0 ){
          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              LBlock& LB = Lcol[ib];
              lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib], 0 ),
                  LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
            }
          } // I do not own the diagonal block
          else{
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              LBlock& LB = Lcol[ib];
              lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ),
                  LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
            }
          } // I owns the diagonal block
        } // Finish updating L	
      } // for (ksup) : Main loop


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_L");
#elif defined(PROFILE)
      TAU_FSTOP(Update_L);
#endif
#endif

#ifndef _RELEASE_
      PopCallStack();
#endif


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Barrier");
#elif defined(PROFILE)
      TAU_FSTART(Barrier);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
        std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
        mpi::Waitall( mpireqsSendToRight );
        mpi::Waitall( mpireqsSendToBelow );


        //Barrier in the column required because of the reduction of Diagonal block = L^T Sinv L
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          MPI_Barrier(grid_->colComm);
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" is done sending DiagBuf to P" << PROW(ksup,grid_) <<  std::endl;
#endif
        }     
      }
      mpi::Waitall( arrMpireqsRecvContentFromAny );
      mpi::Waitall( arrMpireqsRecvSizeFromAny );


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Barrier");
#elif defined(PROFILE)
      TAU_FSTOP(Barrier);
#endif
#endif



#ifdef SELINV_MEMORY
#ifndef USE_TAU
      Real localBufSize = 0;

      localBufSize += sizeof(arrMpireqsSendToBelow) + arrMpireqsSendToBelow.capacity()*sizeof(std::vector<MPI_Request>);
      localBufSize += sizeof(arrMpireqsSendToRight) + arrMpireqsSendToRight.capacity()*sizeof(std::vector<MPI_Request>);
      localBufSize += sizeof(arrMpireqsRecvSizeFromAny) + arrMpireqsRecvSizeFromAny.capacity()*sizeof(MPI_Request);
      localBufSize += sizeof(arrMpireqsRecvContentFromAny) + arrMpireqsRecvContentFromAny.capacity()*sizeof(MPI_Request);
      localBufSize += sizeof(arrLUpdateBuf) + arrLUpdateBuf.capacity()*sizeof(NumMat<Scalar>);
      localBufSize += sizeof(arrDiagBuf) + arrDiagBuf.capacity()*sizeof(NumMat<Scalar>);

      localBufSize += sizeof(arrSstrLcolSend) + arrSstrLcolSend.capacity()*sizeof(std::vector<char>);
      localBufSize += sizeof(arrSstrUrowSend) + arrSstrUrowSend.capacity()*sizeof(std::vector<char>);
      localBufSize += sizeof(arrSstrLcolRecv) + arrSstrLcolRecv.capacity()*sizeof(std::vector<char>);
      localBufSize += sizeof(arrSstrUrowRecv) + arrSstrUrowRecv.capacity()*sizeof(std::vector<char>);
      localBufSize += sizeof(arrSstrCrossDiag) + arrSstrCrossDiag.capacity()*sizeof(std::vector<char>);

      localBufSize += sizeof(arrRowLocalPtr) + arrRowLocalPtr.capacity()*sizeof(std::vector<Int>);
      localBufSize += sizeof(arrBlockIdxLocal) + arrBlockIdxLocal.capacity()*sizeof(std::vector<Int>);

      localBufSize += sizeof(arrSstrLcolSizeSend) + arrSstrLcolSizeSend.capacity()*sizeof(Int);
      localBufSize += sizeof(arrSstrUrowSizeSend) + arrSstrUrowSizeSend.capacity()*sizeof(Int);
      localBufSize += sizeof(arrSstrSizeCrossDiag) + arrSstrSizeCrossDiag.capacity()*sizeof(Int);

      localBufSize += sizeof(arrSizeStmFromAbove) + arrSizeStmFromAbove.capacity()*sizeof(Int);
      localBufSize += sizeof(arrSizeStmFromLeft) + arrSizeStmFromLeft.capacity()*sizeof(Int);
      for (Int supidx=0; supidx<stepSuper; supidx++){
        localBufSize+= arrMpireqsSendToBelow[supidx].capacity()*sizeof(MPI_Request);
        localBufSize+= arrMpireqsSendToRight[supidx].capacity()*sizeof(MPI_Request);

        localBufSize+= arrLUpdateBuf[supidx].m()*arrLUpdateBuf[supidx].n()*sizeof(Scalar);
        localBufSize+= arrDiagBuf[supidx].m()*arrDiagBuf[supidx].n()*sizeof(Scalar);


        localBufSize+= arrSstrLcolSend[supidx].capacity()*sizeof(char);
        localBufSize+= arrSstrUrowSend[supidx].capacity()*sizeof(char);
        localBufSize+= arrSstrLcolRecv[supidx].capacity()*sizeof(char);
        localBufSize+= arrSstrUrowRecv[supidx].capacity()*sizeof(char);
        localBufSize+= arrSstrCrossDiag[supidx].capacity()*sizeof(char);

        localBufSize+= arrRowLocalPtr[supidx].capacity()*sizeof(Int);
        localBufSize+= arrBlockIdxLocal[supidx].capacity()*sizeof(Int);
      }
      globalBufSize = std::max(globalBufSize,localBufSize);
#endif
#endif




    }

#ifndef _RELEASE_
    PopCallStack();
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_STOP("SelInv");
#elif defined(PROFILE)
    TAU_FSTOP(SelInv);
#endif
#endif



#ifdef SELINV_MEMORY
#ifdef USE_TAU
    TAU_DISABLE_TRACKING_MEMORY();
#else
    globalBufSize += sizeof(superList) + superList.capacity()*sizeof(std::vector<Int>);
    for (Int lidx=0; lidx<numSteps ; lidx++){
      globalBufSize += superList[lidx].capacity()*sizeof(Int);
    }
    statusOFS<<std::endl<<"Maximum allocated buffer size is : "<< globalBufSize << " Bytes" <<std::endl;
#endif
#endif
    return ;
  } 		// -----  end of method PMatrix::SelInv  ----- 




#ifdef USE_BCAST_UL


  void PMatrix::DestructCommunicationPattern_Bcast	(  )
  {
#ifndef _RELEASE_
    PushCallStack("Destructing communicators");
#endif
    {
      for(int i = 0; i< commSendToBelow_.size(); ++i){
        MPI_Comm_free(&commSendToBelow_[i]);
      }

      for(int i = 0; i< commSendToRight_.size(); ++i){
        MPI_Comm_free(&commSendToRight_[i]);
      }
    }
#ifndef _RELEASE_
    PopCallStack();
#endif


  }

  void PMatrix::ConstructCommunicationPattern_Bcast	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::ConstructCommunicationPattern");
#endif
    Int numSuper = this->NumSuper();
#ifndef _RELEASE_
    PushCallStack( "Initialize the communication pattern" );
#endif
    isSendToBelow_.Resize(grid_->numProcRow, numSuper);
    isSendToRight_.Resize(grid_->numProcCol, numSuper);
    isSendToCrossDiagonal_.Resize( numSuper );
    isSendToDiagonal_.Resize( numSuper );
    SetValue( isSendToBelow_, false );
    SetValue( isSendToRight_, false );
    SetValue( isSendToCrossDiagonal_, false );
    SetValue( isSendToDiagonal_, false );

    isRecvFromAbove_.Resize( numSuper );
    isRecvFromLeft_.Resize( numSuper );
    isRecvFromCrossDiagonal_.Resize( numSuper );
    isRecvFromBelow_.Resize( grid_->numProcCol, numSuper );
    SetValue( isRecvFromAbove_, false );
    SetValue( isRecvFromBelow_, false );
    SetValue( isRecvFromLeft_, false );
    SetValue( isRecvFromCrossDiagonal_, false );

    countSendToBelow_.Resize(numSuper);
    countSendToRight_.Resize(numSuper);
    countRecvFromBelow_.Resize( numSuper );
    SetValue( countSendToBelow_, 0 );
    SetValue( countSendToRight_, 0 );
    SetValue( countRecvFromBelow_, 0 );

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local column communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local column communication" << std::endl;
#endif
    // localColBlockRowIdx stores the nonzero block indices for each local block column.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockCol x numNonzeroBlock
    std::vector<std::set<Int> >   localColBlockRowIdx;

    localColBlockRowIdx.resize( this->NumLocalBlockCol() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<Int>  tBlockRowIdx;
        tBlockRowIdx.clear();

        // L part
        std::vector<LBlock>& Lcol = this->L( LBj(ksup, grid_) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          tBlockRowIdx.push_back( Lcol[ib].blockIdx );
        }

        // U part
        for( Int ib = 0; ib < this->NumLocalBlockRow(); ib++ ){
          std::vector<UBlock>& Urow = this->U(ib);
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            if( Urow[jb].blockIdx == ksup ){
              tBlockRowIdx.push_back( GBi( ib, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockRowIdx;
        mpi::Allgatherv( tBlockRowIdx, tAllBlockRowIdx, grid_->colComm );

        localColBlockRowIdx[LBj( ksup, grid_ )].insert(
            tAllBlockRowIdx.begin(), tAllBlockRowIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Column block " << ksup 
          << " has the following nonzero block rows" << std::endl;
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for(ksup)


#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local row communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local row communication" << std::endl;
#endif
    // localRowBlockColIdx stores the nonzero block indices for each local block row.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockRow x numNonzeroBlock
    std::vector<std::set<Int> >   localRowBlockColIdx;

    localRowBlockColIdx.resize( this->NumLocalBlockRow() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<Int>  tBlockColIdx;
        tBlockColIdx.clear();

        // U part
        std::vector<UBlock>& Urow = this->U( LBi(ksup, grid_) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          tBlockColIdx.push_back( Urow[jb].blockIdx );
        }

        // L part
        for( Int jb = 0; jb < this->NumLocalBlockCol(); jb++ ){
          std::vector<LBlock>& Lcol = this->L(jb);
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            if( Lcol[ib].blockIdx == ksup ){
              tBlockColIdx.push_back( GBj( jb, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockColIdx;
        mpi::Allgatherv( tBlockColIdx, tAllBlockColIdx, grid_->rowComm );

        localRowBlockColIdx[LBi( ksup, grid_ )].insert(
            tAllBlockColIdx.begin(), tAllBlockColIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Row block " << ksup 
          << " has the following nonzero block columns" << std::endl;
        for( std::set<Int>::iterator si = localRowBlockColIdx[LBi( ksup, grid_ )].begin();
            si != localRowBlockColIdx[LBi( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for(ksup)

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack("SendToBelow / RecvFromAbove");
#endif

    MPI_Group baseColGrp;
    MPI_Comm_group(grid_->colComm, &baseColGrp);


    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes to the right of ksup

      NumVec<bool> sTB(grid_->numProcRow);
      SetValue(sTB,false);

      for( Int jsup = ksup + 1; jsup < numSuper; jsup++ ){
        Int jsupLocalBlockCol = LBj( jsup, grid_ );
        Int jsupProcCol = PCOL( jsup, grid_ );
        if( MYCOL( grid_ ) == jsupProcCol ){

          // SendToBelow / RecvFromAbove only if (ksup, jsup) is nonzero.
          if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 ) {
            for( std::set<Int>::iterator si = localColBlockRowIdx[jsupLocalBlockCol].begin();
                si != localColBlockRowIdx[jsupLocalBlockCol].end(); si++	 ){
              Int isup = *si;
              Int isupProcRow = PROW( isup, grid_ );
              if( isup > ksup ){
                if( MYROW( grid_ ) == isupProcRow ){
                  isRecvFromAbove_(ksup) = true;
                }
                if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
                  isSendToBelow_( isupProcRow, ksup ) = true;
                }
              } // if( isup > ksup )
            } // for (si)
          } // if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 )

          sTB( PROW(ksup,grid_) ) = true;
          if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 ) {
            for( std::set<Int>::iterator si = localColBlockRowIdx[jsupLocalBlockCol].begin();
                si != localColBlockRowIdx[jsupLocalBlockCol].end(); si++	 ){
              Int isup = *si;
              Int isupProcRow = PROW( isup, grid_ );
              if( isup > ksup ){
                sTB( isupProcRow ) = true;
              } // if( isup > ksup )
            } // for (si)
          }
        } // if( MYCOL( grid_ ) == PCOL( jsup, grid_ ) )

      } // for(jsup)

#if ( _DEBUGlevel_ >= 1 )
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        statusOFS<<"DEBUGMAT ["<<ksup<<"] sTB is: "; for(int curi=0; curi<sTB.m();curi++){statusOFS<<sTB(curi)<<" ";} statusOFS<<std::endl;
        statusOFS<<"DEBUGMAT ["<<ksup<<"] isSendToBelow_ is: "; for(int curi=0; curi<sTB.m();curi++){statusOFS<<isSendToBelow_(curi,ksup)<<" ";} statusOFS<<std::endl;
      }
#endif
      std::vector<bool> mask( sTB.Data(), sTB.Data() + sTB.m() );
      Int count= std::count(mask.begin(), mask.end(), true);
      Int color = mask[MYROW(grid_)];
      if(count>1){
        std::vector<Int> & snodeList = maskSendToBelow_[mask];
        snodeList.push_back(ksup);
      }
      countSendToBelow_(ksup) = count * color;
    } // for(ksup)

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToBelow:" << isSendToBelow_ << std::endl;


    statusOFS << std::endl << "isSendToBelow:" << std::endl;
    for(int j = 0;j< isSendToBelow_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToBelow_.m();i++){
        statusOFS<< isSendToBelow_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }



    statusOFS << std::endl << "isRecvFromAbove:" << isRecvFromAbove_ << std::endl;
#endif
#ifdef PRINT_COMMUNICATOR_STAT
    {
      statusOFS << std::endl << "countSendToBelow:" << countSendToBelow_ << std::endl;
      statusOFS << std::endl << "DEBUGMAT maskSendToBelow_:" << maskSendToBelow_.size() <<std::endl; 
      bitMaskSet::iterator it;
      for(it = maskSendToBelow_.begin(); it != maskSendToBelow_.end(); it++){
        //print the involved processors
        for(int curi = 0; curi < it->first.size(); curi++){
          statusOFS << it->first[curi] << " "; 
        }

        statusOFS<< "    ( ";
        //print the involved supernode indexes
        for(int curi = 0; curi < it->second.size(); curi++){
          statusOFS<< it->second[curi]<<" ";
        }

        statusOFS << ")"<< std::endl;
      }
    }
#endif
#ifndef _RELEASE_
    PopCallStack();
#endif








#ifndef _RELEASE_
    PushCallStack("Creating SendToBelow communicator");
#endif
    {
      MPI_Group colCommGroup;  
      MPI_Comm_group(grid_->colComm,&colCommGroup);

      commSendToBelow_.resize(maskSendToBelow_.size());
      commSendToBelowRoot_.resize(numSuper);
      commSendToBelowPtr_.resize(numSuper);

      Int commIdx = 0;
      bitMaskSet::iterator it;
      for(it = maskSendToBelow_.begin(); it != maskSendToBelow_.end(); it++){
        Int count= std::count(it->first.begin(), it->first.end(), true);

        if(count>1){
          MPI_Group commGroup;  

          Int color = it->first[MYROW(grid_)];

          MPI_Comm_split(grid_->colComm, color  ,MYROW(grid_) , &commSendToBelow_[commIdx]);
          MPI_Comm_group(commSendToBelow_[commIdx],&commGroup);
          //now for each supernode, we need to store the pointer to the communnicator and the rank of the root
          std::vector<Int> & snodeList = it->second;
          for(int curi = 0; curi < snodeList.size(); curi++){
            commSendToBelowPtr_[snodeList[curi]] = &commSendToBelow_[commIdx];
            Int ksup = snodeList[curi];
            Int curRoot = PROW(snodeList[curi],grid_);
            Int newRank = -1;
            MPI_Group_translate_ranks(colCommGroup, 1,&curRoot,commGroup, &newRank);
if(newRank==MPI_UNDEFINED){
statusOFS<<"["<<ksup<<"] Root ROW "<<curRoot<<" has no matching rank"<<std::endl;
}
            commSendToBelowRoot_[snodeList[curi]] = newRank;
          }

          commIdx++;

        }
      }

statusOFS<<std::endl<<"commSendToBelowRoot: "<<commSendToBelowRoot_<<std::endl;
    }
#ifndef _RELEASE_
    PopCallStack();
#endif









#ifndef _RELEASE_
    PushCallStack("SendToRight / RecvFromLeft");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes below ksup

      NumVec<bool> sTR(grid_->numProcCol);
      SetValue(sTR,false);



      for( Int isup = ksup + 1; isup < numSuper; isup++ ){
        Int isupLocalBlockRow = LBi( isup, grid_ );
        Int isupProcRow       = PROW( isup, grid_ );
        if( MYROW( grid_ ) == isupProcRow ){
          // SendToRight / RecvFromLeft only if (isup, ksup) is nonzero.
          if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 ){
            for( std::set<Int>::iterator si = localRowBlockColIdx[isupLocalBlockRow].begin();
                si != localRowBlockColIdx[isupLocalBlockRow].end(); si++ ){
              Int jsup = *si;
              Int jsupProcCol = PCOL( jsup, grid_ );
              if( jsup > ksup ){

                if( MYCOL( grid_ ) == jsupProcCol ){
                  isRecvFromLeft_(ksup) = true;
                }
                if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
                  isSendToRight_( jsupProcCol, ksup ) = true;
                }
              }
            } // for (si)
          } // if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 )


          sTR( PCOL(ksup,grid_) ) = true;
          if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 ){
            for( std::set<Int>::iterator si = localRowBlockColIdx[isupLocalBlockRow].begin();
                si != localRowBlockColIdx[isupLocalBlockRow].end(); si++ ){
              Int jsup = *si;
              Int jsupProcCol = PCOL( jsup, grid_ );
              if( jsup > ksup ){
                sTR( jsupProcCol ) = true;
              } // if( jsup > ksup )
            } // for (si)
          }



        } // if( MYROW( grid_ ) == isupProcRow )


        if( MYCOL( grid_ ) == PCOL(ksup, grid_) ){

          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){ 
            isRecvFromBelow_(isupProcRow,ksup) = true;
          }    
          else if (MYROW(grid_) == isupProcRow){
            isSendToDiagonal_(ksup)=true;
          }    
        } // if( MYCOL( grid_ ) == PCOL(ksup, grid_) )




      } // for (isup)
#if ( _DEBUGlevel_ >= 1 )
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        statusOFS<<"DEBUGMAT ["<<ksup<<"] sTR is: "; for(int curi=0; curi<sTR.m();curi++){statusOFS<<sTR(curi)<<" ";} statusOFS<<std::endl;
        statusOFS<<"DEBUGMAT ["<<ksup<<"] isSendToRight_ is: "; for(int curi=0; curi<sTR.m();curi++){statusOFS<<isSendToRight_(curi,ksup)<<" ";} statusOFS<<std::endl;
      }
#endif
      std::vector<bool> mask( sTR.Data(), sTR.Data() + sTR.m() );
      Int count= std::count(mask.begin(), mask.end(), true);
      Int color = mask[MYCOL(grid_)];
      if(count>1){
        std::vector<Int> & snodeList = maskSendToRight_[mask];
        snodeList.push_back(ksup);
      }
      countSendToRight_(ksup) = count * color;
    }	 // for (ksup)


#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToRight:" << isSendToRight_ << std::endl;

    statusOFS << std::endl << "isSendToRight:" << std::endl;
    for(int j = 0;j< isSendToRight_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToRight_.m();i++){
        statusOFS<< isSendToRight_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }

    statusOFS << std::endl << "isRecvFromLeft:" << isRecvFromLeft_ << std::endl;
    statusOFS << std::endl << "isRecvFromBelow:" << isRecvFromBelow_ << std::endl;
#endif
#ifdef PRINT_COMMUNICATOR_STAT
    {
      statusOFS << std::endl << "countSendToRight:" << countSendToRight_ << std::endl;
      statusOFS << std::endl << "DEBUGMAT maskSendToRight_:" << maskSendToRight_.size() <<std::endl; 
      bitMaskSet::iterator it;
      for(it = maskSendToRight_.begin(); it != maskSendToRight_.end(); it++){
        //print the involved processors
        for(int curi = 0; curi < it->first.size(); curi++){
          statusOFS << it->first[curi] << " "; 
        }

        statusOFS<< "    ( ";
        //print the involved supernode indexes
        std::vector<Int> & snodeList = it->second;
        for(int curi = 0; curi < snodeList.size(); curi++){
          statusOFS<<snodeList[curi]<<" ";
        }

        statusOFS << ")"<< std::endl;
      }
    }
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif





#ifndef _RELEASE_
    PushCallStack("Creating SendToRight communicator");
#endif
    {
      MPI_Group rowCommGroup;  
      MPI_Comm_group(grid_->rowComm,&rowCommGroup);

      commSendToRight_.resize(maskSendToRight_.size());
      commSendToRightRoot_.resize(numSuper);
      commSendToRightPtr_.resize(numSuper);

      Int commIdx = 0;
      bitMaskSet::iterator it;
      for(it = maskSendToRight_.begin(); it != maskSendToRight_.end(); it++){
        Int count= std::count(it->first.begin(), it->first.end(), true);

        if(count>1){
          MPI_Group commGroup;  

          Int color = it->first[MYCOL(grid_)];

          MPI_Comm_split(grid_->rowComm, color  ,MYCOL(grid_) , &commSendToRight_[commIdx]);
          MPI_Comm_group(commSendToRight_[commIdx],&commGroup);
          //now for each supernode, we need to store the pointer to the communnicator and the rank of the root
          std::vector<Int> & snodeList = it->second;
          for(int curi = 0; curi < snodeList.size(); curi++){
            commSendToRightPtr_[snodeList[curi]] = &commSendToRight_[commIdx];
            Int ksup = snodeList[curi];
            Int curRoot = PCOL(snodeList[curi],grid_);
            Int newRank = -1;
            if(color>0){
              MPI_Group_translate_ranks(rowCommGroup, 1,&curRoot,commGroup, &newRank);

              if(newRank==MPI_UNDEFINED){
                statusOFS<<"["<<ksup<<"] Root COL "<<curRoot<<" has no matching rank"<<std::endl;
              }
            }

            commSendToRightRoot_[snodeList[curi]] = newRank;
          }

          commIdx++;
        }
      }
    }

statusOFS<<std::endl<<"commSendToRightRoot: "<<commSendToRightRoot_<<std::endl;

#ifndef _RELEASE_
    PopCallStack();
#endif








#ifndef _RELEASE_
    PushCallStack("SendToCrossDiagonal / RecvFromCrossDiagonal");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end(); si++ ){
          Int isup = *si;
          Int isupProcRow = PROW( isup, grid_ );
          if( isup > ksup && MYROW( grid_ ) == isupProcRow ){
            isSendToCrossDiagonal_( ksup ) = true;
          }
        } // for (si)
      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for (ksup)

    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localRowBlockColIdx[ LBi(ksup, grid_) ].begin();
            si != localRowBlockColIdx[ LBi(ksup, grid_) ].end(); si++ ){
          Int jsup = *si;
          Int jsupProcCol = PCOL( jsup, grid_ );
          if( jsup > ksup && MYCOL(grid_) == jsupProcCol ){
            isRecvFromCrossDiagonal_[ksup] = true;
          }
        } // for (si)
      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for (ksup)
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToCrossDiagonal:" << isSendToCrossDiagonal_ << std::endl;
    statusOFS << std::endl << "isRecvFromCrossDiagonal:" << isRecvFromCrossDiagonal_ << std::endl;
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif

    //Build the list of supernodes based on the elimination tree from SuperLU
    std::vector<std::vector<Int> > & WSet = this->WorkingSet();


    if (options_->MaxPipelineDepth!=1){
    //do the real stuff with elimination trees
    //translate from columns to supernodes etree using supIdx
    std::vector<Int> snodeEtree(this->NumSuper());
    GetEtree(snodeEtree);
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << " Supernodal elimination tree is " << snodeEtree <<std::endl<<std::endl;
#endif

#ifndef _RELEASE_
    double begin = MPI_Wtime();
#endif


    //find roots in the supernode etree (it must be postordered)
    //initialize the parent we are looking at 
    Int rootParent = snodeEtree[this->NumSuper()-2];

    //look for roots in the forest
    std::vector< Int>  initialRootList(1,rootParent);
    std::vector< Int>  mergeRootBuf;
    bool needMerge = false;
    Int prevRootIdx = -1;
    std::vector< Int> & prevRoot = initialRootList;

    /* initialize the num of child for each node */
    Int nsupers = this->NumSuper();
    std::vector<Int> num_child;
    num_child.resize(nsupers,0);
    for(Int i=0; i<nsupers; i++ ) if( snodeEtree[i] != nsupers ) num_child[snodeEtree[i]] ++;

    while(prevRoot.size()>0){
      WSet.push_back(std::vector<Int>());
      Int totalChild =0;
      for(Int i = 0; i<prevRoot.size();++i){ totalChild += num_child[prevRoot[i]]; }
      WSet.back().reserve(totalChild);

      for(Int i = 0; i<prevRoot.size();++i){
        rootParent = prevRoot[i];
        std::vector<Int>::iterator parentIt = snodeEtree.begin()+rootParent;
        std::vector<Int>::iterator curRootIt = std::find (snodeEtree.begin() ,parentIt, rootParent);
        while(curRootIt != parentIt){
          Int curNode = curRootIt - snodeEtree.begin();
          WSet.back().push_back(curNode);
          //switch the sign to remove this root
          *curRootIt =-*curRootIt;
          //look for next root
          curRootIt = std::find (snodeEtree.begin() ,parentIt, rootParent);
        }
      }
      //No we have now several roots >> must maintain a vector of roots
      if(needMerge){
        mergeRootBuf.clear();
        prevRootIdx++;
        for(Int j=prevRootIdx;j<WSet.size();++j){
          mergeRootBuf.insert(mergeRootBuf.end(),WSet[j].begin(),WSet[j].end());
        }
        prevRoot = mergeRootBuf;
        needMerge = false;
      }
      else{
        prevRootIdx++;
        prevRoot = WSet[prevRootIdx];
      }
    }
    if(WSet.back().size()==0){
      WSet.pop_back();
    }


    for (Int lidx=0; lidx<WSet.size() ; lidx++){
      if(options_->MaxPipelineDepth){
        if(WSet[lidx].size()>options_->MaxPipelineDepth)
        {
          std::vector<std::vector<Int> >::iterator pos = WSet.begin()+lidx+1;               
          WSet.insert(pos,std::vector<Int>());
          WSet[lidx+1].insert(WSet[lidx+1].begin(),WSet[lidx].begin() +options_->MaxPipelineDepth ,WSet[lidx].end());
          WSet[lidx].erase(WSet[lidx].begin()+options_->MaxPipelineDepth,WSet[lidx].end());
        }
      }
    }
#ifndef _RELEASE_
    double end =  MPI_Wtime( );
    statusOFS<<std::endl<<"Time for building working set: "<<end-begin<<std::endl<<std::endl;
#endif
#if ( _DEBUGlevel_ >= 0 )
    for (Int lidx=0; lidx<WSet.size() ; lidx++){
      statusOFS << std::endl << "L"<< lidx << " is: {";
      for (Int supidx=0; supidx<WSet[lidx].size() ; supidx++){
        statusOFS << WSet[lidx][supidx] << " ["<<snodeEtree[WSet[lidx][supidx]]<<"] ";
      }
      statusOFS << " }"<< std::endl;
    }
#endif


    }
    else{
      for( Int ksup = numSuper - 2; ksup >= 0; ksup-- ){
        WSet.push_back(std::vector<Int>());
        WSet.back().push_back(ksup);
      }
#if ( _DEBUGlevel_ >= 0 )
    for (Int lidx=0; lidx<WSet.size() ; lidx++){
      statusOFS << std::endl << "L"<< lidx << " is: {";
      for (Int supidx=0; supidx<WSet[lidx].size() ; supidx++){
        statusOFS << WSet[lidx][supidx] << " ";
      }
      statusOFS << " }"<< std::endl;
    }
#endif



    }


    return ;
  } 		// -----  end of method PMatrix::ConstructCommunicationPattern  ----- 










  void PMatrix::SelInv_Bcast	(  )
  {
#ifdef SELINV_MEMORY
#ifdef USE_TAU
    TAU_TRACK_MEMORY();
    TAU_ENABLE_TRACKING_MEMORY();
#else
    Real globalBufSize = 0;
#endif
#endif

#ifdef SELINV_TIMING
    Real begin_SendULWaitContentFirst, end_SendULWaitContentFirst, time_SendULWaitContentFirst = 0;
#if defined (PROFILE) || defined(PMPI) || defined(USE_TAU)
    TAU_PROFILE_SET_CONTEXT(MPI_COMM_WORLD);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_START("SelInvBcast");
#elif defined (PROFILE)
    TAU_FSTART(SelInvBcast);
#endif
#endif

#ifndef _RELEASE_
    PushCallStack("PMatrix::SelInv");
#endif



    Int numSuper = this->NumSuper(); 

    // Main loop
    std::vector<std::vector<Int> > & superList = this->WorkingSet();
    Int numSteps = superList.size();

    for (Int lidx=0; lidx<numSteps ; lidx++){
      Int stepSuper = superList[lidx].size(); 


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("AllocateBuffer");
#elif defined(PROFILE)
      TAU_FSTART(AllocateBuffer);
#endif
#endif

      std::vector<std::vector<MPI_Request> >  arrMpireqsSendToBelow;
      std::vector<std::vector<MPI_Request> >  arrMpireqsSendToRight;
      std::vector<MPI_Request>   arrMpireqsRecvSizeFromAny;
      std::vector<MPI_Request>   arrMpireqsRecvContentFromAny;
      std::vector<NumMat<Scalar> >  arrLUpdateBuf;
#ifdef USE_REDUCE_L
//      std::vector<NumMat<Scalar> >  arrLUpdateBufReduced;
//      arrLUpdateBufReduced.resize(stepSuper,NumMat<Scalar>());
#endif
      std::vector<NumMat<Scalar> >  arrDiagBuf;
      std::vector<std::vector<Int> >  arrRowLocalPtr;
      std::vector<std::vector<Int> >  arrBlockIdxLocal;
      std::vector<std::vector<char> > arrSstrLcolSend;
      std::vector<std::vector<char> > arrSstrUrowSend;
      std::vector<std::vector<char> > arrSstrLcolRecv;
      std::vector<std::vector<char> > arrSstrUrowRecv;
      std::vector< std::vector<char> > arrSstrCrossDiag;
      std::vector<Int > arrSstrLcolSizeSend;
      std::vector<Int > arrSstrUrowSizeSend;
      std::vector<Int> arrSizeStmFromAbove;
      std::vector<Int> arrSizeStmFromLeft;
      std::vector<Int >  arrSstrSizeCrossDiag;

      //allocate the buffers for this supernode
      arrMpireqsSendToBelow.resize( stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcRow, MPI_REQUEST_NULL ));
      arrMpireqsSendToRight.resize(stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcCol, MPI_REQUEST_NULL ));
      arrMpireqsRecvSizeFromAny.resize(stepSuper*2 , MPI_REQUEST_NULL);
      arrMpireqsRecvContentFromAny.resize(stepSuper*2 , MPI_REQUEST_NULL);
      arrSstrUrowSend.resize(stepSuper, std::vector<char>( ));
      arrSstrLcolSend.resize(stepSuper, std::vector<char>( ));
      arrSstrUrowSizeSend.resize(stepSuper, 0);
      arrSstrLcolSizeSend.resize(stepSuper, 0);
      arrSstrUrowRecv.resize(stepSuper, std::vector<char>( ));
      arrSstrLcolRecv.resize(stepSuper, std::vector<char>( ));
      arrSizeStmFromLeft.resize(stepSuper,0);
      arrSizeStmFromAbove.resize(stepSuper,0);
      arrLUpdateBuf.resize(stepSuper,NumMat<Scalar>());
      arrRowLocalPtr.resize(stepSuper,std::vector<Int>());
      arrBlockIdxLocal.resize(stepSuper,std::vector<Int>());
      arrDiagBuf.resize(stepSuper,NumMat<Scalar>());
      arrSstrCrossDiag.resize(stepSuper);
      arrSstrSizeCrossDiag.resize(stepSuper,0);

      arrMpireqsSendToBelow.assign( stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcRow, MPI_REQUEST_NULL ));
      arrMpireqsSendToRight.assign(stepSuper, std::vector<MPI_Request>( 2 * grid_->numProcCol, MPI_REQUEST_NULL ));
      arrMpireqsRecvSizeFromAny.assign(stepSuper*2 , MPI_REQUEST_NULL );
      arrMpireqsRecvContentFromAny.assign(stepSuper*2 , MPI_REQUEST_NULL );


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("AllocateBuffer");
#elif defined(PROFILE)
      TAU_FSTOP(AllocateBuffer);
#endif
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateL");
#endif
#if ( _DEBUGlevel_ >= 1 )
      statusOFS << std::endl << "Communication to the Schur complement." << std::endl << std::endl; 
#endif








#ifdef SELINV_TIMING
#if defined(PROFILE)
      TAU_FSTART(WaitContent_UL_BCAST);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper ; supidx++){
        Int ksup = superList[lidx][supidx];
        // Communication for the U part.
        if(countSendToBelow_(ksup)>1){
          MPI_Comm * colComm = commSendToBelowPtr_[ksup];
          Int root = commSendToBelowRoot_[ksup];
          bool isRecvFromAbove =  isRecvFromAbove_( ksup );
          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
            std::vector<char> & sstrUrowSend = arrSstrUrowSend[supidx];

            // Pack the data in U
            std::stringstream sstm;
            std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
            std::vector<UBlock>&  Urow = this->U( LBi(ksup, grid_) );
            // All blocks are to be sent down.
            serialize( (Int)Urow.size(), sstm, NO_MASK );
            for( Int jb = 0; jb < Urow.size(); jb++ ){
              serialize( Urow[jb], sstm, mask );
            }
            sstrUrowSend.resize( Size( sstm ) );
            sstm.read( &sstrUrowSend[0], sstrUrowSend.size() );
            arrSstrUrowSizeSend[supidx] = sstrUrowSend.size();

            //send size
            MPI_Bcast(&arrSstrUrowSizeSend[supidx], 1 , MPI_INT, root, *colComm );
            //send content
            MPI_Bcast(&sstrUrowSend[0], arrSstrUrowSizeSend[supidx] , MPI_BYTE, root, *colComm );
          }
          else if( isRecvFromAbove && 
              MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            //size
            MPI_Bcast(&arrSizeStmFromAbove[supidx], 1 , MPI_INT, root, *colComm );
            //content
            Int & sizeStmFromAbove = arrSizeStmFromAbove[supidx];
            std::vector<char> & sstrUrowRecv = arrSstrUrowRecv[supidx];
            sstrUrowRecv.resize( sizeStmFromAbove );
            MPI_Bcast(&sstrUrowRecv[0], sizeStmFromAbove , MPI_BYTE, root, *colComm );
          }
        }
        // Communication for the L part.
        if(countSendToRight_(ksup)>1){
          MPI_Comm * colComm = commSendToRightPtr_[ksup];
          Int root = commSendToRightRoot_[ksup];
          bool isRecvFromLeft =  isRecvFromLeft_( ksup );
          if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
            std::vector<char> & sstrLcolSend = arrSstrLcolSend[supidx];

            // Pack the data in L 
            std::stringstream sstm;
            std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
            mask[LBlockMask::NZVAL] = 0; // nzval is excluded 

            std::vector<LBlock>&  Lcol = this->L( LBj(ksup, grid_) );
            // All blocks except for the diagonal block are to be sent right
            if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
              serialize( (Int)Lcol.size() - 1, sstm, NO_MASK );
            else
              serialize( (Int)Lcol.size(), sstm, NO_MASK );

            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              if( Lcol[ib].blockIdx > ksup ){
                serialize( Lcol[ib], sstm, mask );
              }
            }
            sstrLcolSend.resize( Size( sstm ) );
            sstm.read( &sstrLcolSend[0], sstrLcolSend.size() );
            arrSstrLcolSizeSend[supidx] = sstrLcolSend.size();
            //send size
            MPI_Bcast(&arrSstrLcolSizeSend[supidx], 1 , MPI_INT, root, *colComm );
            //send content
            MPI_Bcast(&sstrLcolSend[0], arrSstrLcolSizeSend[supidx] , MPI_BYTE, root, *colComm );
          }
          else if( isRecvFromLeft && 
              MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
            //size
            MPI_Bcast(&arrSizeStmFromLeft[supidx], 1 , MPI_INT, root, *colComm );
            //content
            Int & sizeStmFromLeft = arrSizeStmFromLeft[supidx];
            std::vector<char> & sstrLcolRecv = arrSstrLcolRecv[supidx];
            sstrLcolRecv.resize( sizeStmFromLeft );
            MPI_Bcast(&sstrLcolRecv[0], sizeStmFromLeft , MPI_BYTE, root, *colComm );
          }
        }

      }
#ifdef SELINV_TIMING
#if defined(PROFILE)
      TAU_FSTOP(WaitContent_UL_BCAST);
#endif
#endif




#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Compute_Sinv_LT);
#endif
#endif

      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        // Overlap the communication with computation.  All processors move
        // to Gemm phase when ready 
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
        TAU_FSTART(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<  "Unpack the received data for processors participate in Gemm. " << std::endl << std::endl; 
#endif

        std::vector<char> & sstrUrowRecv = arrSstrUrowRecv[supidx];
        std::vector<char> & sstrLcolRecv = arrSstrLcolRecv[supidx];
        Int & sizeStmFromLeft = arrSizeStmFromLeft[supidx];
        Int & sizeStmFromAbove = arrSizeStmFromAbove[supidx];
        NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];


        std::vector<LBlock> LcolRecv;
        std::vector<UBlock> UrowRecv;
        if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){
          // U part
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            std::stringstream sstm;
            sstm.write( &sstrUrowRecv[0], sizeStmFromAbove );
            std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
            Int numUBlock;
            deserialize( numUBlock, sstm, NO_MASK );
            UrowRecv.resize( numUBlock );
            for( Int jb = 0; jb < numUBlock; jb++ ){
              deserialize( UrowRecv[jb], sstm, mask );
            } 
          } // sender is not the same as receiver
          else{
            // U is obtained locally, just make a copy. Include everything
            // (there is no diagonal block)
            UrowRecv = this->U( LBi( ksup, grid_ ) );
          } // sender is the same as receiver


          //L part
          if( MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
            std::stringstream     sstm;
            sstm.write( &sstrLcolRecv[0], sizeStmFromLeft );
            std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
            mask[LBlockMask::NZVAL] = 0; // nzval is excluded
            Int numLBlock;
            deserialize( numLBlock, sstm, NO_MASK );
            LcolRecv.resize( numLBlock );
            for( Int ib = 0; ib < numLBlock; ib++ ){
              deserialize( LcolRecv[ib], sstm, mask );
            }
          } // sender is not the same as receiver
          else{
            // L is obtained locally, just make a copy. 
            // Do not include the diagonal block
            std::vector<LBlock>& Lcol =  this->L( LBj( ksup, grid_ ) );
            if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
              LcolRecv.resize( Lcol.size() );
              for( Int ib = 0; ib < Lcol.size(); ib++ ){
                LcolRecv[ib] = Lcol[ib];
              }
            }
            else{
              LcolRecv.resize( Lcol.size() - 1 );
              for( Int ib = 0; ib < Lcol.size() - 1; ib++ ){
                LcolRecv[ib] = Lcol[ib+1];
              }
            }
          } // sender is the same as receiver
        } // if I am a receiver


#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<  "Main work: Gemm" << std::endl << std::endl; 
#endif

        // Save all the data to be updated for { L( isup, ksup ) | isup > ksup }.
        // The size will be updated in the Gemm phase and the reduce phase

        // Only the processors received information participate in the Gemm 
        if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){
          // rowPtr[ib] gives the row index in LUpdateBuf for the first
          // nonzero row in LcolRecv[ib]. The total number of rows in
          // LUpdateBuf is given by rowPtr[end]-1
          std::vector<Int> rowPtr(LcolRecv.size() + 1);
          // colPtr[jb] gives the column index in UBuf for the first
          // nonzero column in UrowRecv[jb]. The total number of rows in
          // UBuf is given by colPtr[end]-1
          std::vector<Int> colPtr(UrowRecv.size() + 1);

          rowPtr[0] = 0;
          for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
            rowPtr[ib+1] = rowPtr[ib] + LcolRecv[ib].numRow;
          }
          colPtr[0] = 0;
          for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
            colPtr[jb+1] = colPtr[jb] + UrowRecv[jb].numCol;
          }

          Int numRowAinvBuf = *rowPtr.rbegin();
          Int numColAinvBuf = *colPtr.rbegin();

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << "["<<ksup<<"] "<<  "AinvBuf ~ " << numRowAinvBuf << " x " << numColAinvBuf << std::endl;
          statusOFS << "["<<ksup<<"] "<<  "rowPtr:" << std::endl << rowPtr << std::endl;
          statusOFS << "["<<ksup<<"] "<<  "colPtr:" << std::endl << colPtr << std::endl;
#endif
          // Allocate for the computational storage
          NumMat<Scalar> AinvBuf( numRowAinvBuf, numColAinvBuf );


          LUpdateBuf.Resize( numRowAinvBuf, SuperSize( ksup, super_ ) );
          NumMat<Scalar> UBuf( SuperSize( ksup, super_ ), numColAinvBuf );
          SetValue( AinvBuf, SCALAR_ZERO );
          SetValue( LUpdateBuf, SCALAR_ZERO );
          SetValue( UBuf, SCALAR_ZERO );

          // Fill UBuf first.  Make the transpose later in the Gemm phase.
          for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
            UBlock& UB = UrowRecv[jb];
            if( UB.numRow != SuperSize(ksup, super_) ){
              throw std::logic_error( "The size of UB is not right.  Something is seriously wrong." );
            }
            lapack::Lacpy( 'A', UB.numRow, UB.numCol, UB.nzval.Data(),
                UB.numRow, UBuf.VecData( colPtr[jb] ), UBuf.m() );	
          }

          // Calculate the relative indices for (isup, jsup)
          // Fill AinvBuf with the information in L or U block.
          for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
            for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
              LBlock& LB = LcolRecv[ib];
              UBlock& UB = UrowRecv[jb];
              Int isup = LB.blockIdx;
              Int jsup = UB.blockIdx;
              Scalar* nzvalAinv = &AinvBuf( rowPtr[ib], colPtr[jb] );
              Int     ldAinv    = AinvBuf.m();

              // Pin down the corresponding block in the part of Sinv.
              if( isup >= jsup ){
                std::vector<LBlock>&  LcolSinv = this->L( LBj(jsup, grid_ ) );
                bool isBlockFound = false;
                for( Int ibSinv = 0; ibSinv < LcolSinv.size(); ibSinv++ ){
                  // Found the (isup, jsup) block in Sinv
                  if( LcolSinv[ibSinv].blockIdx == isup ){
                    LBlock& SinvB = LcolSinv[ibSinv];

                    // Row relative indices
                    std::vector<Int> relRows( LB.numRow );
                    Int* rowsLBPtr    = LB.rows.Data();
                    Int* rowsSinvBPtr = SinvB.rows.Data();
                    for( Int i = 0; i < LB.numRow; i++ ){
                      bool isRowFound = false;
                      for( Int i1 = 0; i1 < SinvB.numRow; i1++ ){
                        if( rowsLBPtr[i] == rowsSinvBPtr[i1] ){
                          isRowFound = true;
                          relRows[i] = i1;
                          break;
                        }
                      }
                      if( isRowFound == false ){
                        std::ostringstream msg;
                        msg << "Row " << rowsLBPtr[i] << 
                          " in LB cannot find the corresponding row in SinvB" << std::endl
                          << "LB.rows    = " << LB.rows << std::endl
                          << "SinvB.rows = " << SinvB.rows << std::endl;
                        throw std::runtime_error( msg.str().c_str() );
                      }
                    }

                    // Column relative indicies
                    std::vector<Int> relCols( UB.numCol );
                    Int SinvColsSta = FirstBlockCol( jsup, super_ );
                    for( Int j = 0; j < UB.numCol; j++ ){
                      relCols[j] = UB.cols[j] - SinvColsSta;
                    }

                    // Transfer the values from Sinv to AinvBlock
                    Scalar* nzvalSinv = SinvB.nzval.Data();
                    Int     ldSinv    = SinvB.numRow;
                    for( Int j = 0; j < UB.numCol; j++ ){
                      for( Int i = 0; i < LB.numRow; i++ ){
                        nzvalAinv[i+j*ldAinv] =
                          nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                      }
                    }

                    isBlockFound = true;
                    break;
                  }	
                } // for (ibSinv )
                if( isBlockFound == false ){
                  std::ostringstream msg;
                  msg << "Block(" << isup << ", " << jsup 
                    << ") did not find a matching block in Sinv." << std::endl;
                  throw std::runtime_error( msg.str().c_str() );
                }
              } // if (isup, jsup) is in L
              else{
                std::vector<UBlock>&   UrowSinv = this->U( LBi( isup, grid_ ) );
                bool isBlockFound = false;
                for( Int jbSinv = 0; jbSinv < UrowSinv.size(); jbSinv++ ){
                  // Found the (isup, jsup) block in Sinv
                  if( UrowSinv[jbSinv].blockIdx == jsup ){
                    UBlock& SinvB = UrowSinv[jbSinv];

                    // Row relative indices
                    std::vector<Int> relRows( LB.numRow );
                    Int SinvRowsSta = FirstBlockCol( isup, super_ );
                    for( Int i = 0; i < LB.numRow; i++ ){
                      relRows[i] = LB.rows[i] - SinvRowsSta;
                    }

                    // Column relative indices
                    std::vector<Int> relCols( UB.numCol );
                    Int* colsUBPtr    = UB.cols.Data();
                    Int* colsSinvBPtr = SinvB.cols.Data();
                    for( Int j = 0; j < UB.numCol; j++ ){
                      bool isColFound = false;
                      for( Int j1 = 0; j1 < SinvB.numCol; j1++ ){
                        if( colsUBPtr[j] == colsSinvBPtr[j1] ){
                          isColFound = true;
                          relCols[j] = j1;
                          break;
                        }
                      }
                      if( isColFound == false ){
                        std::ostringstream msg;
                        msg << "Col " << colsUBPtr[j] << 
                          " in UB cannot find the corresponding row in SinvB" << std::endl
                          << "UB.cols    = " << UB.cols << std::endl
                          << "UinvB.cols = " << SinvB.cols << std::endl;
                        throw std::runtime_error( msg.str().c_str() );
                      }
                    }

                    // Transfer the values from Sinv to AinvBlock
                    Scalar* nzvalSinv = SinvB.nzval.Data();
                    Int     ldSinv    = SinvB.numRow;
                    for( Int j = 0; j < UB.numCol; j++ ){
                      for( Int i = 0; i < LB.numRow; i++ ){
                        nzvalAinv[i+j*ldAinv] =
                          nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                      }
                    }

                    isBlockFound = true;
                    break;
                  }
                } // for (jbSinv)
                if( isBlockFound == false ){
                  std::ostringstream msg;
                  msg << "Block(" << isup << ", " << jsup 
                    << ") did not find a matching block in Sinv." << std::endl;
                  throw std::runtime_error( msg.str().c_str() );
                }
              } // if (isup, jsup) is in U

            } // for( ib )
          } // for ( jb )

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "AinvBuf: " << AinvBuf << std::endl;
          statusOFS << std::endl << "["<<ksup<<"] "<<  "UBuf   : " << UBuf << std::endl;
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
          TAU_FSTOP(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
          TAU_FSTART(Compute_Sinv_LT_GEMM);
#endif
#endif

          // Gemm for LUpdateBuf = -AinvBuf * UBuf^T
          blas::Gemm( 'N', 'T', AinvBuf.m(), UBuf.m(), AinvBuf.n(), SCALAR_MINUS_ONE, 
              AinvBuf.Data(), AinvBuf.m(), 
              UBuf.Data(), UBuf.m(), SCALAR_ZERO,
              LUpdateBuf.Data(), LUpdateBuf.m() ); 
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
          TAU_FSTOP(Compute_Sinv_LT_GEMM);
#endif
#endif

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<  "LUpdateBuf: " << LUpdateBuf << std::endl;
#endif
        } // if Gemm is to be done locally




#ifndef USE_REDUCE_L
        //If I was a receiver, I need to send my data to proc in column of ksup
        if( isRecvFromLeft_( ksup ) && MYCOL( grid_ ) != PCOL( ksup, grid_ ) )
        {
          std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
          MPI_Isend( LUpdateBuf.Data(), LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar), MPI_BYTE, PCOL(ksup,grid_) ,SELINV_TAG_COUNT*supidx+SELINV_TAG_L_REDUCE, grid_->rowComm, &mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYCOL(grid_)<<" has sent "<< LUpdateBuf.m()*LUpdateBuf.n()*sizeof(Scalar) << " bytes to " << PCOL(ksup,grid_) << std::endl;
#endif
        }//Sender
#endif

      }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Compute_Sinv_LT);
#endif
#endif


      //Reduce Sinv L^T to the processors in PCOL(ksup,grid_)
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Sinv_LT);
#endif
#endif

#ifndef USE_REDUCE_L
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

        NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

          //determine the number of rows in LUpdateBufReduced
          std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
          std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
          Int numRowLUpdateBuf;
          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            rowLocalPtr.resize( Lcol.size() + 1 );
            blockIdxLocal.resize( Lcol.size() );
            rowLocalPtr[0] = 0;
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib+1] = rowLocalPtr[ib] + Lcol[ib].numRow;
              blockIdxLocal[ib] = Lcol[ib].blockIdx;
            }
          } // I do not own the diagonal block
          else{
            rowLocalPtr.resize( Lcol.size() );
            blockIdxLocal.resize( Lcol.size() - 1 );
            rowLocalPtr[0] = 0;
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib] = rowLocalPtr[ib-1] + Lcol[ib].numRow;
              blockIdxLocal[ib-1] = Lcol[ib].blockIdx;
            }
          } // I own the diagonal block, skip the diagonal block
          numRowLUpdateBuf = *rowLocalPtr.rbegin();


          NumMat<Scalar>  LUpdateBufRecv(numRowLUpdateBuf,SuperSize( ksup, super_ ) );
          NumMat<Scalar> & LUpdateBufReduced = LUpdateBuf;
          if( numRowLUpdateBuf > 0 ){
            if( LUpdateBuf.m() == 0 && LUpdateBuf.n() == 0 ){
              LUpdateBufReduced.Resize( numRowLUpdateBuf,SuperSize( ksup, super_ ) );
              // Fill zero is important
              SetValue( LUpdateBufReduced, SCALAR_ZERO );
            }
          }

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBuf Before Reduction: " <<  LUpdateBufReduced << std::endl << std::endl; 
#endif

          Int totCountRecv = 0;
          Int numRecv = CountSendToRight(ksup);
          for( Int countRecv = 0; countRecv < numRecv ; ++countRecv ){
            //Do the blocking recv
            MPI_Status stat;
            Int size = 0;
            MPI_Recv(LUpdateBufRecv.Data(), numRowLUpdateBuf*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, MPI_ANY_SOURCE,SELINV_TAG_COUNT*supidx+SELINV_TAG_L_REDUCE, grid_->rowComm,&stat);
            MPI_Get_count(&stat, MPI_BYTE, &size);
            //if the processor contributes
            if(size>0){

#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYCOL(grid_)<<" has received "<< size << " bytes from " << stat.MPI_SOURCE << std::endl;
#endif
#if ( _DEBUGlevel_ >= 2 )
              statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufRecv: " <<  LUpdateBufRecv << std::endl << std::endl; 
#endif

              //do the sum
              blas::Axpy(numRowLUpdateBuf*SuperSize( ksup, super_ ), SCALAR_ONE, LUpdateBufRecv.Data(),
                  1, LUpdateBufReduced.Data(), 1 );
            }
          } // for (iProcCol)

#if ( _DEBUGlevel_ >= 2 ) 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufReduced: " <<  LUpdateBufReduced << std::endl << std::endl; 
#endif
#ifdef SEND_CROSSDIAG_ASYNC
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Send_L_CrossDiag);
#endif
#endif


          // Send LUpdateBufReduced to the cross diagonal blocks. 
          // NOTE: This assumes square processor grid
          if( isSendToCrossDiagonal_( ksup ) ){
            Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
            if( MYPROC( grid_ ) != dest	){
              std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
              std::stringstream sstm;
              std::vector<char> & sstr = arrSstrCrossDiag[supidx];
              Int & sizeStm = arrSstrSizeCrossDiag[supidx];
              serialize( rowLocalPtr, sstm, NO_MASK );
              serialize( blockIdxLocal, sstm, NO_MASK );
              serialize( LUpdateBufReduced, sstm, NO_MASK );
              mpi::Isend( sstm, sstr, sizeStm, dest, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm, mpireqsSendToBelow[0], mpireqsSendToBelow[1]);
            }
          } // sender to cross diagonal
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif
#endif
        } // Receiver
      }

#endif
#ifdef USE_REDUCE_L
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

        NumMat<Scalar> & LUpdateBuf = arrLUpdateBuf[supidx];

        // Processor column of ksup collects the symbolic data for LUpdateBuf.
        std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
        std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
        Int numRowLUpdateBuf;

        NumMat<Scalar> LUpdateBufReduced; 

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            rowLocalPtr.resize( Lcol.size() + 1 );
            blockIdxLocal.resize( Lcol.size() );
            rowLocalPtr[0] = 0;
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib+1] = rowLocalPtr[ib] + Lcol[ib].numRow;
              blockIdxLocal[ib] = Lcol[ib].blockIdx;
            }
          } // I do not own the diaogonal block
          else{
            rowLocalPtr.resize( Lcol.size() );
            blockIdxLocal.resize( Lcol.size() - 1 );
            rowLocalPtr[0] = 0;
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              rowLocalPtr[ib] = rowLocalPtr[ib-1] + Lcol[ib].numRow;
              blockIdxLocal[ib-1] = Lcol[ib].blockIdx;
            }
          } // I owns the diagonal block, skip the diagonal block
          numRowLUpdateBuf = *rowLocalPtr.rbegin();
          if( numRowLUpdateBuf > 0 ){
            LUpdateBufReduced.Resize( numRowLUpdateBuf, SuperSize( ksup, super_ ) );
            SetValue( LUpdateBufReduced, SCALAR_ZERO );
            //              if( LUpdateBuf.m() == 0 && LUpdateBuf.n() == 0 ){
            //                LUpdateBuf.Resize( numRowLUpdateBuf, SuperSize( ksup, super_ ) );
            //                // Fill zero is important
            //                SetValue( LUpdateBuf, SCALAR_ZERO );
            //              }
          }
        } 


        if(countSendToRight_(ksup)>1){

          MPI_Comm * rowComm = commSendToRightPtr_[ksup];
          Int root = commSendToRightRoot_[ksup];



          // Processor column sends the total row dimension to all processors
          // in the same row to prepare for reduce
          MPI_Bcast( &numRowLUpdateBuf, 1, MPI_INT, root, *rowComm );

          // If LUpdatebuf has not been constructed, resize and fill with zero
          if( numRowLUpdateBuf > 0 ){
            if( LUpdateBuf.m() == 0 && LUpdateBuf.n() == 0 ){
              LUpdateBuf.Resize( numRowLUpdateBuf, SuperSize( ksup, super_ ) );
              // Fill zero is important
              SetValue( LUpdateBuf, SCALAR_ZERO );
            }

            //          if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
            //            mpi::Reduce( (Scalar *) MPI_IN_PLACE, LUpdateBuf.Data(),
            //                numRowLUpdateBuf * SuperSize( ksup, super_ ), MPI_SUM, 
            //                root, *rowComm );
            //          }
            //          else{
            mpi::Reduce( LUpdateBuf.Data(), LUpdateBufReduced.Data(),
                numRowLUpdateBuf * SuperSize( ksup, super_ ), MPI_SUM, 
                root, *rowComm );
            //          }


            if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
              LUpdateBuf = LUpdateBufReduced;

#if ( _DEBUGlevel_ >= 1 ) 
              statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufReduced: " <<  LUpdateBufReduced << std::endl << std::endl; 
              statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBuf: " <<  LUpdateBuf << std::endl << std::endl; 
#endif

            }

          } // Perform reduce for nonzero block rows in the column of ksup

        }

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

#ifdef SEND_CROSSDIAG_ASYNC
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Send_L_CrossDiag);
#endif
#endif


          // Send LUpdateBufReduced to the cross diagonal blocks. 
          // NOTE: This assumes square processor grid
          if( isSendToCrossDiagonal_( ksup ) ){
            Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
            if( MYPROC( grid_ ) != dest	){
              std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
              std::stringstream sstm;
              std::vector<char> & sstr = arrSstrCrossDiag[supidx];
              Int & sizeStm = arrSstrSizeCrossDiag[supidx];
              serialize( rowLocalPtr, sstm, NO_MASK );
              serialize( blockIdxLocal, sstm, NO_MASK );
              serialize( LUpdateBufReduced, sstm, NO_MASK );
              mpi::Isend( sstm, sstr, sizeStm, dest, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm, mpireqsSendToBelow[0], mpireqsSendToBelow[1]);
            }
          } // sender to cross diagonal
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif
#endif








        }




      }
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Sinv_LT);
#endif
#endif



      //--------------------- End of reduce of LUpdateBuf-------------------------

#if ( _DEBUGlevel_ >= 1 )
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && LUpdateBufReduced.m() > 0 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "LUpdateBufReduced: " << LUpdateBufReduced << std::endl << std::endl; 
      }
#endif






#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateD");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Update_Diagonal);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<   "Update the diagonal block" << std::endl << std::endl; 
#endif
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

          //---------Computing  Diagonal block, all processors in the column are participating to all pipelined supernodes

          std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
          std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
          NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
          Int numRowLUpdateBuf = LUpdateBufReduced.m();


          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );

          //Allocate DiagBuf even if Lcol.size() == 0
          NumMat<Scalar> & DiagBuf = arrDiagBuf[supidx];
          DiagBuf.Resize(SuperSize( ksup, super_ ), SuperSize( ksup, super_ ));
          SetValue(DiagBuf, SCALAR_ZERO);

          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                  SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib], 0 ), LUpdateBufReduced.m(),
                  Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                  SCALAR_ONE, DiagBuf.Data(), SuperSize( ksup, super_ ) );
            }


#ifndef USE_REDUCE_DIAG
            if(isSendToDiagonal_(ksup)){
              //send to above
              std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
              MPI_Isend( DiagBuf.Data(),  SuperSize( ksup, super_ )*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, PROW(ksup,grid_) ,SELINV_TAG_COUNT*supidx+SELINV_TAG_D_REDUCE, grid_->colComm, &mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" has sent "<< DiagBuf.m()*DiagBuf.n()*sizeof(Scalar) << " bytes of DiagBuf to " << PROW(ksup,grid_) << " isSendToDiagonal = "<< isSendToDiagonal_(ksup) <<  std::endl;
#endif
            }
#endif

          } // I do not own the diagonal block
          else{
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                  SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ), LUpdateBufReduced.m(),
                  Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                  SCALAR_ONE, DiagBuf.Data(), SuperSize( ksup, super_ ) );
            }
          } // I own the diagonal block, skip the diagonal block
        }
      }





#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Update_Diagonal);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Diagonal);
#endif
#endif

#ifdef USE_REDUCE_DIAG
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){




          NumMat<Scalar> & DiagBuf = arrDiagBuf[supidx];
//#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBuf: " << DiagBuf << std::endl << std::endl; 
//#endif

          NumMat<Scalar> DiagBufReduced( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ) );

          if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
            SetValue( DiagBufReduced, SCALAR_ZERO );

          if(countSendToBelow_(ksup)>1)
//          if(isSendToDiagonal_(ksup) || MYROW( grid_ ) == PROW( ksup, grid_ ) )
          {
            MPI_Comm * colComm = commSendToBelowPtr_[ksup];
            Int root = commSendToBelowRoot_[ksup];

            if(DiagBuf.m()==0 && DiagBuf.n()==0){
              DiagBuf.Resize( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ));
              SetValue(DiagBuf, SCALAR_ZERO);
            }



            mpi::Reduce( DiagBuf.Data(), DiagBufReduced.Data(), 
                SuperSize( ksup, super_ ) * SuperSize( ksup, super_ ),
                MPI_SUM, root, *colComm );
          }

          // Add DiagBufReduced to diagonal block.
          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){

            LBlock&  LB = this->L( LBj( ksup, grid_ ) )[0];
            // Symmetrize LB
            blas::Axpy( LB.numRow * LB.numCol, SCALAR_ONE, DiagBufReduced.Data(),
                1, LB.nzval.Data(), 1 );
            Symmetrize( LB.nzval );
          }
        } 
      }
#endif

#ifndef USE_REDUCE_DIAG
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){


          NumMat<Scalar> & DiagBuf = arrDiagBuf[supidx];
#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBuf: " << DiagBuf << std::endl << std::endl; 
#endif


          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
            //receive from below
            Int totCountRecv = 0;
            Int numRecv = CountRecvFromBelow(ksup);

#if ( _DEBUGlevel_ >= 1 )
            statusOFS << std::endl << "["<<ksup<<"] "<< "P"<<MYROW(grid_)<<" should receive DiagBuf from "<< numRecv << " processors" << std::endl;
#endif


            if(DiagBuf.m()==0 && DiagBuf.n()==0){
              DiagBuf.Resize( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ));
              SetValue(DiagBuf, SCALAR_ZERO);
            }
            else{
statusOFS<<"["<<ksup<<"] BCAST PROW "<<MYROW(grid_)<<" participates"<<std::endl;
            }

            NumMat<Scalar>  DiagBufRecv(DiagBuf.m(),DiagBuf.n());

            for( Int countRecv = 0; countRecv < numRecv ; ++countRecv ){
              //Do the blocking recv
              MPI_Status stat;
              Int size = 0;
              MPI_Recv(DiagBufRecv.Data(), SuperSize( ksup, super_ )*SuperSize( ksup, super_ )*sizeof(Scalar), MPI_BYTE, MPI_ANY_SOURCE,SELINV_TAG_COUNT*supidx+SELINV_TAG_D_REDUCE, grid_->colComm,&stat);
              MPI_Get_count(&stat, MPI_BYTE, &size);
#if ( _DEBUGlevel_ >= 1 )
              statusOFS << std::endl << "["<<ksup<<"] "<< "P"<<MYROW(grid_)<<" has received "<< size << " bytes of DiagBuf from " << stat.MPI_SOURCE << std::endl;
#endif
statusOFS<<"["<<ksup<<"] BCAST PROW "<<stat.MPI_SOURCE<<" participates"<<std::endl;
              //if the processor contributes
              if(size>0){
#if ( _DEBUGlevel_ >= 2 )
                statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBufRecv: " <<  DiagBufRecv << std::endl << std::endl; 
#endif

                //do the sum
                blas::Axpy(SuperSize( ksup, super_ )*SuperSize( ksup, super_ ), SCALAR_ONE, DiagBufRecv.Data(), 1, DiagBuf.Data(), 1 );
              }
            }
            // Add DiagBufReduced to diagonal block.
#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<   "DiagBufReduced: " << DiagBuf << std::endl << std::endl; 
#endif

            LBlock&  LB = this->L( LBj( ksup, grid_ ) )[0];
            // Symmetrize LB
            blas::Axpy( LB.numRow * LB.numCol, SCALAR_ONE, DiagBuf.Data(),
                1, LB.nzval.Data(), 1 );
            Symmetrize( LB.nzval );

#if ( _DEBUGlevel_ >= 2 )
            statusOFS << std::endl << "["<<ksup<<"] "<<   "Diag of Ainv: " << LB.nzval << std::endl << std::endl; 
#endif
          }
        } 
      }
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Diagonal);
#endif
#endif


#ifndef USE_REDUCE_DIAG


      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && isSendToDiagonal_(ksup) ){
          // Now all the Isend / Irecv should have finished.
          std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
          mpi::Wait( mpireqsSendToRight[0] );

#if ( _DEBUGlevel_ >= 1 )
          statusOFS << std::endl << "["<<ksup<<"] "<< " P"<<MYROW(grid_)<<" is done sending DiagBuf to P" << PROW(ksup,grid_) <<  std::endl;
#endif
        }
      }


#endif

#ifndef _RELEASE_
      PopCallStack();
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateU");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_U");
#elif defined(PROFILE)
      TAU_FSTART(Update_U);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl <<  "["<<ksup<<"] "<<  "Update the upper triangular block" << std::endl << std::endl; 
#endif
        // Send LUpdateBufReduced to the cross diagonal blocks. 
        // NOTE: This assumes square processor grid
        std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
        std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        Int numRowLUpdateBuf = LUpdateBufReduced.m();

#if not (defined(SEND_CROSSDIAG_ASYNC)) 
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTART(Send_L_CrossDiag);
#endif
#endif
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && isSendToCrossDiagonal_( ksup ) ){
          Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
          if( MYPROC( grid_ ) != dest	){
            std::stringstream sstm;
            serialize( rowLocalPtr, sstm, NO_MASK );
            serialize( blockIdxLocal, sstm, NO_MASK );
            serialize( LUpdateBufReduced, sstm, NO_MASK );
            mpi::Send( sstm, dest, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm );
          }
        } // sender
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif
#endif

        if( MYROW( grid_ ) == PROW( ksup, grid_ ) && isRecvFromCrossDiagonal_( ksup ) ){
          std::vector<Int>  rowLocalPtrRecv;
          std::vector<Int>  blockIdxLocalRecv;
          NumMat<Scalar> UUpdateBuf;
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Recv_L_CrossDiag);
#endif
#endif

          Int src = PNUM( MYCOL( grid_ ), PCOL( ksup, grid_ ), grid_ );
          if( MYPROC( grid_ ) != src ){
            std::stringstream sstm;
            mpi::Recv( sstm, src, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_SIZE, SELINV_TAG_COUNT*supidx+SELINV_TAG_L_CONTENT, grid_->comm );

            deserialize( rowLocalPtrRecv, sstm, NO_MASK );
            deserialize( blockIdxLocalRecv, sstm, NO_MASK );
            deserialize( UUpdateBuf, sstm, NO_MASK );	
          } // sender is not the same as receiver
          else{
            rowLocalPtrRecv   = rowLocalPtr;
            blockIdxLocalRecv = blockIdxLocal;
            UUpdateBuf = LUpdateBufReduced;
          } // sender is the same as receiver

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Recv_L_CrossDiag);
#endif
#endif

#if ( _DEBUGlevel_ >= 2 )
          statusOFS << std::endl << "["<<ksup<<"] "<<   "rowLocalPtrRecv:" << rowLocalPtrRecv << std::endl << std::endl; 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "blockIdxLocalRecv:" << blockIdxLocalRecv << std::endl << std::endl; 
          statusOFS << std::endl << "["<<ksup<<"] "<<   "UUpdateBuf:" << UUpdateBuf << std::endl << std::endl; 
#endif

          // Update U
          std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
          Int cntRow = 0;
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            UBlock& UB = Urow[jb];
            bool isBlockFound = false;
            NumMat<Scalar> Ltmp( UB.numCol, UB.numRow );
            for( Int ib = 0; ib < blockIdxLocalRecv.size(); ib++ ){
              if( UB.blockIdx == blockIdxLocalRecv[ib] ){
                lapack::Lacpy( 'A', Ltmp.m(), Ltmp.n(), 
                    &UUpdateBuf( rowLocalPtrRecv[ib], 0 ),
                    UUpdateBuf.m(), Ltmp.Data(), Ltmp.m() );
                cntRow += UB.numCol;
                isBlockFound = true;
              }
            }
            if( isBlockFound == false ){
              throw std::logic_error( "UBlock cannot find its update. Something is seriously wrong." );
            }
           Transpose( Ltmp, UB.nzval );
          } // for (jb)
          if( cntRow != UUpdateBuf.m() ){
            std::ostringstream msg;
            msg << "The number of rows received from L is " << UUpdateBuf.m()
              << ", which does not match the total number of columns in U which is "
              << cntRow <<  std::endl;
            throw std::runtime_error( msg.str().c_str() );
          }
        } // receiver
      }


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_U");
#elif defined(PROFILE)
      TAU_FSTOP(Update_U);
#endif
#endif

#ifndef _RELEASE_
      PopCallStack();
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInv::UpdateLFinal");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_L");
#elif defined(PROFILE)
      TAU_FSTART(Update_L);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];

#if ( _DEBUGlevel_ >= 1 )
        statusOFS << std::endl << "["<<ksup<<"] "<<  "Finish updating the L part by filling LUpdateBufReduced back to L" << std::endl << std::endl; 
#endif
        std::vector<Int> & rowLocalPtr = arrRowLocalPtr[supidx];
        std::vector<Int> & blockIdxLocal = arrBlockIdxLocal[supidx];
        NumMat<Scalar> & LUpdateBufReduced = arrLUpdateBuf[supidx];
        Int numRowLUpdateBuf = LUpdateBufReduced.m();

        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && numRowLUpdateBuf > 0 ){
          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              LBlock& LB = Lcol[ib];
              lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib], 0 ),
                  LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
            }
          } // I do not own the diagonal block
          else{
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              LBlock& LB = Lcol[ib];
              lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ),
                  LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
            }
          } // I owns the diagonal block
        } // Finish updating L	
      } // for (ksup) : Main loop


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_L");
#elif defined(PROFILE)
      TAU_FSTOP(Update_L);
#endif
#endif

#ifndef _RELEASE_
      PopCallStack();
#endif


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Barrier");
#elif defined(PROFILE)
      TAU_FSTART(Barrier);
#endif
#endif
      for (Int supidx=0; supidx<stepSuper; supidx++){
        Int ksup = superList[lidx][supidx];
        std::vector<MPI_Request> & mpireqsSendToRight = arrMpireqsSendToRight[supidx];
        std::vector<MPI_Request> & mpireqsSendToBelow = arrMpireqsSendToBelow[supidx];
        mpi::Waitall( mpireqsSendToRight );
        mpi::Waitall( mpireqsSendToBelow );


        //Barrier in the column required because of the reduction of Diagonal block = L^T Sinv L
        if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          MPI_Barrier(grid_->colComm);
        }     
      }
      mpi::Waitall( arrMpireqsRecvContentFromAny );
      mpi::Waitall( arrMpireqsRecvSizeFromAny );


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Barrier");
#elif defined(PROFILE)
      TAU_FSTOP(Barrier);
#endif
#endif



    }

#ifndef _RELEASE_
    PopCallStack();
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_STOP("SelInvBcast");
#elif defined(PROFILE)
    TAU_FSTOP(SelInvBcast);
#endif
#endif



#ifdef SELINV_MEMORY
#ifdef USE_TAU
    TAU_DISABLE_TRACKING_MEMORY();
#else
    globalBufSize += sizeof(superList) + superList.capacity()*sizeof(std::vector<Int>);
    for (Int lidx=0; lidx<numSteps ; lidx++){
      globalBufSize += superList[lidx].capacity()*sizeof(Int);
    }
    statusOFS<<std::endl<<"Maximum allocated buffer size is : "<< globalBufSize << " Bytes" <<std::endl;
#endif
#endif
    return ;
  }
#endif







#ifdef SANITY_CHECK

  void PMatrix::ConstructCommunicationPatternOriginal	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::ConstructCommunicationPattern");
#endif
    Int numSuper = this->NumSuper();
#ifndef _RELEASE_
    PushCallStack( "Initialize the communication pattern" );
#endif
    isSendToBelow_.Resize(grid_->numProcRow, numSuper);
    isSendToRight_.Resize(grid_->numProcCol, numSuper);
    isSendToCrossDiagonal_.Resize( numSuper );
    isSendToDiagonal_.Resize( numSuper );
    SetValue( isSendToBelow_, false );
    SetValue( isSendToRight_, false );
    SetValue( isSendToCrossDiagonal_, false );
    SetValue( isSendToDiagonal_, false );

    isRecvFromAbove_.Resize( numSuper );
    isRecvFromLeft_.Resize( numSuper );
    isRecvFromCrossDiagonal_.Resize( numSuper );
    isRecvFromBelow_.Resize( grid_->numProcCol, numSuper );
    SetValue( isRecvFromAbove_, false );
    SetValue( isRecvFromBelow_, false );
    SetValue( isRecvFromLeft_, false );
    SetValue( isRecvFromCrossDiagonal_, false );
#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local column communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local column communication" << std::endl;
#endif
    // localColBlockRowIdx stores the nonzero block indices for each local block column.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockCol x numNonzeroBlock
    std::vector<std::set<Int> >   localColBlockRowIdx;

    localColBlockRowIdx.resize( this->NumLocalBlockCol() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<Int>  tBlockRowIdx;
        tBlockRowIdx.clear();

        // L part
        std::vector<LBlock>& Lcol = this->L( LBj(ksup, grid_) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          tBlockRowIdx.push_back( Lcol[ib].blockIdx );
        }

        // U part
        for( Int ib = 0; ib < this->NumLocalBlockRow(); ib++ ){
          std::vector<UBlock>& Urow = this->U(ib);
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            if( Urow[jb].blockIdx == ksup ){
              tBlockRowIdx.push_back( GBi( ib, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockRowIdx;
        mpi::Allgatherv( tBlockRowIdx, tAllBlockRowIdx, grid_->colComm );

        localColBlockRowIdx[LBj( ksup, grid_ )].insert(
            tAllBlockRowIdx.begin(), tAllBlockRowIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Column block " << ksup 
          << " has the following nonzero block rows" << std::endl;
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for(ksup)


#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack( "Local row communication" );
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Local row communication" << std::endl;
#endif
    // localRowBlockColIdx stores the nonzero block indices for each local block row.
    // The nonzero block indices including contribution from both L and U.
    // Dimension: numLocalBlockRow x numNonzeroBlock
    std::vector<std::set<Int> >   localRowBlockColIdx;

    localRowBlockColIdx.resize( this->NumLocalBlockRow() );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // All block columns perform independently
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<Int>  tBlockColIdx;
        tBlockColIdx.clear();

        // U part
        std::vector<UBlock>& Urow = this->U( LBi(ksup, grid_) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          tBlockColIdx.push_back( Urow[jb].blockIdx );
        }

        // L part
        for( Int jb = 0; jb < this->NumLocalBlockCol(); jb++ ){
          std::vector<LBlock>& Lcol = this->L(jb);
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            if( Lcol[ib].blockIdx == ksup ){
              tBlockColIdx.push_back( GBj( jb, grid_ ) );
            }
          }
        }

        // Communication
        std::vector<Int> tAllBlockColIdx;
        mpi::Allgatherv( tBlockColIdx, tAllBlockColIdx, grid_->rowComm );

        localRowBlockColIdx[LBi( ksup, grid_ )].insert(
            tAllBlockColIdx.begin(), tAllBlockColIdx.end() );

#if ( _DEBUGlevel_ >= 1 )
        statusOFS 
          << " Row block " << ksup 
          << " has the following nonzero block columns" << std::endl;
        for( std::set<Int>::iterator si = localRowBlockColIdx[LBi( ksup, grid_ )].begin();
            si != localRowBlockColIdx[LBi( ksup, grid_ )].end();
            si++ ){
          statusOFS << *si << "  ";
        }
        statusOFS << std::endl; 
#endif

      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for(ksup)

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack("SendToBelow / RecvFromAbove");
#endif

    MPI_Group baseColGrp;
    MPI_Comm_group(grid_->colComm, &baseColGrp);


    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes to the right of ksup


      for( Int jsup = ksup + 1; jsup < numSuper; jsup++ ){
        Int jsupLocalBlockCol = LBj( jsup, grid_ );
        Int jsupProcCol = PCOL( jsup, grid_ );
        if( MYCOL( grid_ ) == jsupProcCol ){

          // SendToBelow / RecvFromAbove only if (ksup, jsup) is nonzero.
          if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 ) {
            for( std::set<Int>::iterator si = localColBlockRowIdx[jsupLocalBlockCol].begin();
                si != localColBlockRowIdx[jsupLocalBlockCol].end(); si++	 ){
              Int isup = *si;
              Int isupProcRow = PROW( isup, grid_ );
              if( isup > ksup ){
                if( MYROW( grid_ ) == isupProcRow ){
                  isRecvFromAbove_(ksup) = true;
                }
                if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
                  isSendToBelow_( isupProcRow, ksup ) = true;
                }
              } // if( isup > ksup )
            } // for (si)
          } // if( localColBlockRowIdx[jsupLocalBlockCol].count( ksup ) > 0 )

        } // if( MYCOL( grid_ ) == PCOL( jsup, grid_ ) )

      } // for(jsup)
    } // for(ksup)

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToBelow:" << isSendToBelow_ << std::endl;


    statusOFS << std::endl << "isSendToBelow:" << std::endl;
    for(int j = 0;j< isSendToBelow_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToBelow_.m();i++){
        statusOFS<< isSendToBelow_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }



    statusOFS << std::endl << "isRecvFromAbove:" << isRecvFromAbove_ << std::endl;
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif












#ifndef _RELEASE_
    PushCallStack("SendToRight / RecvFromLeft");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      // Loop over all the supernodes below ksup

      for( Int isup = ksup + 1; isup < numSuper; isup++ ){
        Int isupLocalBlockRow = LBi( isup, grid_ );
        Int isupProcRow       = PROW( isup, grid_ );
        if( MYROW( grid_ ) == isupProcRow ){
          // SendToRight / RecvFromLeft only if (isup, ksup) is nonzero.
          if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 ){
            for( std::set<Int>::iterator si = localRowBlockColIdx[isupLocalBlockRow].begin();
                si != localRowBlockColIdx[isupLocalBlockRow].end(); si++ ){
              Int jsup = *si;
              Int jsupProcCol = PCOL( jsup, grid_ );
              if( jsup > ksup ){

                if( MYCOL( grid_ ) == jsupProcCol ){
                  isRecvFromLeft_(ksup) = true;
                }
                if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
                  isSendToRight_( jsupProcCol, ksup ) = true;
                }
              }
            } // for (si)
          } // if( localRowBlockColIdx[isupLocalBlockRow].count( ksup ) > 0 )



        } // if( MYROW( grid_ ) == isupProcRow )


        if( MYCOL( grid_ ) == PCOL(ksup, grid_) ){

          if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){ 
            isRecvFromBelow_(isupProcRow,ksup) = true;
          }    
          else if (MYROW(grid_) == isupProcRow){
            isSendToDiagonal_(ksup)=true;
          }    
        } // if( MYCOL( grid_ ) == PCOL(ksup, grid_) )




      } // for (isup)





    }	 // for (ksup)


#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToRight:" << isSendToRight_ << std::endl;

    statusOFS << std::endl << "isSendToRight:" << std::endl;
    for(int j = 0;j< isSendToRight_.n();j++){
      statusOFS << "["<<j<<"] ";
      for(int i =0; i < isSendToRight_.m();i++){
        statusOFS<< isSendToRight_(i,j) << " ";
      }
      statusOFS<<std::endl;
    }

    statusOFS << std::endl << "isRecvFromLeft:" << isRecvFromLeft_ << std::endl;
    statusOFS << std::endl << "isRecvFromBelow:" << isRecvFromBelow_ << std::endl;
#endif
#ifndef _RELEASE_
    PopCallStack();
#endif

#ifndef _RELEASE_
    PushCallStack("SendToCrossDiagonal / RecvFromCrossDiagonal");
#endif
    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localColBlockRowIdx[LBj( ksup, grid_ )].begin();
            si != localColBlockRowIdx[LBj( ksup, grid_ )].end(); si++ ){
          Int isup = *si;
          Int isupProcRow = PROW( isup, grid_ );
          if( isup > ksup && MYROW( grid_ ) == isupProcRow ){
            isSendToCrossDiagonal_( ksup ) = true;
          }
        } // for (si)
      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for (ksup)

    for( Int ksup = 0; ksup < numSuper - 1; ksup++ ){
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        for( std::set<Int>::iterator si = localRowBlockColIdx[ LBi(ksup, grid_) ].begin();
            si != localRowBlockColIdx[ LBi(ksup, grid_) ].end(); si++ ){
          Int jsup = *si;
          Int jsupProcCol = PCOL( jsup, grid_ );
          if( jsup > ksup && MYCOL(grid_) == jsupProcCol ){
            isRecvFromCrossDiagonal_[ksup] = true;
          }
        } // for (si)
      } // if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
    } // for (ksup)
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "isSendToCrossDiagonal:" << isSendToCrossDiagonal_ << std::endl;
    statusOFS << std::endl << "isRecvFromCrossDiagonal:" << isRecvFromCrossDiagonal_ << std::endl;
#endif

#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif


    return ;
  } 		// -----  end of method PMatrix::ConstructCommunicationPattern  ----- 









  void PMatrix::SelInvOriginal	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::SelInvOriginal");
#endif
    Int numSuper = this->NumSuper(); 


#ifdef SELINV_TIMING
#if defined (PROFILE) || defined(PMPI) || defined(USE_TAU)
    TAU_PROFILE_SET_CONTEXT(MPI_COMM_WORLD);
#endif
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_START("SelInvOriginal");
#elif defined (PROFILE)
    TAU_FSTART(SelInvOriginal);
#endif
#endif


    for( Int ksup = numSuper - 2; ksup >= 0; ksup-- ){
#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInvOriginal::UpdateL");
#endif

      // Communication for the U part.
      std::vector<MPI_Request> mpireqsSendToBelow( 2 * grid_->numProcRow, MPI_REQUEST_NULL );
      std::vector<char> sstrUrowSend;

      // Senders
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        // Pack the data in U
        std::stringstream sstm;
        std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
        std::vector<UBlock>&  Urow = this->U( LBi(ksup, grid_) );
        // All blocks are to be sent down.
        serialize( (Int)Urow.size(), sstm, NO_MASK );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          serialize( Urow[jb], sstm, mask );
        }
        sstrUrowSend.resize( Size( sstm ) );
        sstm.read( &sstrUrowSend[0], sstrUrowSend.size() );
        for( Int iProcRow = 0; iProcRow < grid_->numProcRow; iProcRow++ ){
          if( MYROW( grid_ ) != iProcRow &&
              isSendToBelow_( iProcRow,ksup ) == true ){
            // Use Isend to send to multiple targets
            Int sizeStm = sstrUrowSend.size();
            MPI_Isend( &sizeStm, 1, MPI_INT,  
                iProcRow, SELINV_TAG_U_SIZE, 
                grid_->colComm, &mpireqsSendToBelow[grid_->numProcRow + iProcRow] );
            MPI_Isend( (void*)&sstrUrowSend[0], sizeStm, MPI_BYTE, 
                iProcRow, SELINV_TAG_U_CONTENT, 
                grid_->colComm, &mpireqsSendToBelow[iProcRow] );
          } // Send 
        } // for (iProcRow)
      } // if I am the sender

      // Communication for the L part.

      std::vector<MPI_Request> mpireqsSendToRight( 2 * grid_->numProcCol, MPI_REQUEST_NULL ); 
      std::vector<char> sstrLcolSend;

      // Senders
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        // Pack the data in L 
        std::stringstream sstm;
        std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
        mask[LBlockMask::NZVAL] = 0; // nzval is excluded 

        std::vector<LBlock>&  Lcol = this->L( LBj(ksup, grid_) );
        // All blocks except for the diagonal block are to be sent right
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
          serialize( (Int)Lcol.size() - 1, sstm, NO_MASK );
        else
          serialize( (Int)Lcol.size(), sstm, NO_MASK );

        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          if( Lcol[ib].blockIdx > ksup ){
            serialize( Lcol[ib], sstm, mask );
          }
        }
        sstrLcolSend.resize( Size( sstm ) );
        sstm.read( &sstrLcolSend[0], sstrLcolSend.size() );
        for( Int iProcCol = 0; iProcCol < grid_->numProcCol ; iProcCol++ ){
          if( MYCOL( grid_ ) != iProcCol &&
              isSendToRight_( iProcCol, ksup ) == true ){
            // Use Isend to send to multiple targets
            Int sizeStm = sstrLcolSend.size();
            MPI_Isend( &sizeStm, 1, MPI_INT,  
                iProcCol, SELINV_TAG_L_SIZE, 
                grid_->rowComm, &mpireqsSendToRight[grid_->numProcCol + iProcCol] );
            MPI_Isend( (void*)&sstrLcolSend[0], sizeStm, MPI_BYTE, 
                iProcCol, SELINV_TAG_L_CONTENT, 
                grid_->rowComm, &mpireqsSendToRight[iProcCol] );
          } // Send 
        } // for (iProcCol)
      } // if I am the sender

      // Receive
      std::vector<MPI_Request> mpireqsRecvFromAbove( 2, MPI_REQUEST_NULL ); 
      std::vector<MPI_Request> mpireqsRecvFromLeft( 2, MPI_REQUEST_NULL ); 
      Int sizeStmFromAbove, sizeStmFromLeft;

      std::vector<char>     sstrLcolRecv;
      std::vector<char>     sstrUrowRecv;

      // Receive the size first

      if( isRecvFromAbove_( ksup ) && 
          MYROW( grid_ ) != PROW( ksup, grid_ ) ){
        MPI_Irecv( &sizeStmFromAbove, 1, MPI_INT, PROW( ksup, grid_ ), 
            SELINV_TAG_U_SIZE,
            grid_->colComm, &mpireqsRecvFromAbove[0] );

      } // if I need to receive from up


      if( isRecvFromLeft_( ksup ) &&
          MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
        MPI_Irecv( &sizeStmFromLeft, 1, MPI_INT, PCOL( ksup, grid_ ), 
            SELINV_TAG_L_SIZE,
            grid_->rowComm, &mpireqsRecvFromLeft[0] );
      } // if I need to receive from left





#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("WaitSize_UL");
#elif defined(PROFILE)
          TAU_FSTART(WaitSize_UL);
#endif
#endif
      // Wait to obtain size information
      mpi::Wait( mpireqsRecvFromAbove[0] );
      mpi::Wait( mpireqsRecvFromLeft[0] );

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("WaitSize_UL");
#elif defined(PROFILE)
          TAU_FSTOP(WaitSize_UL);
#endif
#endif

      if( isRecvFromAbove_( ksup ) && 
          MYROW( grid_ ) != PROW( ksup, grid_ ) ){

        sstrUrowRecv.resize( sizeStmFromAbove );
        MPI_Irecv( &sstrUrowRecv[0], sizeStmFromAbove, MPI_BYTE, 
            PROW( ksup, grid_ ), SELINV_TAG_U_CONTENT, 
            grid_->colComm, &mpireqsRecvFromAbove[1] );
      } // if I need to receive from up

      if( isRecvFromLeft_( ksup ) &&
          MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){

        sstrLcolRecv.resize( sizeStmFromLeft );
        MPI_Irecv( &sstrLcolRecv[0], sizeStmFromLeft, MPI_BYTE, 
            PCOL( ksup, grid_ ), SELINV_TAG_L_CONTENT, 
            grid_->rowComm,
            &mpireqsRecvFromLeft[1] );
      } // if I need to receive from left

      // Wait for all communication to finish
      // Wait to obtain packed information in a string and then write into stringstream
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("WaitContent_UL");
#elif defined(PROFILE)
          TAU_FSTART(WaitContent_UL);
#endif
#endif

      mpi::Wait( mpireqsRecvFromAbove[1] );
      mpi::Wait( mpireqsRecvFromLeft[1] );
#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("WaitContent_UL");
#elif defined(PROFILE)
          TAU_FSTOP(WaitContent_UL);
#endif
#endif




      // Overlap the communication with computation.  All processors move
      // to Gemm phase when ready 
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Compute_Sinv_LT);
#endif
#endif



      std::vector<LBlock>   LcolRecv;
      std::vector<UBlock>   UrowRecv;
      if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){
        // U part
        if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          std::stringstream sstm;
          sstm.write( &sstrUrowRecv[0], sizeStmFromAbove );
          std::vector<Int> mask( UBlockMask::TOTAL_NUMBER, 1 );
          Int numUBlock;
          deserialize( numUBlock, sstm, NO_MASK );
          UrowRecv.resize( numUBlock );
          for( Int jb = 0; jb < numUBlock; jb++ ){
            deserialize( UrowRecv[jb], sstm, mask );
          } 
        } // sender is not the same as receiver
        else{
          // U is obtained locally, just make a copy. Include everything
          // (there is no diagonal block)
          UrowRecv = this->U( LBi( ksup, grid_ ) );
        } // sender is the same as receiver

        if( MYCOL( grid_ ) != PCOL( ksup, grid_ ) ){
          std::stringstream     sstm;
          sstm.write( &sstrLcolRecv[0], sizeStmFromLeft );
          std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
          mask[LBlockMask::NZVAL] = 0; // nzval is excluded
          Int numLBlock;
          deserialize( numLBlock, sstm, NO_MASK );
          LcolRecv.resize( numLBlock );
          for( Int ib = 0; ib < numLBlock; ib++ ){
            deserialize( LcolRecv[ib], sstm, mask );
          }
        } // sender is not the same as receiver
        else{
          // L is obtained locally, just make a copy. 
          // Do not include the diagonal block
          std::vector<LBlock>& Lcol =  this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            LcolRecv.resize( Lcol.size() );
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              LcolRecv[ib] = Lcol[ib];
            }
          }
          else{
            LcolRecv.resize( Lcol.size() - 1 );
            for( Int ib = 0; ib < Lcol.size() - 1; ib++ ){
              LcolRecv[ib] = Lcol[ib+1];
            }
          }
        } // sender is the same as receiver
      } // if I am a receiver

      // Save all the data to be updated for { L( isup, ksup ) | isup > ksup }.
      // The size will be updated in the Gemm phase and the reduce phase
      NumMat<Scalar>  LUpdateBuf;

      // Only the processors received information participate in the Gemm 
      if( isRecvFromAbove_( ksup ) && isRecvFromLeft_( ksup ) ){

#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
        TAU_FSTART(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif


        // rowPtr[ib] gives the row index in LUpdateBuf for the first
        // nonzero row in LcolRecv[ib]. The total number of rows in
        // LUpdateBuf is given by rowPtr[end]-1
        std::vector<Int> rowPtr(LcolRecv.size() + 1);
        // colPtr[jb] gives the column index in UBuf for the first
        // nonzero column in UrowRecv[jb]. The total number of rows in
        // UBuf is given by colPtr[end]-1
        std::vector<Int> colPtr(UrowRecv.size() + 1);

        rowPtr[0] = 0;
        for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
          rowPtr[ib+1] = rowPtr[ib] + LcolRecv[ib].numRow;
        }
        colPtr[0] = 0;
        for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
          colPtr[jb+1] = colPtr[jb] + UrowRecv[jb].numCol;
        }

        Int numRowAinvBuf = *rowPtr.rbegin();
        Int numColAinvBuf = *colPtr.rbegin();

        // Allocate for the computational storage
        NumMat<Scalar> AinvBuf( numRowAinvBuf, numColAinvBuf );
        LUpdateBuf.Resize( numRowAinvBuf, SuperSize( ksup, super_ ) );
        NumMat<Scalar> UBuf( SuperSize( ksup, super_ ), numColAinvBuf );
        SetValue( AinvBuf, SCALAR_ZERO );
        SetValue( LUpdateBuf, SCALAR_ZERO );
        SetValue( UBuf, SCALAR_ZERO );

        // Fill UBuf first.  Make the transpose later in the Gemm phase.
        for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
          UBlock& UB = UrowRecv[jb];
          if( UB.numRow != SuperSize(ksup, super_) ){
            throw std::logic_error( "The size of UB is not right.  Something is seriously wrong." );
          }
          lapack::Lacpy( 'A', UB.numRow, UB.numCol, UB.nzval.Data(),
              UB.numRow, UBuf.VecData( colPtr[jb] ), UBuf.m() );	
        }


        // Calculate the relative indices for (isup, jsup)
        // Fill AinvBuf with the information in L or U block.
        for( Int jb = 0; jb < UrowRecv.size(); jb++ ){
          for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
            LBlock& LB = LcolRecv[ib];
            UBlock& UB = UrowRecv[jb];
            Int isup = LB.blockIdx;
            Int jsup = UB.blockIdx;
            Scalar* nzvalAinv = &AinvBuf( rowPtr[ib], colPtr[jb] );
            Int     ldAinv    = AinvBuf.m();

            // Pin down the corresponding block in the part of Sinv.
            if( isup >= jsup ){
              std::vector<LBlock>&  LcolSinv = this->L( LBj(jsup, grid_ ) );
              bool isBlockFound = false;
              for( Int ibSinv = 0; ibSinv < LcolSinv.size(); ibSinv++ ){
                // Found the (isup, jsup) block in Sinv
                if( LcolSinv[ibSinv].blockIdx == isup ){
                  LBlock& SinvB = LcolSinv[ibSinv];

                  // Row relative indices
                  std::vector<Int> relRows( LB.numRow );
                  Int* rowsLBPtr    = LB.rows.Data();
                  Int* rowsSinvBPtr = SinvB.rows.Data();
                  for( Int i = 0; i < LB.numRow; i++ ){
                    bool isRowFound = false;
                    for( Int i1 = 0; i1 < SinvB.numRow; i1++ ){
                      if( rowsLBPtr[i] == rowsSinvBPtr[i1] ){
                        isRowFound = true;
                        relRows[i] = i1;
                        break;
                      }
                    }
                    if( isRowFound == false ){
                      std::ostringstream msg;
                      msg << "Row " << rowsLBPtr[i] << 
                        " in LB cannot find the corresponding row in SinvB" << std::endl
                        << "LB.rows    = " << LB.rows << std::endl
                        << "SinvB.rows = " << SinvB.rows << std::endl;
                      throw std::runtime_error( msg.str().c_str() );
                    }
                  }

                  // Column relative indicies
                  std::vector<Int> relCols( UB.numCol );
                  Int SinvColsSta = FirstBlockCol( jsup, super_ );
                  for( Int j = 0; j < UB.numCol; j++ ){
                    relCols[j] = UB.cols[j] - SinvColsSta;
                  }

                  // Transfer the values from Sinv to AinvBlock
                  Scalar* nzvalSinv = SinvB.nzval.Data();
                  Int     ldSinv    = SinvB.numRow;
                  for( Int j = 0; j < UB.numCol; j++ ){
                    for( Int i = 0; i < LB.numRow; i++ ){
                      nzvalAinv[i+j*ldAinv] =
                        nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                    }
                  }

                  isBlockFound = true;
                  break;
                }	
              } // for (ibSinv )
              if( isBlockFound == false ){
                std::ostringstream msg;
                msg << "Block(" << isup << ", " << jsup 
                  << ") did not find a matching block in Sinv." << std::endl;
                throw std::runtime_error( msg.str().c_str() );
              }
            } // if (isup, jsup) is in L
            else{
              std::vector<UBlock>&   UrowSinv = this->U( LBi( isup, grid_ ) );
              bool isBlockFound = false;
              for( Int jbSinv = 0; jbSinv < UrowSinv.size(); jbSinv++ ){
                // Found the (isup, jsup) block in Sinv
                if( UrowSinv[jbSinv].blockIdx == jsup ){
                  UBlock& SinvB = UrowSinv[jbSinv];

                  // Row relative indices
                  std::vector<Int> relRows( LB.numRow );
                  Int SinvRowsSta = FirstBlockCol( isup, super_ );
                  for( Int i = 0; i < LB.numRow; i++ ){
                    relRows[i] = LB.rows[i] - SinvRowsSta;
                  }

                  // Column relative indices
                  std::vector<Int> relCols( UB.numCol );
                  Int* colsUBPtr    = UB.cols.Data();
                  Int* colsSinvBPtr = SinvB.cols.Data();
                  for( Int j = 0; j < UB.numCol; j++ ){
                    bool isColFound = false;
                    for( Int j1 = 0; j1 < SinvB.numCol; j1++ ){
                      if( colsUBPtr[j] == colsSinvBPtr[j1] ){
                        isColFound = true;
                        relCols[j] = j1;
                        break;
                      }
                    }
                    if( isColFound == false ){
                      std::ostringstream msg;
                      msg << "Col " << colsUBPtr[j] << 
                        " in UB cannot find the corresponding row in SinvB" << std::endl
                        << "UB.cols    = " << UB.cols << std::endl
                        << "UinvB.cols = " << SinvB.cols << std::endl;
                      throw std::runtime_error( msg.str().c_str() );
                    }
                  }

                  // Trasnfer the values from Sinv to AinvBlock
                  Scalar* nzvalSinv = SinvB.nzval.Data();
                  Int     ldSinv    = SinvB.numRow;
                  for( Int j = 0; j < UB.numCol; j++ ){
                    for( Int i = 0; i < LB.numRow; i++ ){
                      nzvalAinv[i+j*ldAinv] =
                        nzvalSinv[relRows[i] + relCols[j] * ldSinv];
                    }
                  }

                  isBlockFound = true;
                  break;
                }
              } // for (jbSinv)
              if( isBlockFound == false ){
                std::ostringstream msg;
                msg << "Block(" << isup << ", " << jsup 
                  << ") did not find a matching block in Sinv." << std::endl;
                throw std::runtime_error( msg.str().c_str() );
              }
            } // if (isup, jsup) is in U

          } // for( ib )
        } // for ( jb )
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_STOP("Compute_Sinv_LT_Lookup_Indexes");
#elif defined(PROFILE)
        TAU_FSTOP(Compute_Sinv_LT_Lookup_Indexes);
#endif
#endif


#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
          TAU_FSTART(Compute_Sinv_LT_GEMM);
#endif
#endif

        // Gemm for LUpdateBuf = AinvBuf * UBuf^T
        blas::Gemm( 'N', 'T', AinvBuf.m(), UBuf.m(), AinvBuf.n(), SCALAR_MINUS_ONE, 
            AinvBuf.Data(), AinvBuf.m(), 
            UBuf.Data(), UBuf.m(), SCALAR_ZERO,
            LUpdateBuf.Data(), LUpdateBuf.m() ); 


#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Compute_Sinv_LT_GEMM");
#elif defined(PROFILE)
          TAU_FSTOP(Compute_Sinv_LT_GEMM);
#endif
#endif



      } // if Gemm is to be done locally
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Compute_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Compute_Sinv_LT);
#endif
#endif




      // Now all the Isend / Irecv should have finished.
      mpi::Waitall( mpireqsSendToRight );
      mpi::Waitall( mpireqsSendToBelow );


      // Reduce LUpdateBuf across all the processors in the same processor row.

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Sinv_LT);
#endif
#endif
      NumMat<Scalar> LUpdateBufReduced;

      // Processor column of ksup collects the symbolic data for LUpdateBuf.
      std::vector<Int>  rowLocalPtr;
      std::vector<Int>  blockIdxLocal;
      Int numRowLUpdateBuf;
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          rowLocalPtr.resize( Lcol.size() + 1 );
          blockIdxLocal.resize( Lcol.size() );
          rowLocalPtr[0] = 0;
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            rowLocalPtr[ib+1] = rowLocalPtr[ib] + Lcol[ib].numRow;
            blockIdxLocal[ib] = Lcol[ib].blockIdx;
          }
        } // I do not own the diaogonal block
        else{
          rowLocalPtr.resize( Lcol.size() );
          blockIdxLocal.resize( Lcol.size() - 1 );
          rowLocalPtr[0] = 0;
          for( Int ib = 1; ib < Lcol.size(); ib++ ){
            rowLocalPtr[ib] = rowLocalPtr[ib-1] + Lcol[ib].numRow;
            blockIdxLocal[ib-1] = Lcol[ib].blockIdx;
          }
        } // I owns the diagonal block, skip the diagonal block
        numRowLUpdateBuf = *rowLocalPtr.rbegin();
        if( numRowLUpdateBuf > 0 ){
          LUpdateBufReduced.Resize( numRowLUpdateBuf, SuperSize( ksup, super_ ) );
          SetValue( LUpdateBufReduced, SCALAR_ZERO );
        }
      } 

      // Processor column sends the total row dimension to all processors
      // in the same row to prepare for reduce
      MPI_Bcast( &numRowLUpdateBuf, 1, MPI_INT, PCOL( ksup, grid_ ), grid_->rowComm );

      // If LUpdatebuf has not been constructed, resize and fill with zero
      if( numRowLUpdateBuf > 0 ){
        if( LUpdateBuf.m() == 0 && LUpdateBuf.n() == 0 ){
          LUpdateBuf.Resize( numRowLUpdateBuf, SuperSize( ksup, super_ ) );
          // Fill zero is important
          SetValue( LUpdateBuf, SCALAR_ZERO );
        }


        mpi::Reduce( LUpdateBuf.Data(), LUpdateBufReduced.Data(),
            numRowLUpdateBuf * SuperSize( ksup, super_ ), MPI_SUM, 
            PCOL( ksup, grid_ ), grid_->rowComm );


      } // Perform reduce for nonzero block rows in the column of ksup


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Sinv_LT");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Sinv_LT);
#endif
#endif

#ifndef _RELEASE_
      PopCallStack();
#endif


#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInvOriginal::UpdateD");
#endif


      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Update_Diagonal);
#endif
#endif
        NumMat<Scalar> DiagBuf( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ) );
        SetValue( DiagBuf, SCALAR_ZERO );
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib], 0 ), LUpdateBufReduced.m(),
                Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                SCALAR_ONE, DiagBuf.Data(), DiagBuf.m() );
statusOFS<<"["<<ksup<<"] PROW "<<MYROW(grid_)<<" participates"<<std::endl;
          }
        } // I do not own the diaogonal block
        else{
          for( Int ib = 1; ib < Lcol.size(); ib++ ){
            blas::Gemm( 'T', 'N', SuperSize( ksup, super_ ), SuperSize( ksup, super_ ), Lcol[ib].numRow, 
                SCALAR_MINUS_ONE, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ), LUpdateBufReduced.m(),	
                Lcol[ib].nzval.Data(), Lcol[ib].nzval.m(), 
                SCALAR_ONE, DiagBuf.Data(), DiagBuf.m() );
statusOFS<<"["<<ksup<<"] PROW "<<MYROW(grid_)<<" participates"<<std::endl;
          }
        } // I owns the diagonal block, skip the diagonal block

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Update_Diagonal);
#endif
#endif




#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTART(Reduce_Diagonal);
#endif
#endif
        NumMat<Scalar> DiagBufReduced( SuperSize( ksup, super_ ), SuperSize( ksup, super_ ) );

        if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
          SetValue( DiagBufReduced, SCALAR_ZERO );

        mpi::Reduce( DiagBuf.Data(), DiagBufReduced.Data(), 
            SuperSize( ksup, super_ ) * SuperSize( ksup, super_ ),
           MPI_SUM, PROW( ksup, grid_ ), grid_->colComm );


        // Add DiagBufReduced to diagonal block.
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
          LBlock&  LB = this->L( LBj( ksup, grid_ ) )[0];

          // Symmetrize LB
          blas::Axpy( LB.numRow * LB.numCol, SCALAR_ONE, DiagBufReduced.Data(),
              1, LB.nzval.Data(), 1 );

          Symmetrize( LB.nzval );
        }

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Reduce_Diagonal");
#elif defined(PROFILE)
      TAU_FSTOP(Reduce_Diagonal);
#endif
#endif
      } // Update the diagonal in the processor column of ksup. All processors participate


#ifndef _RELEASE_
      PopCallStack();
#endif



#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInvOriginal::UpdateU");
#endif


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_U");
#elif defined(PROFILE)
      TAU_FSTART(Update_U);
#endif
#endif
      // Send LUpdateBufReduced to the cross diagonal blocks. 
      // NOTE: This assumes square processor grid
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_START("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTART(Send_L_CrossDiag);
#endif
#endif

      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && isSendToCrossDiagonal_( ksup ) ){
        Int dest = PNUM( PROW( ksup, grid_ ), MYROW( grid_ ), grid_ );
        if( MYPROC( grid_ ) != dest	){
          std::stringstream sstm;
          serialize( rowLocalPtr, sstm, NO_MASK );
          serialize( blockIdxLocal, sstm, NO_MASK );
          serialize( LUpdateBufReduced, sstm, NO_MASK );
          mpi::Send( sstm, dest, SELINV_TAG_D_SIZE, SELINV_TAG_D_CONTENT, grid_->comm );
        }
      } // sender
#ifdef SELINV_TIMING
#ifdef USE_TAU
        TAU_STOP("Send_L_CrossDiag");
#elif defined(PROFILE)
        TAU_FSTOP(Send_L_CrossDiag);
#endif
#endif

      std::vector<Int>  rowLocalPtrRecv;
      std::vector<Int>  blockIdxLocalRecv;
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) && isRecvFromCrossDiagonal_( ksup ) ){
        Int src = PNUM( MYCOL( grid_ ), PCOL( ksup, grid_ ), grid_ );
        NumMat<Scalar> UUpdateBuf;

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_START("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTART(Recv_L_CrossDiag);
#endif
#endif



        if( MYPROC( grid_ ) != src ){
          std::stringstream sstm;
          mpi::Recv( sstm, src, SELINV_TAG_D_SIZE, SELINV_TAG_D_CONTENT, grid_->comm );

          deserialize( rowLocalPtrRecv, sstm, NO_MASK );
          deserialize( blockIdxLocalRecv, sstm, NO_MASK );
          deserialize( UUpdateBuf, sstm, NO_MASK );	
        } // sender is not the same as receiver
        else{
          rowLocalPtrRecv   = rowLocalPtr;
          blockIdxLocalRecv = blockIdxLocal;
          UUpdateBuf = LUpdateBufReduced;
        } // sender is the same as receiver

#ifdef SELINV_TIMING
#ifdef USE_TAU
          TAU_STOP("Recv_L_CrossDiag");
#elif defined(PROFILE)
          TAU_FSTOP(Recv_L_CrossDiag);
#endif
#endif


        // Update U
        std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
        Int cntRow = 0;
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          UBlock& UB = Urow[jb];
          bool isBlockFound = false;
          NumMat<Scalar> Ltmp( UB.numCol, UB.numRow );
          for( Int ib = 0; ib < blockIdxLocalRecv.size(); ib++ ){
            if( UB.blockIdx == blockIdxLocalRecv[ib] ){
              lapack::Lacpy( 'A', Ltmp.m(), Ltmp.n(), 
                  &UUpdateBuf( rowLocalPtrRecv[ib], 0 ),
                  UUpdateBuf.m(), Ltmp.Data(), Ltmp.m() );
              cntRow += UB.numCol;
              isBlockFound = true;
            }
          }
          if( isBlockFound == false ){
            throw std::logic_error( "UBlock cannot find its update. Something is seriously wrong." );
          }
          Transpose( Ltmp, UB.nzval );
        } // for (jb)
        if( cntRow != UUpdateBuf.m() ){
          std::ostringstream msg;
          msg << "The number of rows received from L is " << UUpdateBuf.m()
            << ", which does not match the total number of columns in U which is "
            << cntRow <<  std::endl;
          throw std::runtime_error( msg.str().c_str() );
        }
      } // receiver

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_U");
#elif defined(PROFILE)
      TAU_FSTOP(Update_U);
#endif
#endif
#ifndef _RELEASE_
      PopCallStack();
#endif

#ifndef _RELEASE_
      PushCallStack("PMatrix::SelInvOriginal::UpdateLFinal");
#endif

#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_START("Update_L");
#elif defined(PROFILE)
      TAU_FSTART(Update_L);
#endif
#endif



      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) && numRowLUpdateBuf > 0 ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
          for( Int ib = 0; ib < Lcol.size(); ib++ ){
            LBlock& LB = Lcol[ib];
            lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib], 0 ),
                LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
          }
        } // I do not own the diagonal block
        else{
          for( Int ib = 1; ib < Lcol.size(); ib++ ){
            LBlock& LB = Lcol[ib];
            lapack::Lacpy( 'A', LB.numRow, LB.numCol, &LUpdateBufReduced( rowLocalPtr[ib-1], 0 ),
                LUpdateBufReduced.m(), LB.nzval.Data(), LB.numRow );
          }
        } // I owns the diagonal block
      } // Finish updating L	


#ifdef SELINV_TIMING
#ifdef USE_TAU
      TAU_STOP("Update_L");
#elif defined(PROFILE)
      TAU_FSTOP(Update_L);
#endif
#endif


#ifndef _RELEASE_
      PopCallStack();
#endif

      MPI_Barrier( grid_-> comm );

    } // for (ksup) : Main loop

#ifdef SELINV_TIMING
#ifdef USE_TAU
    TAU_STOP("SelInvOriginal");
#elif defined (PROFILE)
    TAU_FSTOP(SelInvOriginal);
#endif
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::SelInvOriginal  ----- 
#endif

  void PMatrix::PreSelInv	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::PreSelInv");
#endif

    Int numSuper = this->NumSuper(); 

#ifndef _RELEASE_
    PushCallStack("L(i,k) <- L(i,k) * L(k,k)^{-1}");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "L(i,k) <- L(i,k) * L(k,k)^{-1}" << std::endl << std::endl; 
#endif
    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        // Broadcast the diagonal L block
        NumMat<Scalar> nzvalLDiag;
        std::vector<LBlock>& Lcol = this->L( LBj( ksup, grid_ ) );
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
          nzvalLDiag = Lcol[0].nzval;
          if( nzvalLDiag.m() != SuperSize(ksup, super_) ||
              nzvalLDiag.n() != SuperSize(ksup, super_) ){
            throw std::runtime_error( "The size of the diagonal block of L is wrong." );
          }
        } // Owns the diagonal block
        else
        {
          nzvalLDiag.Resize( SuperSize(ksup, super_), SuperSize(ksup, super_) );
        }
        MPI_Bcast( (void*)nzvalLDiag.Data(), 
            sizeof(Scalar) * SuperSize(ksup, super_) * SuperSize(ksup, super_),
            MPI_BYTE, PROW( ksup, grid_ ), grid_->colComm );

        // Triangular solve
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          LBlock& LB = Lcol[ib];
          if( LB.blockIdx > ksup ){
#if ( _DEBUGlevel_ >= 2 )
            // Check the correctness of the triangular solve for the first local column
            if( LBj( ksup, grid_ ) == 0 ){
              statusOFS << "Diag   L(" << ksup << ", " << ksup << "): " << nzvalLDiag << std::endl;
              statusOFS << "Before solve L(" << LB.blockIdx << ", " << ksup << "): " << LB.nzval << std::endl;
            }
#endif
            blas::Trsm( 'R', 'L', 'N', 'U', LB.numRow, LB.numCol, SCALAR_ONE,
                nzvalLDiag.Data(), LB.numCol, LB.nzval.Data(), LB.numRow );
#if ( _DEBUGlevel_ >= 2 )
            // Check the correctness of the triangular solve for the first local column
            if( LBj( ksup, grid_ ) == 0 ){
              statusOFS << "After solve  L(" << LB.blockIdx << ", " << ksup << "): " << LB.nzval << std::endl;
            }
#endif
          }
        }
      } // if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) )
    } // for (ksup)


#ifndef _RELEASE_
    PopCallStack();
#endif


#ifndef _RELEASE_
    PushCallStack("U(k,i) <- L(i,k)");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "U(k,i) <- L(i,k)" << std::endl << std::endl; 
#endif

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      Int ksupProcRow = PROW( ksup, grid_ );
      Int ksupProcCol = PCOL( ksup, grid_ );

      // Sender
      if( isSendToCrossDiagonal_[ksup]  &&
          MYPROC( grid_ ) !=  PNUM( ksupProcRow, MYROW( grid_ ), grid_ ) ){
        // Pack L data
        std::stringstream sstm;
        std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
        std::vector<LBlock>&  Lcol = this->L( LBj(ksup, grid_) );
        // All blocks except for the diagonal block are to be sent right
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) )
          serialize( (Int)Lcol.size() - 1, sstm, NO_MASK );
        else
          serialize( (Int)Lcol.size(), sstm, NO_MASK );

        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          if( Lcol[ib].blockIdx > ksup ) 
            serialize( Lcol[ib], sstm, mask );
        }
        // Send/Recv is possible here due to the one to one correspondence
        // in the case of square processor grid
        mpi::Send( sstm, PNUM( ksupProcRow, MYROW( grid_ ), grid_ ), 
            SELINV_TAG_D_SIZE, SELINV_TAG_D_CONTENT, grid_->comm );
      } // if I am a sender

      // Receiver
      if( isRecvFromCrossDiagonal_[ksup] ){

        std::vector<LBlock> LcolRecv;
        if( PNUM( MYCOL( grid_ ), ksupProcCol, grid_ ) != MYPROC( grid_ ) ){
          std::stringstream sstm;
          mpi::Recv( sstm, PNUM( MYCOL( grid_ ), ksupProcCol, grid_ ), 
              SELINV_TAG_D_SIZE, SELINV_TAG_D_CONTENT, grid_->comm );

          // Unpack L data.  
          Int numLBlock;
          std::vector<Int> mask( LBlockMask::TOTAL_NUMBER, 1 );
          deserialize( numLBlock, sstm, NO_MASK );
          LcolRecv.resize(numLBlock);
          for( Int ib = 0; ib < numLBlock; ib++ ){
            deserialize( LcolRecv[ib], sstm, mask );
          }
        } // sender is not the same as receiver
        else{
          // L is obtained locally, just make a copy. Do not include the diagonal block
          std::vector<LBlock>& Lcol = this->L( LBj( ksup, grid_ ) );
          if( MYROW( grid_ ) != PROW( ksup, grid_ ) ){
            LcolRecv.resize( Lcol.size() );
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              LcolRecv[ib] = Lcol[ib];
            }
          }
          else{
            LcolRecv.resize( Lcol.size() - 1 );
            for( Int ib = 0; ib < Lcol.size() - 1; ib++ ){
              LcolRecv[ib] = Lcol[ib+1];
            }
          }
        } // sender is the same as receiver

        // Update U
        // Make sure that the size of L and the corresponding U blocks match.
        std::vector<UBlock>& Urow = this->U( LBi( ksup, grid_ ) );
        for( Int ib = 0; ib < LcolRecv.size(); ib++ ){
          LBlock& LB = LcolRecv[ib];
          if( LB.blockIdx <= ksup ){
            throw std::logic_error( "LcolRecv contains the wrong blocks." );
          }
          bool isUBFound = false;
          for( Int jb = 0; jb < Urow.size(); jb++ ){
            UBlock&  UB = Urow[jb];
            if( LB.blockIdx == UB.blockIdx ){
              // Compare size
              if( LB.numRow != UB.numCol || LB.numCol != UB.numRow ){
                std::ostringstream msg;
                msg << "LB(" << LB.blockIdx << ", " << ksup << ") and UB(" 
                  << ksup << ", " << UB.blockIdx << ")	do not share the same size." << std::endl
                  << "LB: " << LB.numRow << " x " << LB.numCol << std::endl
                  << "UB: " << UB.numRow << " x " << UB.numCol << std::endl;
                throw std::runtime_error( msg.str().c_str() );
              }

              // Note that the order of the column indices of the U
              // block may not follow the order of the row indices,
              // overwrite the information in U.
              UB.cols = LB.rows;
              Transpose( LB.nzval, UB.nzval );

              isUBFound = true;
              break;
            } // if( LB.blockIdx == UB.blockIdx )
          } // for (jb)
          // Did not find a matching block
          if( !isUBFound ){
            std::ostringstream msg;
            msg << "LB(" << LB.blockIdx << ", " << ksup << ") did not find a matching block in U." << std::endl;
            throw std::runtime_error( msg.str().c_str() );
          }
        } // for (ib)
      } // if I am a receiver
    } // for (ksup)

#ifndef _RELEASE_
    PopCallStack();
#endif

#ifndef _RELEASE_
    PushCallStack("L(i,i) <- [L(k,k) * U(k,k)]^{-1} ");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "L(i,i) <- [L(k,k) * U(k,k)]^{-1}" << std::endl << std::endl; 
#endif

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) &&
          MYCOL( grid_ ) == PCOL( ksup, grid_ )	){
        IntNumVec ipiv( SuperSize( ksup, super_ ) );
        // Note that the pivoting vector ipiv should follow the FORTRAN
        // notation by adding the +1
        for(Int i = 0; i < SuperSize( ksup, super_ ); i++){
          ipiv[i] = i + 1;
        }
        LBlock& LB = (this->L( LBj( ksup, grid_ ) ))[0];
#if ( _DEBUGlevel_ >= 2 )
        // Check the correctness of the matrix inversion for the first local column
        statusOFS << "Factorized A (" << ksup << ", " << ksup << "): " << LB.nzval << std::endl;
#endif
        lapack::Getri( SuperSize( ksup, super_ ), LB.nzval.Data(), 
            SuperSize( ksup, super_ ), ipiv.Data() );

        // Symmetrize the diagonal block
        Symmetrize( LB.nzval );
#if ( _DEBUGlevel_ >= 2 )
        // Check the correctness of the matrix inversion for the first local column
        statusOFS << "Inversed   A (" << ksup << ", " << ksup << "): " << LB.nzval << std::endl;
#endif
      } // if I need to inverse the diagonal block
    } // for (ksup)


#ifndef _RELEASE_
    PopCallStack();
#endif



#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::PreSelInv  ----- 


  void PMatrix::GetDiagonal	( NumVec<Scalar>& diag )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::GetDiagonal");
#endif
    Int numSuper = this->NumSuper(); 

    Int numCol = this->NumCol();
    const IntNumVec& permInv = super_->permInv;

    NumVec<Scalar> diagLocal( numCol );
    SetValue( diagLocal, SCALAR_ZERO );

    diag.Resize( numCol );
    SetValue( diag, SCALAR_ZERO );


    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // I own the diagonal block	
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) &&
          MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        LBlock& LB = this->L( LBj( ksup, grid_ ) )[0];
        for( Int i = 0; i < LB.numRow; i++ ){
          diagLocal( permInv( LB.rows(i) ) ) = LB.nzval( i, i );
        }
      }
    }

    // All processors own diag
    mpi::Allreduce( diagLocal.Data(), diag.Data(), numCol, MPI_SUM, grid_->comm );

#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::GetDiagonal  ----- 


#ifdef SANITY_CHECK
  void PMatrix::CompareDiagonal	( PMatrix & Ref, Real & globalMaxError )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::CompareDiagonal");
#endif
    Int numSuper = this->NumSuper(); 

    Int numCol = this->NumCol();
    Real maxError = 0;

    for( Int ksup = numSuper - 2; ksup >= 0; ksup-- ){
        // I own the diagonal block	
        if( MYROW( grid_ ) == PROW( ksup, grid_ ) &&
            MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
          LBlock& LB = this->L( LBj( ksup, grid_ ) )[0];
          LBlock& LBRef = Ref.L( LBj( ksup, grid_ ) )[0];

          for( Int i = 0; i < LB.numRow; i++ ){
            std::stringstream msg;
            Real error = abs(LB.nzval(i,i)-LBRef.nzval(i,i))/abs(LBRef.nzval(i,i));
            msg<< "["<<ksup<< "] Row "<<i<<" is wrong : "<< LB.nzval(i,i) << " vs "<<LBRef.nzval(i,i)<< " error is "<< error <<std::endl; 
            if( error > maxError){ maxError = error;}
            if(error > SANITY_PRECISION){
              statusOFS<<msg;
            }

            //            if(abs(LB.nzval(i,i)-LBRef.nzval(i,i))>SANITY_PRECISION){throw std::runtime_error(msg.str());}
          }
        }
    }

    globalMaxError = 0;
    mpi::Allreduce( &maxError, &globalMaxError, 1, MPI_MAX, grid_->comm );

#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::CompareDiagonal  ----- 








  void PMatrix::CompareOffDiagonal	( PMatrix & Ref, Real & globalMaxError )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::CompareOffDiagonal");
#endif
    Int numSuper = this->NumSuper(); 

    Int numCol = this->NumCol();
    Real maxError = 0;



    for( Int ksup = numSuper - 2; ksup >= 0; ksup-- ){
        if( MYROW( grid_ ) == PROW( ksup, grid_ )){
          std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
          std::vector<UBlock>&  UrowRef = Ref.U( LBi( ksup, grid_ ) );
            for( Int ib = 0; ib < Urow.size(); ib++ ){
              for( Int i = 0; i < Urow[ib].numRow; i++ ){
                for( Int j = 0; j < Urow[ib].numCol; j++ ){
                  std::stringstream msg;
                  Real error = abs(Urow[ib].nzval(i,j)-UrowRef[ib].nzval(i,j))/abs(UrowRef[ib].nzval(i,j));
                  msg<< "["<<ksup<< "] U Block "<<ib<< " Row "<<i<<" Col "<<j<<" is wrong : "<< Urow[ib].nzval(i,j) << " vs "<<UrowRef[ib].nzval(i,j)<< " error is "<< error <<std::endl; 
                  if( error > maxError){ maxError = error;}
                  if(error > SANITY_PRECISION){
                    statusOFS<<msg;
                  }
                }
              }         
            }
        }

        if ( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){

          std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
          std::vector<LBlock>&  LcolRef = Ref.L( LBj( ksup, grid_ ) );

          if( MYROW( grid_ ) == PROW( ksup, grid_ )){
            for( Int ib = 1; ib < Lcol.size(); ib++ ){
              for( Int i = 0; i < Lcol[ib].numRow; i++ ){
                for( Int j = 0; j < Lcol[ib].numCol; j++ ){
                  std::stringstream msg;
                  Real error = abs(Lcol[ib].nzval(i,j)-LcolRef[ib].nzval(i,j))/abs(LcolRef[ib].nzval(i,j));
                  msg<< "["<<ksup<< "] L Block "<<ib<< " Row "<<i<<" Col "<<j<<" is wrong : "<< Lcol[ib].nzval(i,j) << " vs "<<LcolRef[ib].nzval(i,j)<< " error is "<< error <<std::endl; 
                  if( error > maxError){ maxError = error;}
                  if(error > SANITY_PRECISION){
                    statusOFS<<msg;
                  }
                }
              }         
            }
          }
          else{
            for( Int ib = 0; ib < Lcol.size(); ib++ ){
              for( Int i = 0; i < Lcol[ib].numRow; i++ ){
                for( Int j = 0; j < Lcol[ib].numCol; j++ ){
                  std::stringstream msg;
                  Real error = abs(Lcol[ib].nzval(i,j)-LcolRef[ib].nzval(i,j))/abs(LcolRef[ib].nzval(i,j));
                  msg<< "["<<ksup<< "] L Block "<<ib<< " Row "<<i<<" Col "<<j<<" is wrong : "<< Lcol[ib].nzval(i,j) << " vs "<<LcolRef[ib].nzval(i,j)<< " error is "<< error <<std::endl; 
                  if( error > maxError){ maxError = error;}
                  if(error > SANITY_PRECISION){
                    statusOFS<<msg;
                 }
                }
              }         
            }
          }
        }
    }

    globalMaxError = 0;
    mpi::Allreduce( &maxError, &globalMaxError, 1, MPI_MAX, grid_->comm );

#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::CompareDiagonal  ----- 










#endif


  void PMatrix::PMatrixToDistSparseMatrix	( DistSparseMatrix<Scalar>& A )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::PMatrixToDistSparseMatrix");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Converting PMatrix to DistSparseMatrix." << std::endl;
#endif
    Int mpirank = grid_->mpirank;
    Int mpisize = grid_->mpisize;

    std::vector<Int>     rowSend( mpisize );
    std::vector<Int>     colSend( mpisize );
    std::vector<Scalar>  valSend( mpisize );
    std::vector<Int>     sizeSend( mpisize, 0 );
    std::vector<Int>     displsSend( mpisize, 0 );

    std::vector<Int>     rowRecv( mpisize );
    std::vector<Int>     colRecv( mpisize );
    std::vector<Scalar>  valRecv( mpisize );
    std::vector<Int>     sizeRecv( mpisize, 0 );
    std::vector<Int>     displsRecv( mpisize, 0 );

    Int numSuper = this->NumSuper();
    const IntNumVec& permInv = super_->permInv;

    // The number of local columns in DistSparseMatrix format for the
    // processor with rank 0.  This number is the same for processors
    // with rank ranging from 0 to mpisize - 2, and may or may not differ
    // from the number of local columns for processor with rank mpisize -
    // 1.
    Int numColFirst = this->NumCol() / mpisize;

    // Count the size first.
    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // L blocks
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          for( Int j = 0; j < Lcol[ib].numCol; j++ ){
            Int jcol = permInv( j + FirstBlockCol( ksup, super_ ) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            sizeSend[dest] += Lcol[ib].numRow;
          }
        }
      } // I own the column of ksup 

      // U blocks
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          IntNumVec& cols = Urow[jb].cols;
          for( Int j = 0; j < cols.m(); j++ ){
            Int jcol = permInv( cols(j) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            sizeSend[dest] += Urow[jb].numRow;
          }
        }
      } // I own the row of ksup
    } // for (ksup)

    // All-to-all exchange of size information
    MPI_Alltoall( 
        &sizeSend[0], 1, MPI_INT,
        &sizeRecv[0], 1, MPI_INT, grid_->comm );



    // Reserve the space
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( ip == 0 ){
        displsSend[ip] = 0;
      }
      else{
        displsSend[ip] = displsSend[ip-1] + sizeSend[ip-1];
      }

      if( ip == 0 ){
        displsRecv[ip] = 0;
      }
      else{
        displsRecv[ip] = displsRecv[ip-1] + sizeRecv[ip-1];
      }
    }
    Int sizeSendTotal = displsSend[mpisize-1] + sizeSend[mpisize-1];
    Int sizeRecvTotal = displsRecv[mpisize-1] + sizeRecv[mpisize-1];

    rowSend.resize( sizeSendTotal );
    colSend.resize( sizeSendTotal );
    valSend.resize( sizeSendTotal );

    rowRecv.resize( sizeRecvTotal );
    colRecv.resize( sizeRecvTotal );
    valRecv.resize( sizeRecvTotal );

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "displsSend = " << displsSend << std::endl;
    statusOFS << "displsRecv = " << displsRecv << std::endl;
#endif

    // Put (row, col, val) to the sending buffer
    std::vector<Int>   cntSize( mpisize, 0 );

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // L blocks
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          IntNumVec&  rows = Lcol[ib].rows;
          NumMat<Scalar>& nzval = Lcol[ib].nzval;
          for( Int j = 0; j < Lcol[ib].numCol; j++ ){
            Int jcol = permInv( j + FirstBlockCol( ksup, super_ ) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            for( Int i = 0; i < rows.m(); i++ ){
              rowSend[displsSend[dest] + cntSize[dest]] = permInv( rows(i) );
              colSend[displsSend[dest] + cntSize[dest]] = jcol;
              valSend[displsSend[dest] + cntSize[dest]] = nzval( i, j );
              cntSize[dest]++;
            }
          }
        }
      } // I own the column of ksup 

      // U blocks
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          IntNumVec& cols = Urow[jb].cols;
          NumMat<Scalar>& nzval = Urow[jb].nzval;
          for( Int j = 0; j < cols.m(); j++ ){
            Int jcol = permInv( cols(j) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            for( Int i = 0; i < Urow[jb].numRow; i++ ){
              rowSend[displsSend[dest] + cntSize[dest]] = 
                permInv( i + FirstBlockCol( ksup, super_ ) );
              colSend[displsSend[dest] + cntSize[dest]] = jcol;
              valSend[displsSend[dest] + cntSize[dest]] = nzval( i, j );
              cntSize[dest]++;
            }
          }
        }
      } // I own the row of ksup
    }

    // Check sizes match
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( cntSize[ip] != sizeSend[ip] )
        throw std::runtime_error( "Sizes of the sending information do not match." );
    }


    // Alltoallv to exchange information
    mpi::Alltoallv( 
        &rowSend[0], &sizeSend[0], &displsSend[0],
        &rowRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );
    mpi::Alltoallv( 
        &colSend[0], &sizeSend[0], &displsSend[0],
        &colRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );
    mpi::Alltoallv( 
        &valSend[0], &sizeSend[0], &displsSend[0],
        &valRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "Alltoallv communication finished." << std::endl;
#endif

    //#if ( _DEBUGlevel_ >= 1 )
    //	for( Int ip = 0; ip < mpisize; ip++ ){
    //		statusOFS << "rowSend[" << ip << "] = " << rowSend[ip] << std::endl;
    //		statusOFS << "rowRecv[" << ip << "] = " << rowRecv[ip] << std::endl;
    //		statusOFS << "colSend[" << ip << "] = " << colSend[ip] << std::endl;
    //		statusOFS << "colRecv[" << ip << "] = " << colRecv[ip] << std::endl;
    //		statusOFS << "valSend[" << ip << "] = " << valSend[ip] << std::endl;
    //		statusOFS << "valRecv[" << ip << "] = " << valRecv[ip] << std::endl;
    //	}
    //#endif

    // Organize the received message.
    Int firstCol = mpirank * numColFirst;
    Int numColLocal;
    if( mpirank == mpisize-1 )
      numColLocal = this->NumCol() - numColFirst * (mpisize-1);
    else
      numColLocal = numColFirst;

    std::vector<std::vector<Int> > rows( numColLocal );
    std::vector<std::vector<Scalar> > vals( numColLocal );

    for( Int ip = 0; ip < mpisize; ip++ ){
      Int*     rowRecvCur = &rowRecv[displsRecv[ip]];
      Int*     colRecvCur = &colRecv[displsRecv[ip]];
      Scalar*  valRecvCur = &valRecv[displsRecv[ip]];
      for( Int i = 0; i < sizeRecv[ip]; i++ ){
        rows[colRecvCur[i]-firstCol].push_back( rowRecvCur[i] );
        vals[colRecvCur[i]-firstCol].push_back( valRecvCur[i] );
      } // for (i)
    } // for (ip)

    // Sort the rows
    std::vector<std::vector<Int> > sortIndex( numColLocal );
    for( Int j = 0; j < numColLocal; j++ ){
      sortIndex[j].resize( rows[j].size() );
      for( Int i = 0; i < sortIndex[j].size(); i++ )
        sortIndex[j][i] = i;
      std::sort( sortIndex[j].begin(), sortIndex[j].end(),
          IndexComp<std::vector<Int>& > ( rows[j] ) );
    } // for (j)

    // Form DistSparseMatrix according to the received message	
    // NOTE: for indicies,  DistSparseMatrix follows the FORTRAN
    // convention (1 based) while PMatrix follows the C convention (0
    // based)
    A.size = this->NumCol();
    A.nnzLocal  = 0;
    A.colptrLocal.Resize( numColLocal + 1 );
    // Note that 1 is important since the index follows the FORTRAN convention
    A.colptrLocal(0) = 1;
    for( Int j = 0; j < numColLocal; j++ ){
      A.nnzLocal += rows[j].size();
      A.colptrLocal(j+1) = A.colptrLocal(j) + rows[j].size();
    }

    // TODO Potentially make A.nnz using Long format
    mpi::Allreduce( &A.nnzLocal, &A.nnz, 1, MPI_SUM, grid_->comm );
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "nnzLocal = " << A.nnzLocal << std::endl;
    statusOFS << "nnz      = " << A.nnz      << std::endl;
#endif


    A.rowindLocal.Resize( A.nnzLocal );
    A.nzvalLocal.Resize(  A.nnzLocal );
    A.comm = grid_->comm;

    Int*     rowPtr = A.rowindLocal.Data();
    Scalar*  nzvalPtr = A.nzvalLocal.Data();
    for( Int j = 0; j < numColLocal; j++ ){
      std::vector<Int>& rowsCur = rows[j];
      std::vector<Int>& sortIndexCur = sortIndex[j];
      std::vector<Scalar>& valsCur = vals[j];
      for( Int i = 0; i < rows[j].size(); i++ ){
        // Note that 1 is important since the index follows the FORTRAN convention
        *(rowPtr++)   = rowsCur[sortIndexCur[i]] + 1;
        *(nzvalPtr++) = valsCur[sortIndexCur[i]]; 
      }
    }

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "A.colptrLocal[end]   = " << A.colptrLocal(numColLocal) << std::endl;
    statusOFS << "A.rowindLocal.size() = " << A.rowindLocal.m() << std::endl;
    statusOFS << "A.rowindLocal[end]   = " << A.rowindLocal(A.nnzLocal-1) << std::endl;
    statusOFS << "A.nzvalLocal[end]    = " << A.nzvalLocal(A.nnzLocal-1) << std::endl;
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::PMatrixToDistSparseMatrix  ----- 



  void PMatrix::PMatrixToDistSparseMatrix	( const DistSparseMatrix<Scalar>& A, DistSparseMatrix<Scalar>& B	)
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::PMatrixToDistSparseMatrix");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Converting PMatrix to DistSparseMatrix (2nd format)." << std::endl;
#endif
    Int mpirank = grid_->mpirank;
    Int mpisize = grid_->mpisize;

    std::vector<Int>     rowSend( mpisize );
    std::vector<Int>     colSend( mpisize );
    std::vector<Scalar>  valSend( mpisize );
    std::vector<Int>     sizeSend( mpisize, 0 );
    std::vector<Int>     displsSend( mpisize, 0 );

    std::vector<Int>     rowRecv( mpisize );
    std::vector<Int>     colRecv( mpisize );
    std::vector<Scalar>  valRecv( mpisize );
    std::vector<Int>     sizeRecv( mpisize, 0 );
    std::vector<Int>     displsRecv( mpisize, 0 );

    Int numSuper = this->NumSuper();
    const IntNumVec& permInv = super_->permInv;

    // The number of local columns in DistSparseMatrix format for the
    // processor with rank 0.  This number is the same for processors
    // with rank ranging from 0 to mpisize - 2, and may or may not differ
    // from the number of local columns for processor with rank mpisize -
    // 1.
    Int numColFirst = this->NumCol() / mpisize;

    // Count the size first.
    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // L blocks
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          for( Int j = 0; j < Lcol[ib].numCol; j++ ){
            Int jcol = permInv( j + FirstBlockCol( ksup, super_ ) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            sizeSend[dest] += Lcol[ib].numRow;
          }
        }
      } // I own the column of ksup 

      // U blocks
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          IntNumVec& cols = Urow[jb].cols;
          for( Int j = 0; j < cols.m(); j++ ){
            Int jcol = permInv( cols(j) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            sizeSend[dest] += Urow[jb].numRow;
          }
        }
      } // I own the row of ksup
    } // for (ksup)

    // All-to-all exchange of size information
    MPI_Alltoall( 
        &sizeSend[0], 1, MPI_INT,
        &sizeRecv[0], 1, MPI_INT, grid_->comm );



    // Reserve the space
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( ip == 0 ){
        displsSend[ip] = 0;
      }
      else{
        displsSend[ip] = displsSend[ip-1] + sizeSend[ip-1];
      }

      if( ip == 0 ){
        displsRecv[ip] = 0;
      }
      else{
        displsRecv[ip] = displsRecv[ip-1] + sizeRecv[ip-1];
      }
    }
    Int sizeSendTotal = displsSend[mpisize-1] + sizeSend[mpisize-1];
    Int sizeRecvTotal = displsRecv[mpisize-1] + sizeRecv[mpisize-1];

    rowSend.resize( sizeSendTotal );
    colSend.resize( sizeSendTotal );
    valSend.resize( sizeSendTotal );

    rowRecv.resize( sizeRecvTotal );
    colRecv.resize( sizeRecvTotal );
    valRecv.resize( sizeRecvTotal );

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "displsSend = " << displsSend << std::endl;
    statusOFS << "displsRecv = " << displsRecv << std::endl;
#endif

    // Put (row, col, val) to the sending buffer
    std::vector<Int>   cntSize( mpisize, 0 );


    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // L blocks
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>&  Lcol = this->L( LBj( ksup, grid_ ) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          IntNumVec&  rows = Lcol[ib].rows;
          NumMat<Scalar>& nzval = Lcol[ib].nzval;
          for( Int j = 0; j < Lcol[ib].numCol; j++ ){
            Int jcol = permInv( j + FirstBlockCol( ksup, super_ ) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            for( Int i = 0; i < rows.m(); i++ ){
              rowSend[displsSend[dest] + cntSize[dest]] = permInv( rows(i) );
              colSend[displsSend[dest] + cntSize[dest]] = jcol;
              valSend[displsSend[dest] + cntSize[dest]] = nzval( i, j );
              cntSize[dest]++;
            }
          }
        }
      } // I own the column of ksup 


      // U blocks
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<UBlock>&  Urow = this->U( LBi( ksup, grid_ ) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          IntNumVec& cols = Urow[jb].cols;
          NumMat<Scalar>& nzval = Urow[jb].nzval;
          for( Int j = 0; j < cols.m(); j++ ){
            Int jcol = permInv( cols(j) );
            Int dest = std::min( jcol / numColFirst, mpisize - 1 );
            for( Int i = 0; i < Urow[jb].numRow; i++ ){
              rowSend[displsSend[dest] + cntSize[dest]] = 
                permInv( i + FirstBlockCol( ksup, super_ ) );
              colSend[displsSend[dest] + cntSize[dest]] = jcol;
              valSend[displsSend[dest] + cntSize[dest]] = nzval( i, j );
              cntSize[dest]++;
            }
          }
        }
      } // I own the row of ksup
    }



    // Check sizes match
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( cntSize[ip] != sizeSend[ip] )
        throw std::runtime_error( "Sizes of the sending information do not match." );
    }

    // Alltoallv to exchange information
    mpi::Alltoallv( 
        &rowSend[0], &sizeSend[0], &displsSend[0],
        &rowRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );
    mpi::Alltoallv( 
        &colSend[0], &sizeSend[0], &displsSend[0],
        &colRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );
    mpi::Alltoallv( 
        &valSend[0], &sizeSend[0], &displsSend[0],
        &valRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "Alltoallv communication finished." << std::endl;
#endif

    //#if ( _DEBUGlevel_ >= 1 )
    //	for( Int ip = 0; ip < mpisize; ip++ ){
    //		statusOFS << "rowSend[" << ip << "] = " << rowSend[ip] << std::endl;
    //		statusOFS << "rowRecv[" << ip << "] = " << rowRecv[ip] << std::endl;
    //		statusOFS << "colSend[" << ip << "] = " << colSend[ip] << std::endl;
    //		statusOFS << "colRecv[" << ip << "] = " << colRecv[ip] << std::endl;
    //		statusOFS << "valSend[" << ip << "] = " << valSend[ip] << std::endl;
    //		statusOFS << "valRecv[" << ip << "] = " << valRecv[ip] << std::endl;
    //	}
    //#endif

    // Organize the received message.
    Int firstCol = mpirank * numColFirst;
    Int numColLocal;
    if( mpirank == mpisize-1 )
      numColLocal = this->NumCol() - numColFirst * (mpisize-1);
    else
      numColLocal = numColFirst;

    std::vector<std::vector<Int> > rows( numColLocal );
    std::vector<std::vector<Scalar> > vals( numColLocal );

    for( Int ip = 0; ip < mpisize; ip++ ){
      Int*     rowRecvCur = &rowRecv[displsRecv[ip]];
      Int*     colRecvCur = &colRecv[displsRecv[ip]];
      Scalar*  valRecvCur = &valRecv[displsRecv[ip]];
      for( Int i = 0; i < sizeRecv[ip]; i++ ){
        rows[colRecvCur[i]-firstCol].push_back( rowRecvCur[i] );
        vals[colRecvCur[i]-firstCol].push_back( valRecvCur[i] );
      } // for (i)
    } // for (ip)

    // Sort the rows
    std::vector<std::vector<Int> > sortIndex( numColLocal );
    for( Int j = 0; j < numColLocal; j++ ){
      sortIndex[j].resize( rows[j].size() );
      for( Int i = 0; i < sortIndex[j].size(); i++ )
        sortIndex[j][i] = i;
      std::sort( sortIndex[j].begin(), sortIndex[j].end(),
          IndexComp<std::vector<Int>& > ( rows[j] ) );
    } // for (j)

    // Form DistSparseMatrix according to the received message	
    // NOTE: for indicies,  DistSparseMatrix follows the FORTRAN
    // convention (1 based) while PMatrix follows the C convention (0
    // based)
    if( A.size != this->NumCol() ){
      throw std::runtime_error( "The DistSparseMatrix providing the pattern has a different size from PMatrix." );
    }
    if( A.colptrLocal.m() != numColLocal + 1 ){
      throw std::runtime_error( "The DistSparseMatrix providing the pattern has a different number of local columns from PMatrix." );
    }

    B.size = A.size;
    B.nnz  = A.nnz;
    B.nnzLocal = A.nnzLocal;
    B.colptrLocal = A.colptrLocal;
    B.rowindLocal = A.rowindLocal;
    B.nzvalLocal.Resize( B.nnzLocal );
    SetValue( B.nzvalLocal, SCALAR_ZERO );
    // Make sure that the communicator of A and B are the same.
    if( grid_->comm != A.comm ){
      throw std::runtime_error( "The DistSparseMatrix providing the pattern has a different communicator from PMatrix." );
    }
    B.comm = grid_->comm;

    Int*     rowPtr = B.rowindLocal.Data();
    Scalar*  nzvalPtr = B.nzvalLocal.Data();
    for( Int j = 0; j < numColLocal; j++ ){
      std::vector<Int>& rowsCur = rows[j];
      std::vector<Int>& sortIndexCur = sortIndex[j];
      std::vector<Scalar>& valsCur = vals[j];
      std::vector<Int>  rowsCurSorted( rowsCur.size() );
      // Note that 1 is important since the index follows the FORTRAN convention
      for( Int i = 0; i < rowsCurSorted.size(); i++ ){
        rowsCurSorted[i] = rowsCur[sortIndexCur[i]] + 1;
      }

      // Search and match the indices
      std::vector<Int>::iterator it;
      for( Int i = B.colptrLocal(j) - 1; 
          i < B.colptrLocal(j+1) - 1; i++ ){
        it = std::lower_bound( rowsCurSorted.begin(), rowsCurSorted.end(),
            *(rowPtr++) );
        if( it == rowsCurSorted.end() ){
          // Did not find the row, set it to zero
          *(nzvalPtr++) = SCALAR_ZERO;
        }
        else{
          // Found the row, set it according to the received value
          *(nzvalPtr++) = valsCur[ sortIndexCur[it-rowsCurSorted.begin()] ];
        }
      } // for (i)	
    } // for (j)

#if ( _DEBUGlevel_ >= 1 )
    statusOFS << "B.colptrLocal[end]   = " << B.colptrLocal(numColLocal) << std::endl;
    statusOFS << "B.rowindLocal.size() = " << B.rowindLocal.m() << std::endl;
    statusOFS << "B.rowindLocal[end]   = " << B.rowindLocal(B.nnzLocal-1) << std::endl;
    statusOFS << "B.nzvalLocal[end]    = " << B.nzvalLocal(B.nnzLocal-1) << std::endl;
#endif


#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::PMatrixToDistSparseMatrix  ----- 


  // A (maybe) more memory efficient way for converting the PMatrix to a
  // DistSparseMatrix structure.
  //
  // FIXME NOTE: This routine assumes the matrix to be symmetric!
  void PMatrix::PMatrixToDistSparseMatrix2 ( const DistSparseMatrix<Scalar>& A, DistSparseMatrix<Scalar>& B )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::PMatrixToDistSparseMatrix2");
#endif
#if ( _DEBUGlevel_ >= 1 )
    statusOFS << std::endl << "Converting PMatrix to DistSparseMatrix (2nd format)." << std::endl;
#endif
    Int mpirank = grid_->mpirank;
    Int mpisize = grid_->mpisize;

    std::vector<Int>     rowSend( mpisize );
    std::vector<Int>     colSend( mpisize );
    std::vector<Scalar>  valSend( mpisize );
    std::vector<Int>     sizeSend( mpisize, 0 );
    std::vector<Int>     displsSend( mpisize, 0 );

    std::vector<Int>     rowRecv( mpisize );
    std::vector<Int>     colRecv( mpisize );
    std::vector<Scalar>  valRecv( mpisize );
    std::vector<Int>     sizeRecv( mpisize, 0 );
    std::vector<Int>     displsRecv( mpisize, 0 );

    Int numSuper = this->NumSuper();
    const IntNumVec& perm    = super_->perm;
    const IntNumVec& permInv = super_->permInv;


    // Count the sizes from the A matrix first
    Int numColFirst = this->NumCol() / mpisize;
    Int firstCol = mpirank * numColFirst;
    Int numColLocal;
    if( mpirank == mpisize-1 )
      numColLocal = this->NumCol() - numColFirst * (mpisize-1);
    else
      numColLocal = numColFirst;

    Int*     rowPtr = A.rowindLocal.Data();
    Int*     colPtr = A.colptrLocal.Data();

    for( Int j = 0; j < numColLocal; j++ ){
      Int col         = perm( firstCol + j );
      Int blockColIdx = BlockIdx( col, super_ );
      Int procCol     = PCOL( blockColIdx, grid_ );
      for( Int i = colPtr[j] - 1; i < colPtr[j+1] - 1; i++ ){
        Int row         = perm( *(rowPtr++) - 1 );
        Int blockRowIdx = BlockIdx( row, super_ );
        Int procRow     = PROW( blockRowIdx, grid_ );
        Int dest = PNUM( procRow, procCol, grid_ );
#if ( _DEBUGlevel_ >= 1 )
        statusOFS << "BlockIdx = " << blockRowIdx << ", " <<blockColIdx << std::endl;
        statusOFS << procRow << ", " << procCol << ", " 
          << dest << std::endl;
#endif
        sizeSend[dest]++;
      } // for (i)
    } // for (j)

    // All-to-all exchange of size information
    MPI_Alltoall( 
        &sizeSend[0], 1, MPI_INT,
        &sizeRecv[0], 1, MPI_INT, grid_->comm );

#if ( _DEBUGlevel_ >= 0 )
    statusOFS << std::endl << "sizeSend: " << sizeSend << std::endl;
    statusOFS << std::endl << "sizeRecv: " << sizeRecv << std::endl;
#endif



    // Reserve the space
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( ip == 0 ){
        displsSend[ip] = 0;
      }
      else{
        displsSend[ip] = displsSend[ip-1] + sizeSend[ip-1];
      }

      if( ip == 0 ){
        displsRecv[ip] = 0;
      }
      else{
        displsRecv[ip] = displsRecv[ip-1] + sizeRecv[ip-1];
      }
    }

    Int sizeSendTotal = displsSend[mpisize-1] + sizeSend[mpisize-1];
    Int sizeRecvTotal = displsRecv[mpisize-1] + sizeRecv[mpisize-1];

    rowSend.resize( sizeSendTotal );
    colSend.resize( sizeSendTotal );
    valSend.resize( sizeSendTotal );

    rowRecv.resize( sizeRecvTotal );
    colRecv.resize( sizeRecvTotal );
    valRecv.resize( sizeRecvTotal );

#if ( _DEBUGlevel_ >= 0 )
    statusOFS << "displsSend = " << displsSend << std::endl;
    statusOFS << "displsRecv = " << displsRecv << std::endl;
#endif

    // Put (row, col) to the sending buffer
    std::vector<Int>   cntSize( mpisize, 0 );

    rowPtr = A.rowindLocal.Data();
    colPtr = A.colptrLocal.Data();

    for( Int j = 0; j < numColLocal; j++ ){
      Int col         = perm( firstCol + j );
      Int blockColIdx = BlockIdx( col, super_ );
      Int procCol     = PCOL( blockColIdx, grid_ );
      for( Int i = colPtr[j] - 1; i < colPtr[j+1] - 1; i++ ){
        Int row         = perm( *(rowPtr++) - 1 );
        Int blockRowIdx = BlockIdx( row, super_ );
        Int procRow     = PROW( blockRowIdx, grid_ );
        Int dest = PNUM( procRow, procCol, grid_ );
        rowSend[displsSend[dest] + cntSize[dest]] = row;
        colSend[displsSend[dest] + cntSize[dest]] = col;
        cntSize[dest]++;
      } // for (i)
    } // for (j)


    // Check sizes match
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( cntSize[ip] != sizeSend[ip] )
        throw std::runtime_error( "Sizes of the sending information do not match." );
    }

    // Alltoallv to exchange information
    mpi::Alltoallv( 
        &rowSend[0], &sizeSend[0], &displsSend[0],
        &rowRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );
    mpi::Alltoallv( 
        &colSend[0], &sizeSend[0], &displsSend[0],
        &colRecv[0], &sizeRecv[0], &displsRecv[0],
        grid_->comm );

#if ( _DEBUGlevel_ >= 0 )
    statusOFS << "Alltoallv communication of nonzero indices finished." << std::endl;
#endif


#if ( _DEBUGlevel_ >= 1 )
    for( Int ip = 0; ip < mpisize; ip++ ){
      statusOFS << "rowSend[" << ip << "] = " << rowSend[ip] << std::endl;
      statusOFS << "rowRecv[" << ip << "] = " << rowRecv[ip] << std::endl;
      statusOFS << "colSend[" << ip << "] = " << colSend[ip] << std::endl;
      statusOFS << "colRecv[" << ip << "] = " << colRecv[ip] << std::endl;
    }
#endif

    // For each (row, col), fill the nonzero values to valRecv locally.
    for( Int g = 0; g < sizeRecvTotal; g++ ){
      Int row = rowRecv[g];
      Int col = colRecv[g];

      Int blockRowIdx = BlockIdx( row, super_ );
      Int blockColIdx = BlockIdx( col, super_ );

      // Search for the nzval
      bool isFound = false;

      if( blockColIdx <= blockRowIdx ){
        // Data on the L side

        std::vector<LBlock>&  Lcol = this->L( LBj( blockColIdx, grid_ ) );

        for( Int ib = 0; ib < Lcol.size(); ib++ ){
#if ( _DEBUGlevel_ >= 1 )
          statusOFS << "blockRowIdx = " << blockRowIdx << ", Lcol[ib].blockIdx = " << Lcol[ib].blockIdx << ", blockColIdx = " << blockColIdx << std::endl;
#endif
          if( Lcol[ib].blockIdx == blockRowIdx ){
            IntNumVec& rows = Lcol[ib].rows;
            for( int iloc = 0; iloc < Lcol[ib].numRow; iloc++ ){
              if( rows[iloc] == row ){
                Int jloc = col - FirstBlockCol( blockColIdx, super_ );
                valRecv[g] = Lcol[ib].nzval( iloc, jloc );
                isFound = true;
                break;
              } // found the corresponding row
            }
          }
          if( isFound == true ) break;  
        } // for (ib)
      } 
      else{
        // Data on the U side

        std::vector<UBlock>&  Urow = this->U( LBi( blockRowIdx, grid_ ) );

        for( Int jb = 0; jb < Urow.size(); jb++ ){
          if( Urow[jb].blockIdx == blockColIdx ){
            IntNumVec& cols = Urow[jb].cols;
            for( int jloc = 0; jloc < Urow[jb].numCol; jloc++ ){
              if( cols[jloc] == col ){
                Int iloc = row - FirstBlockRow( blockRowIdx, super_ );
                valRecv[g] = Urow[jb].nzval( iloc, jloc );
                isFound = true;
                break;
              } // found the corresponding col
            }
          }
          if( isFound == true ) break;  
        } // for (jb)
      } // if( blockColIdx <= blockRowIdx ) 

      // Did not find the corresponding row, set the value to zero.
      if( isFound == false ){
        statusOFS << "In the permutated order, (" << row << ", " << col <<
          ") is not found in PMatrix." << std::endl;
        valRecv[g] = SCALAR_ZERO;
      }

    } // for (g)


    // Feed back valRecv to valSend through Alltoallv. NOTE: for the
    // values, the roles of "send" and "recv" are swapped.
    mpi::Alltoallv( 
        &valRecv[0], &sizeRecv[0], &displsRecv[0],
        &valSend[0], &sizeSend[0], &displsSend[0],
        grid_->comm );

#if ( _DEBUGlevel_ >= 0 )
    statusOFS << "Alltoallv communication of nonzero values finished." << std::endl;
#endif

    // Put the nonzero values from valSend to the matrix B.
    B.size = A.size;
    B.nnz  = A.nnz;
    B.nnzLocal = A.nnzLocal;
    B.colptrLocal = A.colptrLocal;
    B.rowindLocal = A.rowindLocal;
    B.nzvalLocal.Resize( B.nnzLocal );
    SetValue( B.nzvalLocal, SCALAR_ZERO );
    // Make sure that the communicator of A and B are the same.
    if( grid_->comm != A.comm ){
      throw std::runtime_error( "The DistSparseMatrix providing the pattern has a different communicator from PMatrix." );
    }
    B.comm = grid_->comm;

    for( Int i = 0; i < mpisize; i++ )
      cntSize[i] = 0;

    rowPtr = B.rowindLocal.Data();
    colPtr = B.colptrLocal.Data();
    Scalar* valPtr = B.nzvalLocal.Data();

    for( Int j = 0; j < numColLocal; j++ ){
      Int col         = perm( firstCol + j );
      Int blockColIdx = BlockIdx( col, super_ );
      Int procCol     = PCOL( blockColIdx, grid_ );
      for( Int i = colPtr[j] - 1; i < colPtr[j+1] - 1; i++ ){
        Int row         = perm( *(rowPtr++) - 1 );
        Int blockRowIdx = BlockIdx( row, super_ );
        Int procRow     = PROW( blockRowIdx, grid_ );
        Int dest = PNUM( procRow, procCol, grid_ );
        *(valPtr++) = valSend[displsSend[dest] + cntSize[dest]];
        cntSize[dest]++;
      } // for (i)
    } // for (j)

    // Check sizes match
    for( Int ip = 0; ip < mpisize; ip++ ){
      if( cntSize[ip] != sizeSend[ip] )
        throw std::runtime_error( "Sizes of the sending information do not match." );
    }


#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  }     // -----  end of method PMatrix::PMatrixToDistSparseMatrix2  ----- 




  Int PMatrix::NnzLocal	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::NnzLocal");
#endif
    Int numSuper = this->NumSuper();
    Int nnzLocal = 0;
    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      if( MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        std::vector<LBlock>& Lcol = this->L( LBj( ksup, grid_ ) );
        for( Int ib = 0; ib < Lcol.size(); ib++ ){
          nnzLocal += Lcol[ib].numRow * Lcol[ib].numCol;
        }
      } // if I own the column of ksup
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) ){
        std::vector<UBlock>& Urow = this->U( LBi( ksup, grid_ ) );
        for( Int jb = 0; jb < Urow.size(); jb++ ){
          nnzLocal += Urow[jb].numRow * Urow[jb].numCol;
        }
      } // if I own the row of ksup
    }

#ifndef _RELEASE_
    PopCallStack();
#endif

    return nnzLocal;
  } 		// -----  end of method PMatrix::NnzLocal  ----- 


  // TODO Make nnz using long format
  Int PMatrix::Nnz	(  )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::Nnz");
#endif
    Int nnzLocal = this->NnzLocal();
    Int nnz;

    mpi::Allreduce( &nnzLocal, &nnz, 1, MPI_SUM, grid_->comm );

#ifndef _RELEASE_
    PopCallStack();
#endif

    return nnz;
  } 		// -----  end of method PMatrix::Nnz  ----- 

  void PMatrix::GetNegativeInertia	( Real& inertia )
  {
#ifndef _RELEASE_
    PushCallStack("PMatrix::GetNegativeInertia");
#endif
    Int numSuper = this->NumSuper(); 

    Real inertiaLocal = 0.0;
    inertia          = 0.0;

    for( Int ksup = 0; ksup < numSuper; ksup++ ){
      // I own the diagonal block	
      if( MYROW( grid_ ) == PROW( ksup, grid_ ) &&
          MYCOL( grid_ ) == PCOL( ksup, grid_ ) ){
        LBlock& LB = this->L( LBj( ksup, grid_ ) )[0];
        for( Int i = 0; i < LB.numRow; i++ ){
          if( LB.nzval(i, i).real() < 0 )
            inertiaLocal++;
        }
      }
    }

    // All processors own diag
    mpi::Allreduce( &inertiaLocal, &inertia, 1, MPI_SUM, grid_->comm );

#ifndef _RELEASE_
    PopCallStack();
#endif

    return ;
  } 		// -----  end of method PMatrix::GetNegativeInertia  ----- 


} // namespace PEXSI
